{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96521a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from requests.compat import urljoin\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53e9621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a5f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install spacy\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7197f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open types.txt and read the types\n",
    "with open('../data/gold_standard_corpus.tsv', 'r') as f:\n",
    "    pmcids = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a03aab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmcids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f622a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_Json_through_PMCID(pmcid, provider):\n",
    "    base_url = \"https://www.ebi.ac.uk/europepmc/annotations_api/\"\n",
    "    article_url = urljoin(base_url, \n",
    "                          f\"annotationsByArticleIds?articleIds=PMC%3A{pmcid}&provider={provider}&format=JSON\")\n",
    "    r = requests.get(article_url)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        return r\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_epmc_annotations_to_file(PMCids):\n",
    "    with open('../data/annotations_from_api.csv', 'w', newline='\\n') as f1:\n",
    "        test_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "        for each_id in tqdm(PMCids):\n",
    "            # Loop through each provider\n",
    "            for provider in [\"Europe PMC\", \"PheneBank\"]:\n",
    "                json_annotations = get_Json_through_PMCID(each_id[3:], provider)  # Just the number is needed, so remove the PMC from the front\n",
    "                if json_annotations:\n",
    "                    json_results = json_annotations.json()\n",
    "                    try:\n",
    "                        pmc_id = json_results[0]['pmcid']\n",
    "                        for each_annotation in json_results[0]['annotations']:\n",
    "                            exact = each_annotation['prefix'] + each_annotation['exact'] + each_annotation['postfix']\n",
    "                            token = each_annotation['tags'][0]['name']\n",
    "                            ner = each_annotation['type']\n",
    "                            row = [pmc_id, exact, token, ner, provider]\n",
    "                            test_writer.writerow(row)\n",
    "                    except(IndexError):\n",
    "                        print('No annotations found for ' + str(each_id) + ' from ' + provider)\n",
    "                else:\n",
    "                    print('No annotations from ' + provider + ' for ' + str(each_id))\n",
    "                    continue\n",
    "\n",
    "# Usage example:\n",
    "# PMCids = ['PMC123456', 'PMC654321', ...]\n",
    "# get_epmc_annotations_to_file(PMCids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb41b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_Json_through_PMCID(pmcid):\n",
    "#     base_url = \"https://www.ebi.ac.uk/europepmc/annotations_api/\"\n",
    "#     article_url = urljoin(base_url,\n",
    "#                           \"annotationsByArticleIds?articleIds=PMC%3A\" + pmcid + \"&provider=Europe%20PMC&format=JSON\")\n",
    "#     r = requests.get(article_url)\n",
    "\n",
    "#     if r.status_code == 200:\n",
    "#         return r\n",
    "#     else:\n",
    "#         return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa0a0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_epmc_annotations_to_file(PMCids):\n",
    "#     with open('../data/annotations_api.csv', 'w', newline='\\n') as f1:\n",
    "#         test_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "#         # count = 0\n",
    "#         for each_id in tqdm(PMCids):\n",
    "#             # count = count+1\n",
    "#             # print(each_test_pmc_id + '\\t' + str(count))\n",
    "#             json_annotations = get_Json_through_PMCID(each_id[3:])  # Just the number is needed. So remove the PMC from the front\n",
    "#             if json_annotations:\n",
    "#                 json_results = json_annotations.json()\n",
    "#                 try:\n",
    "#                     pmc_id = json_results[0]['pmcid']\n",
    "#                     # print(pmc_id)\n",
    "#                     for each_annotation in json_results[0]['annotations']:\n",
    "#                         exact = each_annotation['prefix'] + each_annotation['exact'] + each_annotation['postfix']\n",
    "#                         token = each_annotation['tags'][0]['name']\n",
    "#                         ner = each_annotation['type']\n",
    "#                         row = [pmc_id, exact, token, ner]\n",
    "#                         test_writer.writerow(row)\n",
    "#                 except(IndexError):\n",
    "#                     print('no annotations found!! '+str(each_id))\n",
    "#             else:\n",
    "#                 print('no annotations! '+str(each_id))\n",
    "#                 continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd337b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_epmc_annotations_to_file(pmcids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e7f046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmcid</th>\n",
       "      <th>partial_sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>ner</th>\n",
       "      <th>provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>A subunit of the oligosaccharyltransferase com...</td>\n",
       "      <td>oligosaccharyltransferase complex</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>During plant pollen tube (PT) reception, gamet...</td>\n",
       "      <td>pollen tube</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>ing in PT rupture, sperm release, and double f...</td>\n",
       "      <td>double fertilization</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>fic crosses between Arabidopsis thaliana and r...</td>\n",
       "      <td>Arabidopsis thaliana</td>\n",
       "      <td>Organisms</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>T3/6 subunit of the oligosaccharyltransferase ...</td>\n",
       "      <td>oligosaccharyltransferase complex</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89254</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>plasma viral load, p24-antigen and HIV cap</td>\n",
       "      <td>p24</td>\n",
       "      <td>Gene_Proteins</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89255</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>ad, p24-antigen and HIV capture by erythroc</td>\n",
       "      <td>HIV</td>\n",
       "      <td>Organisms</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89256</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>by erythrocytes in HIV-positive individual</td>\n",
       "      <td>HIV</td>\n",
       "      <td>Organisms</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89257</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>association between detectable plasma viral lo...</td>\n",
       "      <td>plasma</td>\n",
       "      <td>Anatomy</td>\n",
       "      <td>PheneBank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89258</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>anti-HIV associated to erythrocyte ( p &lt;</td>\n",
       "      <td>erythrocyte</td>\n",
       "      <td>Cell</td>\n",
       "      <td>PheneBank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89259 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pmcid                                   partial_sentence  \\\n",
       "0      PMC4792959  A subunit of the oligosaccharyltransferase com...   \n",
       "1      PMC4792959  During plant pollen tube (PT) reception, gamet...   \n",
       "2      PMC4792959  ing in PT rupture, sperm release, and double f...   \n",
       "3      PMC4792959  fic crosses between Arabidopsis thaliana and r...   \n",
       "4      PMC4792959  T3/6 subunit of the oligosaccharyltransferase ...   \n",
       "...           ...                                                ...   \n",
       "89254  PMC3458065         plasma viral load, p24-antigen and HIV cap   \n",
       "89255  PMC3458065        ad, p24-antigen and HIV capture by erythroc   \n",
       "89256  PMC3458065         by erythrocytes in HIV-positive individual   \n",
       "89257  PMC3458065  association between detectable plasma viral lo...   \n",
       "89258  PMC3458065           anti-HIV associated to erythrocyte ( p <   \n",
       "\n",
       "                                   token            ner    provider  \n",
       "0      oligosaccharyltransferase complex  Gene Ontology  Europe PMC  \n",
       "1                            pollen tube  Gene Ontology  Europe PMC  \n",
       "2                   double fertilization  Gene Ontology  Europe PMC  \n",
       "3                   Arabidopsis thaliana      Organisms  Europe PMC  \n",
       "4      oligosaccharyltransferase complex  Gene Ontology  Europe PMC  \n",
       "...                                  ...            ...         ...  \n",
       "89254                                p24  Gene_Proteins  Europe PMC  \n",
       "89255                                HIV      Organisms  Europe PMC  \n",
       "89256                                HIV      Organisms  Europe PMC  \n",
       "89257                             plasma        Anatomy   PheneBank  \n",
       "89258                        erythrocyte           Cell   PheneBank  \n",
       "\n",
       "[89259 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df = pd.read_csv('../data/annotations_from_api.csv', sep='\\t', names=['pmcid', 'partial_sentence', 'token', 'ner', 'provider'])\n",
    "annotations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0468c00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gene Ontology', 'Organisms', 'Gene_Proteins',\n",
       "       'Experimental Methods', 'Diseases', 'Chemicals',\n",
       "       'Accession Numbers', 'Resources', 'Molecule', 'Phenotype',\n",
       "       'Anatomy', 'Cell', 'Pathway', 'Gene Mutations'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df['ner'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5618d114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmcid</th>\n",
       "      <th>partial_sentence</th>\n",
       "      <th>token</th>\n",
       "      <th>ner</th>\n",
       "      <th>provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>A subunit of the oligosaccharyltransferase com...</td>\n",
       "      <td>oligosaccharyltransferase complex</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>During plant pollen tube (PT) reception, gamet...</td>\n",
       "      <td>pollen tube</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>ing in PT rupture, sperm release, and double f...</td>\n",
       "      <td>double fertilization</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>T3/6 subunit of the oligosaccharyltransferase ...</td>\n",
       "      <td>oligosaccharyltransferase complex</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>nsferase complex conferring protein N-glycosyl...</td>\n",
       "      <td>glycosylation</td>\n",
       "      <td>Experimental Methods</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89201</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>The viral capture assay was performed after t</td>\n",
       "      <td>assay</td>\n",
       "      <td>Experimental Methods</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89220</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>ated for 7 to 10 days and assayed for syncytiu...</td>\n",
       "      <td>syncytium formation</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89221</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>is able to inhibit syncytium formation (loss ...</td>\n",
       "      <td>syncytium formation</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89228</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>end-point dilution assay in MT-2 cells cultu</td>\n",
       "      <td>assay</td>\n",
       "      <td>Experimental Methods</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89250</th>\n",
       "      <td>PMC3458065</td>\n",
       "      <td>e IgG anti-HIV binding to erythrocyte membrane.</td>\n",
       "      <td>membrane</td>\n",
       "      <td>Gene Ontology</td>\n",
       "      <td>Europe PMC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30940 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            pmcid                                   partial_sentence  \\\n",
       "0      PMC4792959  A subunit of the oligosaccharyltransferase com...   \n",
       "1      PMC4792959  During plant pollen tube (PT) reception, gamet...   \n",
       "2      PMC4792959  ing in PT rupture, sperm release, and double f...   \n",
       "4      PMC4792959  T3/6 subunit of the oligosaccharyltransferase ...   \n",
       "6      PMC4792959  nsferase complex conferring protein N-glycosyl...   \n",
       "...           ...                                                ...   \n",
       "89201  PMC3458065      The viral capture assay was performed after t   \n",
       "89220  PMC3458065  ated for 7 to 10 days and assayed for syncytiu...   \n",
       "89221  PMC3458065   is able to inhibit syncytium formation (loss ...   \n",
       "89228  PMC3458065       end-point dilution assay in MT-2 cells cultu   \n",
       "89250  PMC3458065   e IgG anti-HIV binding to erythrocyte membrane.    \n",
       "\n",
       "                                   token                   ner    provider  \n",
       "0      oligosaccharyltransferase complex         Gene Ontology  Europe PMC  \n",
       "1                            pollen tube         Gene Ontology  Europe PMC  \n",
       "2                   double fertilization         Gene Ontology  Europe PMC  \n",
       "4      oligosaccharyltransferase complex         Gene Ontology  Europe PMC  \n",
       "6                          glycosylation  Experimental Methods  Europe PMC  \n",
       "...                                  ...                   ...         ...  \n",
       "89201                              assay  Experimental Methods  Europe PMC  \n",
       "89220                syncytium formation         Gene Ontology  Europe PMC  \n",
       "89221                syncytium formation         Gene Ontology  Europe PMC  \n",
       "89228                              assay  Experimental Methods  Europe PMC  \n",
       "89250                           membrane         Gene Ontology  Europe PMC  \n",
       "\n",
       "[30940 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df_other = annotations_df[annotations_df['ner'].isin(['Gene Ontology', 'Experimental Methods', 'Accession Numbers', 'Resources', 'Chemicals'])]\n",
    "annotations_df_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd11fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get unique PMCIDs\n",
    "# unique_pmcids = annotations_df_other['pmcid'].unique()url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML\"\n",
    "# unique_pmcids\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "\n",
    "# def get_full_text_xml(pmcid):\n",
    "#     url = f\"https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.content, 'xml')\n",
    "#         p_tags = soup.find_all('p')\n",
    "#         p_texts = [tag.get_text() for tag in p_tags]\n",
    "#         return p_texts\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09a2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pmcids = annotations_df_other['pmcid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf5eb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text_xml(pmcid):\n",
    "    path = f\"../data/300_articles_source_files/{pmcid}.xml\"\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'lxml-xml')  # Use lxml-xml parser\n",
    "        plain_tags = soup.find_all('plain')\n",
    "        plain_texts = [tag.get_text() for tag in plain_tags]\n",
    "        return plain_texts\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {pmcid}.xml not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a2496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def check_previous_word_pos(sentence, target_word):\n",
    "    # Process the sentence to get tokenized words and their POS tags\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Find the target word and get the POS of the previous word\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.text == target_word and i > 0:\n",
    "            previous_word = doc[i - 1]\n",
    "            pos_of_previous = previous_word.pos_\n",
    "            \n",
    "            # Check if the POS is one of the ones we want to avoid\n",
    "            if pos_of_previous in ['DET', 'ADV', 'VERB', 'ADP']:\n",
    "                return False  # POS to be avoided\n",
    "            else:\n",
    "                return True  # Acceptable POS\n",
    "\n",
    "    return False  # Return False if the target word is not found or is the first word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dfe5245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 299/299 [00:50<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "def find_sentence_with_substring(string_list, substring):\n",
    "    for text in string_list:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        for sentence in sentences:\n",
    "            if substring in sentence:\n",
    "                return sentence\n",
    "    return None\n",
    "\n",
    "def check_previous_word_pos(sentence, prev_word):\n",
    "    # Process the sentence to get tokenized words and their POS tags\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text == prev_word:\n",
    "            if token.i > 0:\n",
    "                pos_of_previous = doc[token.i - 1].pos_\n",
    "                return pos_of_previous not in ['DET', 'ADV', 'VERB', 'ADP', 'NUM', 'CONJ', 'SCONJ']\n",
    "\n",
    "    return False\n",
    "\n",
    "def adjust_token_for_context(sentence, token, ner_):\n",
    "    check_list = ['the', 'an', 'a', 'that', 'after', 'and', 'total', 'using', 'by','as', 'for', 'in', 'with', \n",
    "                  'underwent', 'of', 'with', 'but', 'or', 'before', 'at', \n",
    "                  'activate', 'research', 'favors', 'prevent', 'to', 'drive', 'on', 'both', 'performed', 'upon',\n",
    "                 'calculate', 'like', 'μl', 'support', 'our', 'core', 'eight', 'was', \n",
    "                  'nested', 'show', 'designed', '3', 'each', 'uts', 'these', 'when' '1', 'no', \n",
    "                  'from', 'if', 'individual', 'beyond', 'used', 'facilitated', \n",
    "                  'when', 'perform', 'involving', 'time', 'were', 'rich', '1x', 'this', \n",
    "                  'Briefly,', 'generated', 'following', 'since']\n",
    "    \n",
    "    specific_phrases = ['viral capture assay', 'flow cytometry assay', \n",
    "                        'luminescent cell viability assay', 'real-time PCR', 'RT-PCR', 'T-DNA insertion']\n",
    "    \n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    punctuation_marks = [',', '.', '?', '!', ';', ']','[',')','(', ':']\n",
    "    extended_tokens = set()\n",
    "    \n",
    "            \n",
    "    # Remove parentheses from the token\n",
    "    token_cleaned = token.replace('(', '').replace(')', '')\n",
    "\n",
    "    # Adjust token for larger term if token is a substring and does not contain specific punctuation\n",
    "    if ner_ not in['Chemicals', 'Accession Numbers']:\n",
    "        for word in words:\n",
    "            cleaned_word = word.replace('(', '').replace(')', '')\n",
    "            if token_cleaned in cleaned_word and cleaned_word != token_cleaned and not any(mark in cleaned_word for mark in punctuation_marks):\n",
    "                extended_tokens.add(cleaned_word)\n",
    "            else:\n",
    "                extended_tokens.add(token_cleaned)\n",
    "    else:\n",
    "        extended_tokens.add(token_cleaned)\n",
    "            \n",
    "    \n",
    "\n",
    "    if ner_ in ['Experimental Methods']:\n",
    "        \n",
    "        for phrase in specific_phrases:\n",
    "        # Find all case-insensitive occurrences of the phrase\n",
    "            found_phrases = re.findall(phrase, sentence, re.IGNORECASE)\n",
    "            # Add each found phrase to the extended tokens\n",
    "            for found in found_phrases:\n",
    "                extended_tokens.add(found)\n",
    "    \n",
    "    # Check for words immediately before the token and add those combinations\n",
    "        for i, word in enumerate(words):\n",
    "            cleaned_word = word.replace('(', '').replace(')', '')\n",
    "            if cleaned_word == token_cleaned and i > 0:\n",
    "                prev_word = words[i - 1].replace('(', '').replace(')', '')\n",
    "                if token_cleaned.lower() in ['assay', 'assays', 'insertion', 'insertions', 'elisa', 'pcr', \n",
    "                                             'mda', 'iss' 'biopsy', 'co-expression', \n",
    "                                             'co-expressions', ' transfection']:\n",
    "                    if prev_word.lower() not in check_list and (prev_word.isupper() or not prev_word.islower()):\n",
    "                        extended_tokens.add(prev_word + ' ' + token_cleaned)\n",
    "                elif prev_word.lower() not in check_list and check_previous_word_pos(sentence, prev_word):\n",
    "                    extended_tokens.add(prev_word + ' ' + token_cleaned)\n",
    "\n",
    "        # Always add the token itself, after removing parentheses\n",
    "        extended_tokens.add(token_cleaned)\n",
    "\n",
    "    return list(extended_tokens)\n",
    "\n",
    "\n",
    "def process_pmcid(df, pmcid, p_texts):\n",
    "    sentences_data = {}\n",
    "    for _, row in df[df['pmcid'] == pmcid].iterrows():\n",
    "        sentence = find_sentence_with_substring(p_texts, row['partial_sentence'])\n",
    "        if sentence:\n",
    "            if sentence not in sentences_data:\n",
    "                sentences_data[sentence] = set()\n",
    "\n",
    "            updated_tokens = adjust_token_for_context(sentence, row['token'], row['ner'])\n",
    "            for updated_token in updated_tokens:\n",
    "                sentences_data[sentence].add((updated_token, row['ner']))\n",
    "\n",
    "    return [[pmcid, sentence, list(ner_tags)] for sentence, ner_tags in sentences_data.items()]\n",
    "\n",
    "final_data = []\n",
    "\n",
    "for pmcid in tqdm(unique_pmcids):\n",
    "    p_texts = get_full_text_xml(pmcid)\n",
    "    if p_texts:\n",
    "        processed_data = process_pmcid(annotations_df_other, pmcid, p_texts)\n",
    "        final_data.extend(processed_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "final_df = pd.DataFrame(final_data, columns=['pmcid', 'sentence', 'ner'])\n",
    "\n",
    "# Save as TSV\n",
    "final_df.to_csv('../data/xxx7.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a289219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def calculate_spans(sentence, entities):\n",
    "    spans = []\n",
    "    for entity, etype in entities:\n",
    "        start = sentence.find(entity)\n",
    "        if start != -1:\n",
    "            end = start + len(entity)\n",
    "            spans.append([start, end, entity, etype])  # List instead of tuple\n",
    "    return spans\n",
    "\n",
    "def resolve_overlaps(spans):\n",
    "    spans.sort(key=lambda x: (x[1] - x[0]), reverse=True)  # Sort by span length\n",
    "    resolved = []\n",
    "    for span in spans:\n",
    "        overlap = False\n",
    "        for r in resolved:\n",
    "            if not (span[1] <= r[0] or span[0] >= r[1]):  # Check for overlap\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            resolved.append(span)\n",
    "    return resolved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b68d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/xxx7.tsv', sep = '\\t')\n",
    "df['new_ner'] = df.apply(lambda row: resolve_overlaps(calculate_spans(row['sentence'], ast.literal_eval(row['ner']))), axis=1)\n",
    "df.drop('ner', axis=1, inplace=True)\n",
    "df.rename(columns={'new_ner': 'ner'}, inplace=True)\n",
    "df.to_csv('../data/xxx6.tsv', sep = '\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29974ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmcid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>A subunit of the oligosaccharyltransferase com...</td>\n",
       "      <td>[[17, 50, oligosaccharyltransferase complex, GO]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>During plant pollen tube (PT) reception, gamet...</td>\n",
       "      <td>[[13, 24, pollen tube, GO]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>ARU encodes the OST3/6 subunit of the oligosac...</td>\n",
       "      <td>[[38, 71, oligosaccharyltransferase complex, GO]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>Our results suggest that glycosylation pattern...</td>\n",
       "      <td>[[25, 38, glycosylation, EM]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PMC4792959</td>\n",
       "      <td>show that ARTUMES regulates pollen tube recogn...</td>\n",
       "      <td>[[28, 39, pollen tube, GO]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pmcid                                           sentence  \\\n",
       "0  PMC4792959  A subunit of the oligosaccharyltransferase com...   \n",
       "1  PMC4792959  During plant pollen tube (PT) reception, gamet...   \n",
       "2  PMC4792959  ARU encodes the OST3/6 subunit of the oligosac...   \n",
       "3  PMC4792959  Our results suggest that glycosylation pattern...   \n",
       "4  PMC4792959  show that ARTUMES regulates pollen tube recogn...   \n",
       "\n",
       "                                                 ner  \n",
       "0  [[17, 50, oligosaccharyltransferase complex, GO]]  \n",
       "1                        [[13, 24, pollen tube, GO]]  \n",
       "2  [[38, 71, oligosaccharyltransferase complex, GO]]  \n",
       "3                      [[25, 38, glycosylation, EM]]  \n",
       "4                        [[28, 39, pollen tube, GO]]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename entity names in file2.csv as per the provided mapping\n",
    "entity_rename_map = {\n",
    "    'Gene Ontology': 'GO',\n",
    "    'Experimental Methods': 'EM', \n",
    "    'Accession Numbers': 'AN',\n",
    "    'Resources': 'RS',\n",
    "    'Chemicals': 'CD'\n",
    "}\n",
    "\n",
    "file2_df = pd.read_csv('../data/xxx6.tsv', sep = '\\t')\n",
    "# Applying the renaming\n",
    "file2_df['ner'] = file2_df['ner'].apply(lambda x: eval(x))\n",
    "for row in file2_df['ner']:\n",
    "    for entity in row:\n",
    "        entity[3] = entity_rename_map.get(entity[3], entity[3])\n",
    "\n",
    "# Checking the changes\n",
    "file2_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "492d1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcting the code to work with the 'ners' column, use sets to remove duplicates, \n",
    "# and applying specific handling for 'AN' type\n",
    "\n",
    "# Adjusting the safe_eval_list function to check if the input is a string\n",
    "def safe_eval_list(str_list):\n",
    "    if isinstance(str_list, str):\n",
    "        try:\n",
    "            return eval(str_list)\n",
    "        except SyntaxError:\n",
    "            return []\n",
    "    return str_list\n",
    "\n",
    "# Applying the adjusted renaming and extraction\n",
    "file2_df['ner'] = file2_df['ner'].apply(safe_eval_list)\n",
    "for row in file2_df['ner']:\n",
    "    for entity in row:\n",
    "        entity[3] = entity_rename_map.get(entity[3], entity[3])\n",
    "\n",
    "        \n",
    "def replace_substrings(input_str):\n",
    "    substrings_to_replace = ['*/', '18O ', '% ', 'μm', 'its ', 'are ',  'successful ', 'III ', '6 ', \n",
    "                             'their ', 'μm ', 'between ', 'some ', 'through ', 'mL ', 'while ','s layer', '50 ', \n",
    "                             'vs ', 'induces ', 'p53 ', 'promotes ', 'within ', 'precise ', 'nitrogen ', '—0', \n",
    "                             'other ', 'created ', 'basement ', 'best ', 'study ', 'than ', '94°C,', 'followed ', \n",
    "                             '°CAbbreviations:', '7500', 'simultaneous', 'represent', 'II ', '15 ', '10× ', \n",
    "                             'initiation ', 'all ', 'toward ', 'improve ', 'whose ', \n",
    "                             'influence ', 'convey', 'and/or ', 'acid ', \n",
    "                             'analyzed ', 'thus ', 'per ', 'affect ', 'associated ', 'increased' , \n",
    "                             'releases ', 'kill', 'Nonetheless, ', \n",
    "                            '“', 'against ', 'additional ', 'late ', 'reduced ', 'Distribution ', \n",
    "'giving ', 'maintain ', 'nighttime ', 'increase ', 'during ', '30 ' 'HIV ',\n",
    "'whether ', '- ', 'use ', 'Three ', 'Caused', 'block ', 'IL-8 ', 'No ', 'create ', '”'\n",
    "'Il-21 ', 'what ', 'base ', 'paid ', 'then ', 'without ', 'where ', '16100 ', '2 ', '325,000 ', 'underlying ', 'related ']\n",
    "    \n",
    "    for substring in substrings_to_replace:\n",
    "        input_str = input_str.replace(substring, '')\n",
    "    \n",
    "    return input_str\n",
    "\n",
    "\n",
    "# Extracting dictionary collection of set with AN, EM, RS, and GO\n",
    "extracted_entities = {'AN': set(), 'EM': set(), 'RS': set(), 'GO': set(), 'CD': set()}\n",
    "for row in file2_df['ner']:\n",
    "    for entity in row:\n",
    "        entity_type = entity[3]\n",
    "        if entity_type in extracted_entities:\n",
    "            # For 'AN', split with a space and take the last term\n",
    "            if entity_type == 'AN':\n",
    "                extracted_entities[entity_type].add(entity[2].split()[-1])\n",
    "            elif entity_type == 'RS' and 'database' in entity[2]:\n",
    "                extracted_entities[entity_type].add(entity[2].replace('database','').strip())\n",
    "            elif entity_type == 'GO' or entity_type == 'EM':\n",
    "                if 'membrane' in entity[2].lower() or 'behaviour' in entity[2].lower():\n",
    "                    extracted_entities[entity_type].add(entity[2].split()[-1])\n",
    "                else:\n",
    "                    replac_str = replace_substrings(entity[2]).strip()\n",
    "                    if replac_str and len(replac_str)>1:\n",
    "                        extracted_entities[entity_type].add(replac_str)\n",
    "            else:\n",
    "                extracted_entities[entity_type].add(entity[2])\n",
    "\n",
    "\n",
    "# Converting sets back to lists for easier viewing/usage\n",
    "extracted_entities = {k: list(v) for k, v in extracted_entities.items()}\n",
    "\n",
    "# extracted_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1ff2bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_entities['RS'].extend(['NHGRI', 'GEO', 'Sequence Read Archive', 'SRA', 'KEGG', \n",
    "                                 'Kyoto Encyclopedia of Genes and Genomes', 'IntAct62',\n",
    "                                 'MINT63', 'AtPIN36', 'AtPID37', 'PAIR38', 'APID39', 'PRIN9', 'PHISTO', 'TAIR',\n",
    "                                'InParanoid', 'HPRD', 'Medline', 'EMBASE', 'PsycINFO', 'EconLit', 'HEED',\n",
    "                                 'CRD', 'Health Economic Evaluations Database', 'HEED', 'NHS Economic Evaluation Database',\n",
    "                                'NHS EED', 'Database of Abstracts of Reviews of Effects', 'DARE', 'PROSITE',\n",
    "                                'CINAHL', 'Cochrane', 'ARAMEMNON', 'TrEMBL', 'PhosphoSitePlus', \n",
    "                                 'UniProtKB/TrEMBL', 'SwissProt', 'miRWalk',\n",
    "                                 'DAVID', 'SEER', 'Genebank','MSigDB', 'MGI', 'PsyGeNET', 'SFARI', 'DisGeNET',\n",
    "                                'GeoSymbio', 'TIGR', 'GenBank', 'Swiss-prot' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc9efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accessions = list(set(['FN813615', 'FN813614', 'AM042701', 'AM042698', 'AM042697', 'AM042699', 'FN823053', \n",
    "                  'FN995351', 'KY355383', 'ENSDARG000000', 'XP_694951', 'XP_694893', 'XP_686111', \n",
    "                  'XP_686816', 'XP_689021', 'XP_693637', 'XP_694145', 'XP_693652', 'XP_691478', \n",
    "                  'XP_689023', 'XP_698503', 'XP_690676', 'XP_689954', 'XP_693868', 'XP_688350', \n",
    "                  'XP_697815', 'XP_687258', 'XP_690327', 'XP_687591', 'XP_691896', 'XP_689864', \n",
    "                  'XP_690860', 'XP_694605', 'XP_694871', 'XP_696432', 'XP_698814', 'XP_699128', \n",
    "                  'XP_697396', 'ENSGALG00000011535', 'XM_414969', 'XM_414012', 'XM_414012', 'Q5F3J4', \n",
    "                  'ENSXETG00000022012', 'GENSCAN00000015124', 'AC157698', 'ENSXETG00000021586', \n",
    "                  'scaffold_279.46c', 'scaffold_1247.5', 'scaffold_3599.1', 'scaffold_626.23', \n",
    "                  'scaffold_279.46c', 'scaffold_626.24', 'scaffold_626.25', 'scaffold_279.46c', \n",
    "                  'scaffold_279.46c', 'scaffold_111.48', 'ENSXETG00000018490', 'NEWSINFRUG00000144256', \n",
    "                  'NEWSINFRUG00000134206', 'NEWSINFRUG00000138086', 'GENSCANSLICE00000000561', \n",
    "                  'NEWSINFRUG00000148012', 'TC326097', 'TC353741', 'TC343155', 'EST487', 'MF468321', \n",
    "                  'MF468322', 'MF468319', 'MF468318', 'MF468291', 'MF468293', 'MF468299', 'MF468290', \n",
    "                  'MF468308', 'MF468291', 'MF468293', 'MF468294', 'MF468298', 'MF468309', 'MF468314', \n",
    "                  'MF468295', 'MF468297', 'MF468325', 'MF468328', 'AAL56428', 'AAL56429', 'AAB05820', \n",
    "                  'U39815', 'AF126830 ', 'AAL56430', 'AAL58703', 'AAL58704 ', 'rs4179590', 'rs4179590', \n",
    "                  'rs4179589', 'rs51960690', 'rs50972129', 'rs4179517', 'rs4179515', 'rs48139623', \n",
    "                  'rs32704753', 'rs4179510', 'rs4179504', 'rs4179702', 'rs31418354', 'rs4179917', \n",
    "                  'rs4179913', 'rs45694900', 'rs4179908', 'Q1051R', 'rs4179907', 'rs4179728', \n",
    "                  'rs4179729', 'rs4179745', 'ENSMUSSNP328538', 'rs32712110', 'rs4179843', 'rs4179872', \n",
    "                  'rs31418356', 'rs32712642', 'PRJNA375720', 'AY157059', 'AJ000520', 'BA000042', \n",
    "                  'ABE84009', 'ABE86610', 'AE017283', 'NM116255', 'CP000323', 'CP000653', 'AJ316032', \n",
    "                  'JQ804931', 'GSE97544', 'PRJEB9815', 'GSM1334015', 'GSM1334016', 'GSM1334017', \n",
    "                  'GSM1334018', 'GSM1334019', 'GSM1334020', 'KY355383', 'Q7RTR2', 'Q9Y239', 'Q7RTR2', \n",
    "                  'Q86WI3', 'Q7RTR3', 'Q9C000', 'Q9NX02', 'Q96P20', 'Q96MN2', 'P59047', 'P59044', \n",
    "                  'Q32MH8', 'Q86W28', 'Q86W27', 'Q86W26', 'P59045', 'P59046', 'Q86W25', 'Q86W24', \n",
    "                  'Q9NPP4', 'Q13075', 'P33076', 'KT184354', 'DQ136185', 'AM492684a', 'EU339226', \n",
    "                  'EU339209', 'EU339214', 'EU339217', 'Ynys1', 'AF387302a', 'EU339223', 'EU339216', \n",
    "                  'WJ68', 'AY455805a', 'EU339224', 'EU339213', 'AY950676a', 'EU339231', 'EU304260a', \n",
    "                  'EU339222', 'AY455808a', 'EU339230', 'EU339215', 'GSE24189', 'KJ598498.1', 'GSE9210', \n",
    "                  'KF309066', 'MH238461', 'GSE43316', 'AY335912', 'AM902709', 'PRJNA236112', 'AM492684a', \n",
    "                  'EU339226', 'EU339209', 'EU339214', 'EU339217', 'Ynys1', 'AF387302a', 'EU339223', \n",
    "                  'EU339216', 'WJ68', 'AY455805a', 'EU339224', 'EU339213', 'AY950676a', 'EU339231', \n",
    "                  'EU304260a', 'EU339222', 'AY455808a', 'EU339230', 'EU339215', 'EU792508', 'ACF19852', \n",
    "                  'EU792509', 'ACF19853', 'EU792510', 'ACF19854', 'EU792511', 'ACF19855', 'AAC32826', \n",
    "                  'ABK76312', 'AFJ95132', 'AAC48340', 'AF143817', 'AF041023', 'AB009305', 'AF126830', \n",
    "                  'AAL56430', 'AAL58703', 'AAL58704', 'AAL56428', 'AAL56429', 'AAB07368', 'AAB05820', \n",
    "                  'U39815', 'AF167707', 'AF167708', 'AF167709', 'AF167710', 'CAC28360', 'U25057', \n",
    "                  'BAA14015', 'AAD01628', 'AAB26196', 'P54190', 'U29761', 'AAB53231', 'ACJ64718', \n",
    "                  'AAB00227', 'SRP049689', 'EG10787', 'EG11221', 'EG12144', 'EG10379', 'EG10283', \n",
    "                  'EG11448', 'EG10688', 'EG10489', 'EG10283', 'EG20173', 'EG10702', 'KX262676.1', \n",
    "                  'KX352162', 'KF532967', 'NM_000484', 'KX553930', 'KX553934', 'AM117604', 'FN813615', \n",
    "                  'FN813614', 'AM042701', 'AM042698', 'AM042697', 'AM042699', 'FN823053', 'FN995351', \n",
    "                  'AM117602.1', 'NM-138554.1', 'P25297', 'PHO84', 'NP_001337.2', 'NP_003638', 'GSE50635','MF468290', 'SRR4011627', 'SRR4011628', 'HM019534', 'HM020374', 'FN813615',\n",
    "'FN813614', 'AM042701', 'AM042698', 'AM042697', 'AM042699', 'HM019533', 'HM020128', 'HM020130', 'HM020332',\n",
    "'HM202334', 'HM020374', 'COSM43967', 'COSM1640833', 'COSM562645', 'COSM44853', 'COSM99631']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76ca0d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_accessions)  #previously 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb1ca7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "416\n"
     ]
    }
   ],
   "source": [
    "print(len(extracted_entities['AN']))\n",
    "extracted_entities['AN'].extend(new_accessions)\n",
    "print(len(extracted_entities['AN']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "252e5dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/CD_terms_list.txt', 'w') as file:\n",
    "#     for term in list(set(extracted_entities['CD'])):\n",
    "#         file.write(term + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d25f2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_to_remove = ['uc.134', 'uc','Yorkie','chemical substances','sec','Cuban','8690176','131127258',\n",
    "'CD','DS','steward','ZF-EST487-R3','-EST487','delta','n13131313','6429','sc-376167','QX-314','Cy3',\n",
    "'retina','gamma','singular','electron','1474310','1415180','HM5040',\n",
    "'singular','sec ','Nile Red ','91558 ','KK4824 ','910111213 ','23333435 ','678545556 ','torques ','dump',\n",
    "'torque','at a p','syber green','Short','Short chain fatty acids','2021222324','11192732','1001', '90th',\n",
    "'Gamma',  'Tech', 'fade', ' His-', '-pro-', 'mold', 'electton', 'Psi', 'radio', 'Radio', 'Cho',  'Fmoc', \n",
    " 'aaa', 'h groups', 'His-', '3At', 'a steroid', 'Cy3', 'Tris buffers', 'mega', \n",
    " 'Halogen', 'Ocn', 'an alcohol', 'fmoc', 'lime', '-Trp', '-Pro', '-His', 'Photon', ' suc', 'nape', \n",
    "'Texas red', 'Clo', ' pro-', 'polar amino acids', 'nonyl', 'Dapi', 'hydroxyl radicals', 'aromatic amino acid',\n",
    "  'Foscarnet', 'mannose', ' Cho', 'at a p', '3 ms', '3 at', 'bile salts', 'procure',  'thio',  'an aldehyde',               \n",
    "  'Dopa', 'limestones', ' sec', ' Pro-', 'Retina', 'e is', 'flash',   'molds', 'Flash', 'radical', 'torques']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "492566a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [term for term in list(set(extracted_entities['CD'])) if term not in chem_to_remove]\n",
    "# filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd968049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/CD_terms_list.txt', 'w') as file:\n",
    "#     for term in list(set(filtered_list)):\n",
    "#         file.write(term + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff2d3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_entities['GO'].remove('part')\n",
    "extracted_entities['RS'].remove('HPA')\n",
    "extracted_entities['CD'] = filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6811c7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def enrich_entities(file_path, new_entity_dict):\n",
    "#     # Load the CSV file\n",
    "#     df = pd.read_csv(file_path, names=['sentence', 'ner'])\n",
    "\n",
    "#     # Function to find if there is an overlap with existing spans or if the term is a substring\n",
    "#     def is_valid_entity(sentence, new_span, existing_spans, current_entity_type):\n",
    "#         start_idx, end_idx = new_span\n",
    "#         # Check for overlap with existing spans and allow 'AN' entities to override 'CD'\n",
    "#         for span, span_type in existing_spans:\n",
    "#             if not (end_idx <= span[0] or start_idx >= span[1]):\n",
    "#                 # Allow 'AN' to override 'CD'\n",
    "#                 if current_entity_type == 'AN' and span_type == 'CD':\n",
    "#                     continue\n",
    "#                 return False\n",
    "#         # Check if the entity is not part of a larger word (to avoid substrings)\n",
    "#         if (start_idx > 0 and sentence[start_idx - 1].isalnum()) or (end_idx < len(sentence) and sentence[end_idx].isalnum()):\n",
    "#             return False\n",
    "#         return True\n",
    "\n",
    "#     # Function to add new entity annotations\n",
    "#     def add_new_entities(sentence, entities, entity_dict):\n",
    "#         existing_spans = [(tuple(map(int, entity[:2])), entity[3]) for entity in entities if entity != 'None']  # Tuple of span and type\n",
    "#         for entity_type, terms in entity_dict.items():\n",
    "#             for term in terms:\n",
    "#                 start_idx = sentence.find(term)\n",
    "#                 while start_idx != -1:\n",
    "#                     end_idx = start_idx + len(term)\n",
    "#                     new_span = (start_idx, end_idx)\n",
    "#                     if is_valid_entity(sentence, new_span, existing_spans, entity_type):\n",
    "#                         entities = [entity for entity in entities if not (entity[0] == start_idx and entity[3] == 'CD')]  # Remove overridden 'CD'\n",
    "#                         entities.append([start_idx, end_idx, term, entity_type])\n",
    "#                         existing_spans.append((new_span, entity_type))\n",
    "#                     start_idx = sentence.find(term, start_idx + 1)\n",
    "#         return [entity for entity in entities if entity != 'None']  # Exclude 'None' if other entities are present\n",
    "\n",
    "#     # Enriching the entity annotations\n",
    "#     for i, row in df.iterrows():\n",
    "#         sentence = row['sentence']\n",
    "#         entities = eval(row['ner']) if row['ner'] != \"['None']\" else ['None']\n",
    "#         df.at[i, 'ner'] = add_new_entities(sentence, entities, new_entity_dict)\n",
    "\n",
    "#     return df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def enrich_entities(file_path, new_entity_dict, chem_to_remove):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path, names=['sentence', 'ner'])\n",
    "\n",
    "    # Function to find if there is an overlap with existing spans or if the term is a substring\n",
    "    def is_valid_entity(sentence, new_span, existing_spans, current_entity_type):\n",
    "        start_idx, end_idx = new_span\n",
    "        # Check for overlap with existing spans\n",
    "        for span, span_type in existing_spans:\n",
    "            if not (end_idx <= span[0] or start_idx >= span[1]):\n",
    "                return False\n",
    "        # Check if the entity is not part of a larger word (to avoid substrings)\n",
    "        if (start_idx > 0 and sentence[start_idx - 1].isalnum()) or (end_idx < len(sentence) and sentence[end_idx].isalnum()):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    # Function to add new entity annotations\n",
    "    def add_new_entities(sentence, entities, entity_dict):\n",
    "        existing_spans = [(tuple(map(int, entity[:2])), entity[3]) for entity in entities if entity != 'None']  # Tuple of span and type\n",
    "        for entity_type, terms in entity_dict.items():\n",
    "            for term in terms:\n",
    "                start_idx = sentence.find(term)\n",
    "                while start_idx != -1:\n",
    "                    end_idx = start_idx + len(term)\n",
    "                    new_span = (start_idx, end_idx)\n",
    "                    if is_valid_entity(sentence, new_span, existing_spans, entity_type):\n",
    "                        entities.append([start_idx, end_idx, term, entity_type])\n",
    "                        existing_spans.append((new_span, entity_type))\n",
    "                    start_idx = sentence.find(term, start_idx + 1)\n",
    "\n",
    "        # Remove 'CD' entities if they are in chem_to_remove list\n",
    "        entities = [entity for entity in entities if not (entity[2] in chem_to_remove and entity[3] == 'CD')]\n",
    "        return [entity for entity in entities if entity != 'None']  # Exclude 'None' if other entities are present\n",
    "\n",
    "    # Enriching the entity annotations\n",
    "    for i, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        entities = eval(row['ner']) if row['ner'] != \"['None']\" else ['None']\n",
    "        df.at[i, 'ner'] = add_new_entities(sentence, entities, new_entity_dict)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e96b9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_span(sub_span_range, full_spans_range):\n",
    "    # Check if a sub span is present in full span\n",
    "    if sub_span_range[0] in range(full_spans_range[0], full_spans_range[1]):\n",
    "        return sub_span_range\n",
    "\n",
    "def convert2IOB(text_data, ner_tags):\n",
    "    if 'None' in ner_tags:\n",
    "        # Split text into words and return with 'O' tags if there are no NER tags\n",
    "        split_text = tokenizer.tokenize(text_data)\n",
    "        return zip(split_text, ['O'] * len(split_text))\n",
    "\n",
    "    # Tokenize text and create a list of token spans\n",
    "    tokens = tokenizer.tokenize(text_data)\n",
    "    current_pos = 0\n",
    "    token_spans = []\n",
    "    for token in tokens:\n",
    "        start_pos = text_data.find(token, current_pos)\n",
    "        end_pos = start_pos + len(token)\n",
    "        token_spans.append((start_pos, end_pos))\n",
    "        current_pos = end_pos\n",
    "\n",
    "    # Initialize IOB tags as 'O'\n",
    "    iob_tags = ['O'] * len(tokens)\n",
    "\n",
    "    # Process each NER tag\n",
    "    for start, end, entity, entity_type in sorted(ner_tags, key=lambda x: x[0]):\n",
    "        for i, span in enumerate(token_spans):\n",
    "            if start <= span[0] < end:\n",
    "                if start == span[0]:\n",
    "                    # Beginning of entity\n",
    "                    iob_tags[i] = 'B-' + entity_type\n",
    "                else:\n",
    "                    # Inside of entity\n",
    "                    iob_tags[i] = 'I-' + entity_type\n",
    "\n",
    "    return zip(tokens, iob_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8a6658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "from nltk.tokenize import WordPunctTokenizer, wordpunct_tokenize\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def convert_to_IOB_format(data_file, output_folder,filename):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Define the output path\n",
    "    result_path = os.path.join(output_folder, filename)\n",
    "    pathlib.Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(result_path, 'w', newline='\\n') as f1:\n",
    "        train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            text = row['sentence']\n",
    "            ner_tags = eval(row['ner']) if row['ner'] != \"['None']\" else ['None']\n",
    "            tagged_tokens = convert2IOB(text, ner_tags)\n",
    "            for each_token in tagged_tokens:\n",
    "                train_writer.writerow(list(each_token))\n",
    "            train_writer.writerow('')\n",
    "\n",
    "output_folder = '../data/enriched_IOB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "662184ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "filename = 'train.csv'\n",
    "\n",
    "file_path = '../data/CD_GP_DS_OG_test/clean_CD_GP_DS_OG_'+filename\n",
    "new_df = enrich_entities(file_path, extracted_entities, chem_to_remove)\n",
    "new_df.to_csv(file_path.replace('clean_','enriched_1_'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19d90801",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/CD_GP_DS_OG_test/enriched_1_CD_GP_DS_OG_'+filename\n",
    "convert_to_IOB_format(data_file, output_folder,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91451ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
