{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96521a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from requests.compat import urljoin\n",
    "import requests\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e71ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc005e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/stirunag/falconframes_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc7231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e26080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "================= Installed pipeline packages (spaCy v3.7.0) =================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy\u001b[0m\n",
      "\n",
      "NAME             SPACY            VERSION                     \n",
      "en_core_sci_sm   >=3.7.4,<3.8.0   \u001b[38;5;3m0.5.4\u001b[0m   --> n/a\n",
      "\n",
      "\u001b[38;5;4mℹ The following packages are custom spaCy pipelines or not available\n",
      "for spaCy v3.7.0:\u001b[0m\n",
      "en_core_sci_sm\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d1b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_sci_sm' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:2170: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# nlp = spacy.load(\"en_core_sci_sm\")\n",
    "nlp = spacy.load(\"en_core_sci_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4101b1-1fa5-4f9f-99cd-bce14b5eb414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Function to safely evaluate string as a list, returning an empty list if the evaluation fails\n",
    "def safe_literal_eval(val):\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Function to ensure proper string handling\n",
    "def safe_string(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    return str(val)\n",
    "\n",
    "# Function to load CSV file and convert 'ner_ines' to a list\n",
    "def load_csv_with_list_column(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['sentence', 'ner_ines'])\n",
    "    # Replace NaN in 'ner_ines' and 'sentence' columns\n",
    "    df['ner_ines'] = df['ner_ines'].replace(np.nan, '[]').apply(safe_literal_eval)\n",
    "    df['sentence'] = df['sentence'].apply(safe_string)\n",
    "    return df\n",
    "\n",
    "# Load your files\n",
    "train_df = load_csv_with_list_column('../data/DICT2ML/D2M_train.csv')\n",
    "dev_df = load_csv_with_list_column('../data/DICT2ML/D2M_dev.csv')\n",
    "test_df = load_csv_with_list_column('../data/DICT2ML/D2M_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc2fcd9-d6b5-4d68-819a-4d17313bcc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ner_ines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GR3027 antagonizes GABAA receptor-potentiating...</td>\n",
       "      <td>[[135, 149, hyperammonemia, DS], [117, 121, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johansson and A.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agusti contributed equally to this work.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hepatic encephalopathy (HE) is one of the prim...</td>\n",
       "      <td>[[67, 82, liver cirrhosis, DS], [0, 22, Hepati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80020</th>\n",
       "      <td>ABSTRACT: Coronavirus disease 2019 (COVID-19) ...</td>\n",
       "      <td>[[10, 34, Coronavirus disease 2019, DS], [36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80021</th>\n",
       "      <td>Bilateral ground-glass or patchy opacity (89.6...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80022</th>\n",
       "      <td>RESULTS: An approximately 1:1 ratio of male (5...</td>\n",
       "      <td>[[63, 71, COVID-19, DS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80023</th>\n",
       "      <td>ABSTRACT: Since late December 2019, an outbrea...</td>\n",
       "      <td>[[51, 82, 2019 novel coronavirus diseases, DS]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80024</th>\n",
       "      <td>It includes data on Australian cases notified ...</td>\n",
       "      <td>[[197, 215, COVID-19 infection, DS]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80025 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      GR3027 antagonizes GABAA receptor-potentiating...   \n",
       "1                                                     M.   \n",
       "2                                       Johansson and A.   \n",
       "3               Agusti contributed equally to this work.   \n",
       "4      Hepatic encephalopathy (HE) is one of the prim...   \n",
       "...                                                  ...   \n",
       "80020  ABSTRACT: Coronavirus disease 2019 (COVID-19) ...   \n",
       "80021  Bilateral ground-glass or patchy opacity (89.6...   \n",
       "80022  RESULTS: An approximately 1:1 ratio of male (5...   \n",
       "80023  ABSTRACT: Since late December 2019, an outbrea...   \n",
       "80024  It includes data on Australian cases notified ...   \n",
       "\n",
       "                                                ner_ines  \n",
       "0      [[135, 149, hyperammonemia, DS], [117, 121, ra...  \n",
       "1                                                     []  \n",
       "2                                                     []  \n",
       "3                                                     []  \n",
       "4      [[67, 82, liver cirrhosis, DS], [0, 22, Hepati...  \n",
       "...                                                  ...  \n",
       "80020  [[10, 34, Coronavirus disease 2019, DS], [36, ...  \n",
       "80021                                                 []  \n",
       "80022                           [[63, 71, COVID-19, DS]]  \n",
       "80023  [[51, 82, 2019 novel coronavirus diseases, DS]...  \n",
       "80024               [[197, 215, COVID-19 infection, DS]]  \n",
       "\n",
       "[80025 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f96d61-0ae9-479e-9f84-b331aadee692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the exact match list for 'GO' entities to be removed (all converted to lowercase)\n",
    "go_entities_to_remove = {\n",
    "    \"uptake\", \"transport\", \"intracellular\", \"fusion\", \"behaviors\", \"complex\",\n",
    "    \"behavior\", \"behavior'\", \"behaviors\", \"behavior”\", \"behaviour\", \"behaviours\",\n",
    "    \"gems\", \"learning\", \"memory\", \"protein bodies\", \"protein body\", \"regeneration\",\n",
    "    \"swimming\", \"translation\", \"transports\", \"tooth replacement\", \"system development\",\n",
    "    \"taste\", \"smoother\", \"sex determination\", \"sex differentiation\", \"pores\"\n",
    "}\n",
    "\n",
    "# Function to clean 'GO' entities from the ner_ines list if they match the removal list (case-insensitive)\n",
    "def clean_go_entities(dataframes):\n",
    "    for df in dataframes:\n",
    "        for index, row in df.iterrows():\n",
    "            cleaned_ner_ines = []\n",
    "            ner_ines = row['ner_ines']\n",
    "            for entity in ner_ines:\n",
    "                if len(entity) == 4:\n",
    "                    _, _, text, entity_type = entity\n",
    "                    # Remove entity if it is 'GO' and its text (lowercased) matches any in the removal list\n",
    "                    if entity_type == \"GO\" and text.lower() in go_entities_to_remove:\n",
    "                        continue\n",
    "                cleaned_ner_ines.append(entity)\n",
    "            # Update the ner_ines column with the cleaned list\n",
    "            df.at[index, 'ner_ines'] = cleaned_ner_ines\n",
    "\n",
    "# Clean 'GO' entities from train, dev, and test datasets\n",
    "clean_go_entities([train_df, dev_df, test_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffe1442-3d5c-485e-808f-1ec558f1d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected EM term 'thermal stability assay' in sentence: 'A second binding assay was employed based upon MBP-E6 unfolding in the Sypro Orange- based thermal stability assay (TSA) [25, 33]. '\n",
      "Detected EM term 'thermal stability assay' in sentence: 'The thermal stability assay [25, 33] was performed with the following modifications. '\n",
      "Detected EM term 'thermal stability assay.' in sentence: 'Summary of the apparent Kd and Bmax values of CAF-25, -26, -27 and CAF-40 with MBP-E6 wild-type and mutant proteins determined by thermal stability assay. '\n",
      "Detected EM term 'Electrophoretic mobility shift assay' in sentence: 'Electrophoretic mobility shift assay (EMSA) of RNA'\n",
      "Detected EM term 'Cold tolerance assays' in sentence: 'Cold tolerance assays'\n",
      "Detected EM term 'cold tolerance assay.' in sentence: 'S3), were selected for cold tolerance assay. '\n",
      "Detected EM term 'microdilution assay' in sentence: 'In brief, a microdilution assay in flat bottomed 96-well microtiter plates (TPP, Trasadingen, Switzerland), using MB as a medium was used. '\n",
      "Detected EM term 'Thermal stability assay' in sentence: 'Thermal stability assay'\n",
      "Detected EM term 'RT-PCR Assays' in sentence: 'Quantitative Real-time RT-PCR Assays for 51 Transcripts Representing Highly Significant Differences between NOA Subclasses'\n",
      "Detected EM term 'RT-PCR assays' in sentence: 'RT-PCR assays excluded the possibility of a drastic transcriptional defect for CDC19, PFK1 or ACT1 genes in the elp3uba4 double mutant, indicating the observed effects result from a disturbance of post-transcriptional events. '\n",
      "Detected EM term 'Luminescent prey assay' in sentence: 'Luminescent prey assay shows less efficient predation by a Bdellovibrio bd0881 knockout strain'\n",
      "Detected EM term 'luminescent prey assay.' in sentence: 'We have previously shown [11] that fliC5 deletion shortens flagella and that ΔfliC5 flagellar mutants swim more slowly and prey less efficiently on E. coli in the luminescent prey assay.'\n",
      "Detected EM term 'luminescent prey assay' in sentence: 'Gene knock-out and luminescent prey assay'\n",
      "Detected EM term 'Luminescent prey assays' in sentence: 'Luminescent prey assays (with E. coli S17-1 containing the plasmid pCL100) were carried out as described elsewhere [9,11] except using a Fluostar Optima machine and the final enumeration data were expressed as Bdellovibrio per E. coli. '\n",
      "Detected EM term 'RT-PCR assay' in sentence: 'A quantitative real time RT-PCR assay indicated that these two genes are expressed in the roots and shoots of seedlings whether they are phosphate-deficient or not.'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the exact match list for 'EM' entities to be removed (all converted to lowercase)\n",
    "em_entities_to_remove = {\n",
    "    \"insertion\", \"recombination\", \"co-expression\", \"recombination'\", \"co-expression\", \"translocations\",\n",
    "    \"homologous recombination\", \"acetylation/deacetylation\", \"maf\", \"itc\", \"sga\", \"iss\", \"chips\",\n",
    "    \"inversion\", \"deacetylation\", \"protein folding\", \"ubiquination\", \"acetylation\", \"translocation\",\n",
    "    \"polymerization\", \"sedation\", \"three-hybrid system\", \"transfection\", \"transfections\", \"transgenic\",\n",
    "    \"translocation\", \"translocations\", \"two-hybrid\", \"urine collection\", \"scraping\", \"insertion\", \n",
    "    \"insertions\", \"inversion\", \"inversions\"\n",
    "}\n",
    "\n",
    "# Define the base terms to look for in phrases that include \"assay\"\n",
    "base_terms_to_check = [\n",
    "    \"luminescent prey\", \"rt-pcr\", \"thermal stability\", \n",
    "    \"electrophoretic mobility shift\", \"cold tolerance\", \"microdilution\"\n",
    "]\n",
    "\n",
    "# Function to clean 'EM' entities from the ner_ines list if they match the removal list (case-insensitive)\n",
    "def clean_em_entities_and_annotate(dataframes):\n",
    "    for df in dataframes:\n",
    "        for index, row in df.iterrows():\n",
    "            cleaned_ner_ines = []\n",
    "            ner_ines = row['ner_ines']\n",
    "            original_sentence = row['sentence']\n",
    "            sentence_lower = original_sentence.lower()  # Convert sentence to lowercase for consistent matching\n",
    "\n",
    "            # Clean existing EM entities\n",
    "            for entity in ner_ines:\n",
    "                if len(entity) == 4:\n",
    "                    _, _, text, entity_type = entity\n",
    "                    # Remove entity if it is 'EM' and its text (lowercased) matches any in the removal list\n",
    "                    if entity_type == \"EM\" and text.lower() in em_entities_to_remove:\n",
    "                        continue\n",
    "                cleaned_ner_ines.append(entity)\n",
    "\n",
    "            # Look for \"assay\" and check preceding words\n",
    "            words = sentence_lower.split()\n",
    "            for i, word in enumerate(words):\n",
    "                if \"assay\" in word:  # Identify the word containing \"assay\"\n",
    "                    # Check up to 3 words before to find a match with our terms\n",
    "                    for n in range(1, 4):\n",
    "                        if i - n < 0:\n",
    "                            break\n",
    "                        preceding_words = \" \".join(words[i - n:i])\n",
    "                        # Match if preceding words form a known base term\n",
    "                        if preceding_words in base_terms_to_check:\n",
    "                            start_index = sentence_lower.find(preceding_words)\n",
    "                            end_index = sentence_lower.find(word, start_index) + len(word)\n",
    "                            full_term = original_sentence[start_index:end_index]\n",
    "                            print(f\"Detected EM term '{full_term}' in sentence: '{original_sentence}'\")\n",
    "\n",
    "                            # Always add the new entity, avoid unnecessary merge checks\n",
    "                            cleaned_ner_ines.append([start_index, end_index, full_term, \"EM\"])\n",
    "                            # break  # Stop after adding to avoid double-counting\n",
    "            \n",
    "            # Update the ner_ines column with the cleaned and potentially extended list\n",
    "            df.at[index, 'ner_ines'] = cleaned_ner_ines\n",
    "\n",
    "# Apply the cleaning and annotating function to train, dev, and test datasets\n",
    "clean_em_entities_and_annotate([train_df, dev_df, test_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1519ec6d-286b-4eba-a92f-06141cda4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to collect entity texts into a dictionary\n",
    "def collect_entity_texts(*dataframes):\n",
    "    entity_dict = defaultdict(set)\n",
    "    for df in dataframes:\n",
    "        for _, row in df.iterrows():\n",
    "            ner_ines = row['ner_ines']\n",
    "            for entity in ner_ines:\n",
    "                if len(entity) == 4:\n",
    "                    start, end, text, entity_type = entity\n",
    "                    entity_dict[entity_type].add(text)\n",
    "    return dict(entity_dict)\n",
    "\n",
    "# Collecting entities from all three datasets\n",
    "entity_texts_dict = collect_entity_texts(train_df, dev_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e006f792-036d-4585-9f4d-8ab2b3bee8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'microdilution assay' in entity_texts_dict['EM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1619aa3d-6276-4d15-9813-7ca13e008f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>ner_ines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GR3027 antagonizes GABAA receptor-potentiating...</td>\n",
       "      <td>[[135, 149, hyperammonemia, DS], [117, 121, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Johansson and A.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agusti contributed equally to this work.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hepatic encephalopathy (HE) is one of the prim...</td>\n",
       "      <td>[[67, 82, liver cirrhosis, DS], [0, 22, Hepati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80020</th>\n",
       "      <td>ABSTRACT: Coronavirus disease 2019 (COVID-19) ...</td>\n",
       "      <td>[[10, 34, Coronavirus disease 2019, DS], [36, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80021</th>\n",
       "      <td>Bilateral ground-glass or patchy opacity (89.6...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80022</th>\n",
       "      <td>RESULTS: An approximately 1:1 ratio of male (5...</td>\n",
       "      <td>[[63, 71, COVID-19, DS]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80023</th>\n",
       "      <td>ABSTRACT: Since late December 2019, an outbrea...</td>\n",
       "      <td>[[51, 82, 2019 novel coronavirus diseases, DS]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80024</th>\n",
       "      <td>It includes data on Australian cases notified ...</td>\n",
       "      <td>[[197, 215, COVID-19 infection, DS]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80025 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0      GR3027 antagonizes GABAA receptor-potentiating...   \n",
       "1                                                     M.   \n",
       "2                                       Johansson and A.   \n",
       "3               Agusti contributed equally to this work.   \n",
       "4      Hepatic encephalopathy (HE) is one of the prim...   \n",
       "...                                                  ...   \n",
       "80020  ABSTRACT: Coronavirus disease 2019 (COVID-19) ...   \n",
       "80021  Bilateral ground-glass or patchy opacity (89.6...   \n",
       "80022  RESULTS: An approximately 1:1 ratio of male (5...   \n",
       "80023  ABSTRACT: Since late December 2019, an outbrea...   \n",
       "80024  It includes data on Australian cases notified ...   \n",
       "\n",
       "                                                ner_ines  \n",
       "0      [[135, 149, hyperammonemia, DS], [117, 121, ra...  \n",
       "1                                                     []  \n",
       "2                                                     []  \n",
       "3                                                     []  \n",
       "4      [[67, 82, liver cirrhosis, DS], [0, 22, Hepati...  \n",
       "...                                                  ...  \n",
       "80020  [[10, 34, Coronavirus disease 2019, DS], [36, ...  \n",
       "80021                                                 []  \n",
       "80022                           [[63, 71, COVID-19, DS]]  \n",
       "80023  [[51, 82, 2019 novel coronavirus diseases, DS]...  \n",
       "80024               [[197, 215, COVID-19 infection, DS]]  \n",
       "\n",
       "[80025 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e96b9a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/stirunag/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Processing batches: 100%|██████████████████████████████████████████████████████████| 161/161 [00:01<00:00, 129.12it/s]\n",
      "Processing batches: 100%|█████████████████████████████████████████████████████████████| 33/33 [00:00<00:00, 93.36it/s]\n",
      "Processing batches: 100%|████████████████████████████████████████████████████████████| 36/36 [00:00<00:00, 130.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "# Ensure NLTK is installed and the tokenizer is available\n",
    "nltk.download('punkt')\n",
    "\n",
    "def find_sub_span(token_span, entity_span):\n",
    "    if token_span[0] < entity_span[1] and token_span[1] > entity_span[0]:\n",
    "        return max(token_span[0], entity_span[0]), min(token_span[1], entity_span[1])\n",
    "    return None\n",
    "\n",
    "def convert_to_iob(texts, ner_tags_list):\n",
    "    results = []\n",
    "\n",
    "    for text, ner_tags in zip(texts, ner_tags_list):\n",
    "        # Tokenize using NLTK's wordpunct_tokenizer\n",
    "        tokens = wordpunct_tokenize(text)\n",
    "        token_spans = []\n",
    "        current_idx = 0\n",
    "\n",
    "        # Calculate token spans based on the original text\n",
    "        for token in tokens:\n",
    "            start_idx = text.find(token, current_idx)\n",
    "            end_idx = start_idx + len(token)\n",
    "            token_spans.append((start_idx, end_idx))\n",
    "            current_idx = end_idx\n",
    "\n",
    "        iob_tags = ['O'] * len(tokens)\n",
    "\n",
    "        for start, end, entity, entity_type in sorted(ner_tags, key=lambda x: x[0]):\n",
    "            entity_flag = False  # Flag to indicate if we are inside an entity\n",
    "            for i, token_span in enumerate(token_spans):\n",
    "                if find_sub_span(token_span, (start, end)):\n",
    "                    if not entity_flag:  # If it's the start of an entity\n",
    "                        iob_tags[i] = 'B-' + entity_type\n",
    "                        entity_flag = True\n",
    "                    elif iob_tags[i] == 'O':  # Continue tagging inside of the entity\n",
    "                        iob_tags[i] = 'I-' + entity_type\n",
    "                else:\n",
    "                    entity_flag = False  # Reset flag when we're no longer in an entity\n",
    "\n",
    "        results.append(list(zip(tokens, iob_tags)))\n",
    "    return results\n",
    "\n",
    "def convert_to_IOB_format_from_df(dataframe, output_folder, filename, batch_size=500):\n",
    "    # Prepare data for batch processing\n",
    "    data = [(row['sentence'], row['ner_ines']) for index, row in dataframe.iterrows()]\n",
    "\n",
    "    pathlib.Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "    result_path = os.path.join(output_folder, filename)\n",
    "\n",
    "    with open(result_path, 'w', newline='\\n') as f1:\n",
    "        train_writer = csv.writer(f1, delimiter='\\t', lineterminator='\\n')\n",
    "\n",
    "        for i in tqdm(range(0, len(data), batch_size), desc=\"Processing batches\"):\n",
    "            batch = data[i:i+batch_size]\n",
    "            sentences, ner_tags_batch = zip(*batch)\n",
    "\n",
    "            # Convert to IOB format\n",
    "            batch_results = convert_to_iob(sentences, ner_tags_batch)\n",
    "\n",
    "            for tagged_tokens in batch_results:\n",
    "                for each_token in tagged_tokens:\n",
    "                    train_writer.writerow(list(each_token))\n",
    "                train_writer.writerow('')\n",
    "\n",
    "# Example usage:\n",
    "output_folder = '../data/DICT2ML/'\n",
    "\n",
    "# Convert train, dev, and test dataframes to IOB format\n",
    "convert_to_IOB_format_from_df(train_df, output_folder, 'train_IOB.tsv')\n",
    "convert_to_IOB_format_from_df(dev_df, output_folder, 'dev_IOB.tsv')\n",
    "convert_to_IOB_format_from_df(test_df, output_folder, 'test_IOB.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19d90801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Dataset 1:\n",
      "Train set 1 is valid.\n",
      "Dev set 1 is valid.\n",
      "Test set 1 is valid.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_iob_file(file_path):\n",
    "    \"\"\"\n",
    "    Function to load an IOB file into a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the IOB file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing tokens and their IOB tags.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if line:  # Only process non-empty lines\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    token, tag = parts\n",
    "                    data.append((token, tag))\n",
    "                # else:\n",
    "                    # print(f\"Warning: Skipping invalid line {line_num} in file {file_path}: '{line}'\")\n",
    "            else:\n",
    "                data.append((\"\", \"\"))  # Add empty line as a sentence boundary\n",
    "    return pd.DataFrame(data, columns=[\"token\", \"tag\"])\n",
    "\n",
    "\n",
    "def check_data_integrity(df):\n",
    "    \"\"\"\n",
    "    Function to check the integrity of NER data.\n",
    "    Ensures:\n",
    "    1. Each 'B-ent' is followed by its respective 'I-ent' (if applicable).\n",
    "    2. No 'I-ent' appears without a preceding 'B-ent'.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the NER columns with tokens and entities.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the data integrity is intact, False if there are issues.\n",
    "    issues (list): List of issues found in the dataset.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    found_b_ent = False\n",
    "\n",
    "    for i, entity in enumerate(df['tag']):\n",
    "        if entity.startswith('B-'):\n",
    "            found_b_ent = True\n",
    "        elif entity.startswith('I-'):\n",
    "            if not found_b_ent:\n",
    "                issues.append(f\"Row: {i}, Hanging I-ent without preceding B-ent.\")\n",
    "            # Check if the current I-ent is the correct continuation of a B-ent\n",
    "            elif not entity[2:] == df['tag'].iloc[i-1][2:]:\n",
    "                issues.append(f\"Row: {i}, Mismatch between B-ent and I-ent.\")\n",
    "        # Reset B-ent flag if it's an 'O', empty string, or sentence boundary\n",
    "        if entity == 'O' or entity == '' or (df['token'].iloc[i] == \"\" and entity == \"\"):\n",
    "            found_b_ent = False\n",
    "\n",
    "    return (True, []) if not issues else (False, issues)\n",
    "\n",
    "def check_integrity_of_files(train_file_paths, dev_file_paths, test_file_paths):\n",
    "    \"\"\"\n",
    "    Check the integrity of NER train, development, and test sets for each dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    train_file_paths (list): List of file paths for training datasets.\n",
    "    dev_file_paths (list): List of file paths for development datasets.\n",
    "    test_file_paths (list): List of file paths for test datasets.\n",
    "    \n",
    "    Returns:\n",
    "    None: Prints the integrity check results for each set.\n",
    "    \"\"\"\n",
    "    for i, (train_file, dev_file, test_file) in enumerate(zip(train_file_paths, dev_file_paths, test_file_paths)):\n",
    "        print(f\"\\nChecking Dataset {i+1}:\")\n",
    "\n",
    "        # Load train set and check integrity\n",
    "        train_df = load_iob_file(train_file)\n",
    "        is_train_valid, train_issues = check_data_integrity(train_df)\n",
    "        if is_train_valid:\n",
    "            print(f\"Train set {i+1} is valid.\")\n",
    "        else:\n",
    "            print(f\"Train set {i+1} has issues:\\n\", \"\\n\".join(train_issues))\n",
    "        \n",
    "        # Load dev set and check integrity\n",
    "        dev_df = load_iob_file(dev_file)\n",
    "        is_dev_valid, dev_issues = check_data_integrity(dev_df)\n",
    "        if is_dev_valid:\n",
    "            print(f\"Dev set {i+1} is valid.\")\n",
    "        else:\n",
    "            print(f\"Dev set {i+1} has issues:\\n\", \"\\n\".join(dev_issues))\n",
    "\n",
    "        # Load test set and check integrity\n",
    "        test_df = load_iob_file(test_file)\n",
    "        is_test_valid, test_issues = check_data_integrity(test_df)\n",
    "        if is_test_valid:\n",
    "            print(f\"Test set {i+1} is valid.\")\n",
    "        else:\n",
    "            print(f\"Test set {i+1} has issues:\\n\", \"\\n\".join(test_issues))\n",
    "\n",
    "# Example usage:\n",
    "train_files = ['../data/DICT2ML/train_IOB.tsv']\n",
    "dev_files = ['../data/DICT2ML/dev_IOB.tsv']\n",
    "test_files = ['../data/DICT2ML/test_IOB.tsv']\n",
    "\n",
    "check_integrity_of_files(train_files, dev_files, test_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d3c91f8-09ab-49d5-a752-6287beb1a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████████████████████████████████████████████████████| 193/193 [00:01<00:00, 132.25it/s]\n",
      "Processing batches: 100%|█████████████████████████████████████████████████████████████| 36/36 [00:00<00:00, 97.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to combine train and dev dataframes, then convert to IOB format\n",
    "def save_combined_to_iob(train_df, dev_df, test_df, output_folder):\n",
    "    # Combine train and dev dataframes\n",
    "    train_dev_df = pd.concat([train_df, dev_df], ignore_index=True)\n",
    "    \n",
    "    # Save combined train_dev as train_dev.tsv\n",
    "    convert_to_IOB_format_from_df(train_dev_df, output_folder, 'train_dev.tsv')\n",
    "    \n",
    "    # Save test as dev.tsv\n",
    "    convert_to_IOB_format_from_df(test_df, output_folder, 'dev.tsv')\n",
    "\n",
    "# Example usage:\n",
    "output_folder = '../data/DICT2ML/'\n",
    "\n",
    "# Assuming train_df, dev_df, and test_df are already loaded\n",
    "save_combined_to_iob(train_df, dev_df, test_df, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fd9d7ec-e04e-4d91-aeee-b844a24461e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking Dataset 1:\n",
      "Train set 1 is valid.\n",
      "Dev set 1 is valid.\n",
      "Test set 1 is valid.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "train_files = ['../data/DICT2ML/train_dev.tsv']\n",
    "dev_files = ['../data/DICT2ML/dev.tsv']\n",
    "check_integrity_of_files(train_files, dev_files, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a673b0ba-c282-4069-908a-ec10afa7dd72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
