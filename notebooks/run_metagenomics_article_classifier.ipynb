{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43bdfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-17 02:50:46.542138: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-17 02:50:46.555903: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 02:50:46.571086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 02:50:46.575695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 02:50:46.589830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 02:50:48.221001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import wandb\n",
    "from huggingface_hub import HfFolder\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "import gzip\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f119ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set the notebook name for W&B\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"article_classifier.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588b0dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsantosh7\u001b[0m (\u001b[33mebi_literature\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "wandb.login()\n",
    "# !wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1ea5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve HF token from environment and authenticate\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    HfFolder.save_token(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454642d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_checkpoints(output_dir, best_model_dir=None, last_model_dir=None):\n",
    "    \"\"\"\n",
    "    Deletes unnecessary model checkpoints created during training.\n",
    "    Keeps the best model directory and optionally the last model directory.\n",
    "\n",
    "    :param output_dir: Base directory where the checkpoints are saved.\n",
    "    :param best_model_dir: Directory of the best model checkpoint.\n",
    "    :param last_model_dir: Directory of the last model checkpoint.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(output_dir):\n",
    "        item_path = os.path.join(output_dir, item)\n",
    "        if os.path.isdir(item_path) and item.startswith(\"checkpoint\"):\n",
    "            # Check if this path is not the one we want to keep\n",
    "            if item_path != best_model_dir and item_path != last_model_dir:\n",
    "                shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbbbcb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label2id):\n",
    "    # Tokenize the abstract (input text)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"abstract\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Convert labels to numeric IDs\n",
    "    labels = examples[\"accession_type\"]\n",
    "    \n",
    "    # Debugging: Print the labels\n",
    "#     print(\"Original label:\", labels)\n",
    "    \n",
    "    # Add the labels to tokenized inputs\n",
    "    tokenized_inputs[\"labels\"] = labels  # Now it's already numeric\n",
    "\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebb587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonlines_to_df(file_path):\n",
    "    with gzip.open(file_path, 'rt') as gzfile:\n",
    "        data = [json.loads(line) for line in gzfile]\n",
    "    \n",
    "    # Create a DataFrame from the JSON Lines data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Ensure 'accession_type' is a list of labels\n",
    "    if 'accession_type' in df.columns:\n",
    "        # Modify the 'accession_type' to keep 'metagenomics' and change all others to 'other'\n",
    "        df['accession_type'] = df['accession_type'].apply(\n",
    "            lambda labels: ['metagenomics' if 'metagenomics' in labels else 'other']\n",
    "        )\n",
    "        \n",
    "        # Convert multi-label rows into single-label rows\n",
    "        df_expanded = pd.DataFrame({\n",
    "            'abstract': df['abstract'].repeat(df['accession_type'].str.len()),  # Repeat each abstract\n",
    "            'accession_type': [str(label) for sublist in df['accession_type'] for label in sublist]  # Flatten labels to single string\n",
    "        })\n",
    "    else:\n",
    "        raise KeyError(\"Column 'accession_type' not found in the dataset.\")\n",
    "\n",
    "    return df_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f05d9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/\"\n",
    "model_save_path = \"/nfs/production/literature/santosh_tirunagari/article_classifier/models/\"\n",
    "pretrained_model = \"bioformers/bioformer-8L\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4980581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_file = os.path.join(data_folder, 'final_multi_label_trainingset.jsonl.gz')\n",
    "# df = load_jsonlines_to_df(train_file)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81593ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_expanded = df\n",
    "# # After transformation, inspect the output\n",
    "# print(df_expanded['accession_type'].value_counts())\n",
    "\n",
    "# # Check the unique labels after processing\n",
    "# unique_labels = df_expanded['accession_type'].unique()\n",
    "# print(\"Unique labels after processing:\", unique_labels)\n",
    "\n",
    "# # Mapping \"metagenomics\" to 1 and \"other\" to 0\n",
    "# df_expanded['accession_type'] = df_expanded['accession_type'].apply(lambda x: 1 if x == 'metagenomics' else 0)\n",
    "\n",
    "# # Check the mapping result\n",
    "# print(df_expanded['accession_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f861786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create label mappings\n",
    "# unique_labels = set(df['accession_type'])\n",
    "# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# # Print mappings for debugging\n",
    "# # print(\"Label to ID mapping:\", label2id)\n",
    "\n",
    "# # Initialize the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "    \n",
    "# # Create label mappings\n",
    "# unique_labels = set(df['accession_type'])\n",
    "# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# # Sample 100 examples per class\n",
    "# df_toy = df.groupby('accession_type').apply(lambda x: x.sample(min(100, len(x)))).reset_index(drop=True)\n",
    "\n",
    "# # Convert labels to numeric IDs (single-label classification)\n",
    "# df_toy['accession_type'] = df_toy['accession_type'].apply(lambda x: label2id.get(x, 'Unknown'))\n",
    "\n",
    "# # Check if any labels were not found\n",
    "# # print(\"Labels after conversion:\", df_toy['accession_type'].unique())\n",
    "\n",
    "# # Convert back to Hugging Face Dataset\n",
    "# dataset = Dataset.from_pandas(df_toy)\n",
    "\n",
    "# # Split dataset into training and validation\n",
    "# train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "# train_dataset = train_test_split['train']\n",
    "# validation_dataset = train_test_split['test']\n",
    "\n",
    "# # Tokenize datasets (using the updated function)\n",
    "# tokenized_train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id), batched=True)\n",
    "# tokenized_val_dataset = validation_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d37368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae3de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute class weights for single-label classification\n",
    "def compute_class_weights(labels, num_labels):\n",
    "    \"\"\"\n",
    "    Computes class weights for single-label classification.\n",
    "    :param labels: A list or array of label IDs.\n",
    "    :param num_labels: The total number of unique labels.\n",
    "    :return: A tensor of class weights.\n",
    "    \"\"\"\n",
    "    # Count the occurrence of each label\n",
    "    class_counts = np.bincount(labels, minlength=num_labels)\n",
    "    \n",
    "    # Compute class weights inversely proportional to the frequency of each class\n",
    "    total_samples = len(labels)\n",
    "    class_weights = total_samples / (num_labels * class_counts)\n",
    "    \n",
    "    # Avoid division by zero in case some classes are missing\n",
    "    class_weights = np.where(class_counts == 0, 0, class_weights)\n",
    "    \n",
    "    # Return class weights as a tensor\n",
    "    return torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights  # Save class weights as part of the trainer\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # CrossEntropyLoss for single-label classification with optional class weights\n",
    "        criterion = CrossEntropyLoss(weight=self.class_weights.to(labels.device) if self.class_weights is not None else None)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b43ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits = p.predictions\n",
    "    labels = p.label_ids\n",
    "    \n",
    "    # Apply softmax to logits to get probabilities for each class\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "    \n",
    "    # Get the predicted class by taking the argmax\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Ensure labels are binary (single 0 or 1 per example)\n",
    "    labels = labels.astype(int)\n",
    "    \n",
    "#     # Print debugging info\n",
    "#     print(\"Logits: \", logits[:5])  # Print first 5 logits\n",
    "#     print(\"Probabilities: \", probs[:5])  # Print first 5 probabilities\n",
    "#     print(\"Predictions (argmax): \", preds[:5])  # Print first 5 predictions after argmax\n",
    "#     print(\"True Labels: \", labels[:5])  # Print first 5 true labels\n",
    "    \n",
    "    # Compute binary classification metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"binary\")\n",
    "    precision = precision_score(labels, preds, average=\"binary\")\n",
    "    recall = recall_score(labels, preds, average=\"binary\")\n",
    "    \n",
    "#     print(\"Accuracy: \", accuracy)\n",
    "#     print(\"F1 Score: \", f1)\n",
    "#     print(\"Precision: \", precision)\n",
    "#     print(\"Recall: \", recall)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "\n",
    "# def compute_metrics(p, id2label=None):\n",
    "#     logits = p.predictions\n",
    "#     labels = p.label_ids\n",
    "\n",
    "#     # Apply sigmoid to logits for binary classification\n",
    "#     preds = (logits > 0).astype(int)\n",
    "#     labels = labels.astype(int)\n",
    "#     # Calculate accuracy, F1, precision, and recall\n",
    "#     accuracy = accuracy_score(labels, preds)\n",
    "#     f1 = f1_score(labels, preds, average=\"binary\")\n",
    "#     precision = precision_score(labels, preds, average=\"binary\")\n",
    "#     recall = recall_score(labels, preds, average=\"binary\")\n",
    "\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy,\n",
    "#         \"f1\": f1,\n",
    "#         \"precision\": precision,\n",
    "#         \"recall\": recall,\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "614a9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_folder, model_checkpoint, config):\n",
    "\n",
    "    # Setup WandB if not already initialized\n",
    "    if not wandb.run:\n",
    "        print(\"Initializing WandB...\")\n",
    "        wandb.init(config=config)  # Call init before accessing config\n",
    "        print(\"WandB initialized.\")\n",
    "    \n",
    "    # Now you can access the config safely\n",
    "    print(\"Accessing WandB config...\")\n",
    "    learning_rate = wandb.config.learning_rate\n",
    "    num_train_epochs = wandb.config.num_train_epochs\n",
    "    train_batch_size = wandb.config.train_batch_size\n",
    "    eval_batch_size = wandb.config.eval_batch_size\n",
    "    weight_decay = wandb.config.weight_decay\n",
    "    print(\"WandB config accessed successfully.\")\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Load dataset from file\n",
    "    train_file = os.path.join(data_folder, 'final_multi_label_trainingset.jsonl.gz')\n",
    "    df = load_jsonlines_to_df(train_file)\n",
    "\n",
    "    # Create label mappings\n",
    "    unique_labels = set(df['accession_type'])\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    # Print mappings for debugging\n",
    "    # print(\"Label to ID mapping:\", label2id)\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    # Create label mappings\n",
    "    unique_labels = set(df['accession_type'])\n",
    "    label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    # Sample 100 examples per class\n",
    "    df_toy = df.groupby('accession_type').apply(lambda x: x.sample(min(100, len(x)))).reset_index(drop=True)\n",
    "\n",
    "    # Convert labels to numeric IDs (single-label classification)\n",
    "    df_toy['accession_type'] = df_toy['accession_type'].apply(lambda x: label2id.get(x, 'Unknown'))\n",
    "\n",
    "    # Check if any labels were not found\n",
    "    # print(\"Labels after conversion:\", df_toy['accession_type'].unique())\n",
    "\n",
    "    # Convert back to Hugging Face Dataset\n",
    "    dataset = Dataset.from_pandas(df_toy)\n",
    "\n",
    "    # Split dataset into training and validation\n",
    "    train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "    train_dataset = train_test_split['train']\n",
    "    validation_dataset = train_test_split['test']\n",
    "\n",
    "    # Tokenize datasets (using the updated function)\n",
    "    tokenized_train_dataset = train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id), batched=True)\n",
    "    tokenized_val_dataset = validation_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer, label2id), batched=True)\n",
    "\n",
    "    # Use sweep name directly for saving the model\n",
    "    run_name = wandb.run.name  # Use only the sweep name and no fallback to run ID\n",
    "    model_save_path = os.path.join(\"./models\", run_name)\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=model_save_path,\n",
    "        evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=train_batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',  # Use the correct metric key\n",
    "        logging_dir=os.path.join(model_save_path, 'logs'),\n",
    "        report_to=\"wandb\"\n",
    "    )\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=len(unique_labels)  # Set the number of labels for the model\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Compute class weights using the training labels before tokenization\n",
    "    train_labels = df_toy['accession_type'].values  # Assuming the labels are in the 'accession_type' column\n",
    "    class_weights = compute_class_weights(train_labels, num_labels=len(label2id))\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        class_weights=class_weights  # Pass the computed class weights\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save models\n",
    "    best_model_path = os.path.join(model_save_path, \"best_model\")\n",
    "    trainer.save_model(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "    last_model_path = os.path.join(model_save_path, \"last_model\")\n",
    "    trainer.save_model(last_model_path)\n",
    "    tokenizer.save_pretrained(last_model_path)\n",
    "\n",
    "    # Ensure cleanup function is defined\n",
    "    cleanup_checkpoints(\n",
    "        output_dir=model_save_path, \n",
    "        best_model_dir=best_model_path, \n",
    "        last_model_dir=last_model_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9acdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'f1',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [5e-5, 2e-5, 1e-5]\n",
    "        },\n",
    "        'train_batch_size': {\n",
    "            'values': [16]\n",
    "        },\n",
    "        'eval_batch_size': {\n",
    "            'values': [16]\n",
    "        },\n",
    "        'num_train_epochs': {\n",
    "            'values': [10]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0.1, 0.01, 0.001]\n",
    "        },\n",
    "        'lr_scheduler_type': {\n",
    "            'values': ['linear']\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 3,\n",
    "        'eta': 2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d46419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: d35kcwrp\n",
      "Sweep URL: https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: foji6hx8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/wandb/run-20241017_025057-foji6hx8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/foji6hx8' target=\"_blank\">laced-sweep-1</a></strong> to <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/foji6hx8' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/foji6hx8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n",
      "Accessing WandB config...\n",
      "WandB config accessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17100/17100 [00:03<00:00, 4530.61 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1900/1900 [00:00<00:00, 4519.53 examples/s]\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bioformers/bioformer-8L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10690' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10690/10690 25:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.364597</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.845361</td>\n",
       "      <td>0.889269</td>\n",
       "      <td>0.805584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.371343</td>\n",
       "      <td>0.857368</td>\n",
       "      <td>0.860668</td>\n",
       "      <td>0.855828</td>\n",
       "      <td>0.865564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.371499</td>\n",
       "      <td>0.854211</td>\n",
       "      <td>0.857876</td>\n",
       "      <td>0.851324</td>\n",
       "      <td>0.864529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.417438</td>\n",
       "      <td>0.853158</td>\n",
       "      <td>0.851359</td>\n",
       "      <td>0.878022</td>\n",
       "      <td>0.826267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215700</td>\n",
       "      <td>0.478734</td>\n",
       "      <td>0.845263</td>\n",
       "      <td>0.839695</td>\n",
       "      <td>0.888120</td>\n",
       "      <td>0.796277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.177900</td>\n",
       "      <td>0.512288</td>\n",
       "      <td>0.843684</td>\n",
       "      <td>0.846986</td>\n",
       "      <td>0.843943</td>\n",
       "      <td>0.850052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.640456</td>\n",
       "      <td>0.826842</td>\n",
       "      <td>0.821098</td>\n",
       "      <td>0.865826</td>\n",
       "      <td>0.780765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.137300</td>\n",
       "      <td>0.671371</td>\n",
       "      <td>0.843684</td>\n",
       "      <td>0.844258</td>\n",
       "      <td>0.856383</td>\n",
       "      <td>0.832472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114400</td>\n",
       "      <td>0.763152</td>\n",
       "      <td>0.836316</td>\n",
       "      <td>0.837258</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.827301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.770569</td>\n",
       "      <td>0.838421</td>\n",
       "      <td>0.837995</td>\n",
       "      <td>0.855603</td>\n",
       "      <td>0.821096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▆█▇▇▅▅▁▅▃▄</td></tr><tr><td>eval/f1</td><td>▅██▆▄▆▁▅▄▄</td></tr><tr><td>eval/loss</td><td>▁▁▁▂▃▄▆▆██</td></tr><tr><td>eval/precision</td><td>█▃▂▆█▁▄▃▂▃</td></tr><tr><td>eval/recall</td><td>▃██▅▂▇▁▅▅▄</td></tr><tr><td>eval/runtime</td><td>▁▆▂▄▇▅▅▂▄█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▇▅▂▄▄▇▅▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▇▅▂▄▄▇▅▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▅▄▄▃▃▃▃▂▂▁▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.83842</td></tr><tr><td>eval/f1</td><td>0.83799</td></tr><tr><td>eval/loss</td><td>0.77057</td></tr><tr><td>eval/precision</td><td>0.8556</td></tr><tr><td>eval/recall</td><td>0.8211</td></tr><tr><td>eval/runtime</td><td>6.0887</td></tr><tr><td>eval/samples_per_second</td><td>312.056</td></tr><tr><td>eval/steps_per_second</td><td>19.545</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>10690</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1017</td></tr><tr><td>train/total_flos</td><td>1.3386934388736e+16</td></tr><tr><td>train/train_loss</td><td>0.21641</td></tr><tr><td>train/train_runtime</td><td>1552.5649</td></tr><tr><td>train/train_samples_per_second</td><td>110.14</td></tr><tr><td>train/train_steps_per_second</td><td>6.885</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-sweep-1</strong> at: <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/foji6hx8' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/foji6hx8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241017_025057-foji6hx8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0katjdis with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/wandb/run-20241017_031719-0katjdis</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/0katjdis' target=\"_blank\">logical-sweep-2</a></strong> to <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/0katjdis' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/0katjdis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n",
      "Accessing WandB config...\n",
      "WandB config accessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17100/17100 [00:04<00:00, 4174.84 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1900/1900 [00:00<00:00, 4476.85 examples/s]\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bioformers/bioformer-8L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10690' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10690/10690 25:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.349420</td>\n",
       "      <td>0.857368</td>\n",
       "      <td>0.853751</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.862595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.360324</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.852222</td>\n",
       "      <td>0.868630</td>\n",
       "      <td>0.836423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.498078</td>\n",
       "      <td>0.855789</td>\n",
       "      <td>0.852846</td>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.865867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.598270</td>\n",
       "      <td>0.842632</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.847191</td>\n",
       "      <td>0.822246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.799251</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>0.787379</td>\n",
       "      <td>0.884406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.844737</td>\n",
       "      <td>0.835471</td>\n",
       "      <td>0.855023</td>\n",
       "      <td>0.816794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.933510</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.837134</td>\n",
       "      <td>0.833514</td>\n",
       "      <td>0.840785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.110263</td>\n",
       "      <td>0.835789</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.829880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.125172</td>\n",
       "      <td>0.834211</td>\n",
       "      <td>0.831280</td>\n",
       "      <td>0.816842</td>\n",
       "      <td>0.846238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>1.121210</td>\n",
       "      <td>0.838421</td>\n",
       "      <td>0.833964</td>\n",
       "      <td>0.827253</td>\n",
       "      <td>0.840785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇█▇▄▁▅▄▃▂▃</td></tr><tr><td>eval/f1</td><td>███▂▂▃▃▁▁▂</td></tr><tr><td>eval/loss</td><td>▁▁▂▃▅▅▆███</td></tr><tr><td>eval/precision</td><td>▆█▆▆▁▇▅▅▄▄</td></tr><tr><td>eval/recall</td><td>▆▃▆▂█▁▃▂▄▃</td></tr><tr><td>eval/runtime</td><td>▆▅▆▃▁▅▅█▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▄▃▆█▄▄▁▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▃▄▃▆█▄▄▁▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.83842</td></tr><tr><td>eval/f1</td><td>0.83396</td></tr><tr><td>eval/loss</td><td>1.12121</td></tr><tr><td>eval/precision</td><td>0.82725</td></tr><tr><td>eval/recall</td><td>0.84079</td></tr><tr><td>eval/runtime</td><td>6.0848</td></tr><tr><td>eval/samples_per_second</td><td>312.251</td></tr><tr><td>eval/steps_per_second</td><td>19.557</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>10690</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.013</td></tr><tr><td>train/total_flos</td><td>1.3386934388736e+16</td></tr><tr><td>train/train_loss</td><td>0.13927</td></tr><tr><td>train/train_runtime</td><td>1551.5042</td></tr><tr><td>train/train_samples_per_second</td><td>110.216</td></tr><tr><td>train/train_steps_per_second</td><td>6.89</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-2</strong> at: <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/0katjdis' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/0katjdis</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241017_031719-0katjdis/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nwrcipu1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/wandb/run-20241017_034340-nwrcipu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/nwrcipu1' target=\"_blank\">confused-sweep-3</a></strong> to <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/nwrcipu1' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/nwrcipu1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n",
      "Accessing WandB config...\n",
      "WandB config accessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17100/17100 [00:03<00:00, 4725.12 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1900/1900 [00:00<00:00, 4569.46 examples/s]\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bioformers/bioformer-8L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10690' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10690/10690 25:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.349420</td>\n",
       "      <td>0.857368</td>\n",
       "      <td>0.853751</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.862595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318400</td>\n",
       "      <td>0.360324</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.852222</td>\n",
       "      <td>0.868630</td>\n",
       "      <td>0.836423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.227000</td>\n",
       "      <td>0.498078</td>\n",
       "      <td>0.855789</td>\n",
       "      <td>0.852846</td>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.865867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>0.598270</td>\n",
       "      <td>0.842632</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.847191</td>\n",
       "      <td>0.822246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.799251</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.833077</td>\n",
       "      <td>0.787379</td>\n",
       "      <td>0.884406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.834128</td>\n",
       "      <td>0.844737</td>\n",
       "      <td>0.835471</td>\n",
       "      <td>0.855023</td>\n",
       "      <td>0.816794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.933510</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.837134</td>\n",
       "      <td>0.833514</td>\n",
       "      <td>0.840785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>1.110263</td>\n",
       "      <td>0.835789</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.829880</td>\n",
       "      <td>0.829880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.125172</td>\n",
       "      <td>0.834211</td>\n",
       "      <td>0.831280</td>\n",
       "      <td>0.816842</td>\n",
       "      <td>0.846238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>1.121210</td>\n",
       "      <td>0.838421</td>\n",
       "      <td>0.833964</td>\n",
       "      <td>0.827253</td>\n",
       "      <td>0.840785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▇█▇▄▁▅▄▃▂▃</td></tr><tr><td>eval/f1</td><td>███▂▂▃▃▁▁▂</td></tr><tr><td>eval/loss</td><td>▁▁▂▃▅▅▆███</td></tr><tr><td>eval/precision</td><td>▆█▆▆▁▇▅▅▄▄</td></tr><tr><td>eval/recall</td><td>▆▃▆▂█▁▃▂▄▃</td></tr><tr><td>eval/runtime</td><td>▆█▇▇▁▂▄▇▄▄</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▂▂█▇▅▂▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▃▁▂▂█▇▅▂▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.83842</td></tr><tr><td>eval/f1</td><td>0.83396</td></tr><tr><td>eval/loss</td><td>1.12121</td></tr><tr><td>eval/precision</td><td>0.82725</td></tr><tr><td>eval/recall</td><td>0.84079</td></tr><tr><td>eval/runtime</td><td>6.0587</td></tr><tr><td>eval/samples_per_second</td><td>313.599</td></tr><tr><td>eval/steps_per_second</td><td>19.641</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>10690</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.013</td></tr><tr><td>train/total_flos</td><td>1.3386934388736e+16</td></tr><tr><td>train/train_loss</td><td>0.13927</td></tr><tr><td>train/train_runtime</td><td>1544.2859</td></tr><tr><td>train/train_samples_per_second</td><td>110.731</td></tr><tr><td>train/train_steps_per_second</td><td>6.922</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">confused-sweep-3</strong> at: <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/nwrcipu1' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/nwrcipu1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241017_034340-nwrcipu1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jd65lsqd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/wandb/run-20241017_040950-jd65lsqd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/jd65lsqd' target=\"_blank\">polar-sweep-4</a></strong> to <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/jd65lsqd' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/jd65lsqd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n",
      "Accessing WandB config...\n",
      "WandB config accessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17100/17100 [00:03<00:00, 4356.11 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1900/1900 [00:00<00:00, 4643.19 examples/s]\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bioformers/bioformer-8L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10690' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10690/10690 25:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.358406</td>\n",
       "      <td>0.853684</td>\n",
       "      <td>0.852442</td>\n",
       "      <td>0.830403</td>\n",
       "      <td>0.875682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.344378</td>\n",
       "      <td>0.861579</td>\n",
       "      <td>0.858374</td>\n",
       "      <td>0.847872</td>\n",
       "      <td>0.869138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.288300</td>\n",
       "      <td>0.347443</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.862983</td>\n",
       "      <td>0.851690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.253100</td>\n",
       "      <td>0.379978</td>\n",
       "      <td>0.858947</td>\n",
       "      <td>0.856684</td>\n",
       "      <td>0.840504</td>\n",
       "      <td>0.873501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.484271</td>\n",
       "      <td>0.853158</td>\n",
       "      <td>0.852770</td>\n",
       "      <td>0.826176</td>\n",
       "      <td>0.881134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.522527</td>\n",
       "      <td>0.854211</td>\n",
       "      <td>0.850513</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.859324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.598832</td>\n",
       "      <td>0.846316</td>\n",
       "      <td>0.830626</td>\n",
       "      <td>0.887237</td>\n",
       "      <td>0.780807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.660365</td>\n",
       "      <td>0.846842</td>\n",
       "      <td>0.838960</td>\n",
       "      <td>0.851685</td>\n",
       "      <td>0.826609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.735311</td>\n",
       "      <td>0.848421</td>\n",
       "      <td>0.843308</td>\n",
       "      <td>0.841477</td>\n",
       "      <td>0.845147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.743210</td>\n",
       "      <td>0.848947</td>\n",
       "      <td>0.842912</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.839695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▄▇█▆▄▄▁▁▂▂</td></tr><tr><td>eval/f1</td><td>▇███▇▆▁▃▄▄</td></tr><tr><td>eval/loss</td><td>▁▁▁▂▃▄▅▇██</td></tr><tr><td>eval/precision</td><td>▁▃▅▃▁▃█▄▃▃</td></tr><tr><td>eval/recall</td><td>█▇▆▇█▆▁▄▅▅</td></tr><tr><td>eval/runtime</td><td>▆▆██▅▁▂▂██</td></tr><tr><td>eval/samples_per_second</td><td>▃▃▁▁▄█▇▇▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▃▃▁▁▄█▇▇▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▆▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▄▄▄▃▃▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.84895</td></tr><tr><td>eval/f1</td><td>0.84291</td></tr><tr><td>eval/loss</td><td>0.74321</td></tr><tr><td>eval/precision</td><td>0.84615</td></tr><tr><td>eval/recall</td><td>0.83969</td></tr><tr><td>eval/runtime</td><td>6.0761</td></tr><tr><td>eval/samples_per_second</td><td>312.703</td></tr><tr><td>eval/steps_per_second</td><td>19.585</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>10690</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1004</td></tr><tr><td>train/total_flos</td><td>1.3386934388736e+16</td></tr><tr><td>train/train_loss</td><td>0.21454</td></tr><tr><td>train/train_runtime</td><td>1546.1369</td></tr><tr><td>train/train_samples_per_second</td><td>110.598</td></tr><tr><td>train/train_steps_per_second</td><td>6.914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-4</strong> at: <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/jd65lsqd' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/jd65lsqd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241017_040950-jd65lsqd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gqiggbt1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \teval_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr_scheduler_type: linear\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_train_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find article_classifier.ipynb.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/production/literature/santosh_tirunagari/BACKUP/work/github/metagenomics/notebooks/wandb/run-20241017_043601-gqiggbt1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/gqiggbt1' target=\"_blank\">crisp-sweep-5</a></strong> to <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ebi_literature/single_article_classifier' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/sweeps/d35kcwrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ebi_literature/single_article_classifier/runs/gqiggbt1' target=\"_blank\">https://wandb.ai/ebi_literature/single_article_classifier/runs/gqiggbt1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized.\n",
      "Accessing WandB config...\n",
      "WandB config accessed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17100/17100 [00:03<00:00, 4396.74 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1900/1900 [00:00<00:00, 4715.36 examples/s]\n",
      "/nfs/production/literature/santosh_tirunagari/transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bioformers/bioformer-8L and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_train_epochs' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lr_scheduler_type' was locked by 'sweep' (ignored update).\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2261' max='10690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2261/10690 05:26 < 20:17, 6.92 it/s, Epoch 2.11/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374200</td>\n",
       "      <td>0.358406</td>\n",
       "      <td>0.853684</td>\n",
       "      <td>0.852442</td>\n",
       "      <td>0.830403</td>\n",
       "      <td>0.875682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.344378</td>\n",
       "      <td>0.861579</td>\n",
       "      <td>0.858374</td>\n",
       "      <td>0.847872</td>\n",
       "      <td>0.869138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"single_article_classifier\", entity=\"ebi_literature\")\n",
    "wandb.agent(sweep_id, lambda: train_model(data_folder, pretrained_model, wandb.config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96faddba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
