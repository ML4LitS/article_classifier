{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930f199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_sci_sm' (0.5.4) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.7.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:2170: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f33714c7340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "import spacy\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd4533d-3318-4d0a-8c3a-33b7c288e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile regex patterns for section tagging\n",
    "titleMapsBody = {\n",
    "    'INTRO': [\n",
    "        'introduction', 'background', 'related literature', 'literature review', 'objective',\n",
    "        'aim ', 'purpose of this study', 'study (purpose|aim|aims)', r'\\d+\\. (purpose|aims|aim)',\n",
    "        '(aims|aim|purpose) of the study', '(the|drug|systematic|book) review', 'review of literature',\n",
    "        'related work', 'recent advance', 'overview', 'i ntroduction', 'historical overview',\n",
    "        'scope', 'context', 'rationale', 'hypothesis', 'motivation', 'i ntroduction', 'i ntro', 'i n t r o d u c t i o n'\n",
    "    ],\n",
    "    'METHODS': [\n",
    "        'supplement', 'methods and materials', 'method', 'material', 'experimental procedure',\n",
    "        'implementation', 'methodology', 'treatment', 'statistical analysis', \"experimental\",\n",
    "        r'\\d+\\. experimental$', 'experimental (section|evaluation|design|approach|protocol|setting|set up|investigation|detail|part|perspective|tool)',\n",
    "        \"the study\", r'\\d+\\. the study$', \"protocol\", \"protocols\", 'study protocol',\n",
    "        'construction and content', r'experiment \\d+', '^experiments$', 'analysis', 'utility',\n",
    "        'design', r'\\d+\\. theory$', \"theory\", 'theory and ', 'theory of ',\n",
    "        'data analysis', 'data collection', 'methodological approach', 'techniques', 'sample',\n",
    "        'materials and methods', 'analytical methods', 'research methods', 'methodological framework',\n",
    "        'm aterials and m ethods', 'm a t e r i a l s a n d m e t h o d s', 'm ethods'\n",
    "    ],\n",
    "    'RESULTS': [\n",
    "        'result', 'finding', 'diagnosis', 'outcomes', 'findings', 'observations',\n",
    "        'key results', 'main results', 'data', 'analysis results', 'primary results',\n",
    "        'research findings', 'experimental results', 'empirical findings', 'report of results',\n",
    "        'r esults', 'r e s u l t s'\n",
    "    ],\n",
    "    'DISCUSS': [\n",
    "        'discussion', 'management of', r'\\d+\\. management', 'safety and tolerability',\n",
    "        'limitations', 'perspective', 'commentary', r'\\d+\\. comment', 'interpretation',\n",
    "        'interpretation of results', 'analysis of findings', 'discussion and implications',\n",
    "        'contextualization', 'reflection', 'critical analysis', 'discussion and future work',\n",
    "        'insights', 'consideration', 'comparison with previous studies', 'd iscussion', 'd i s c u s s i o n'\n",
    "    ],\n",
    "    'CONCL': [\n",
    "        'conclusion', 'key message', 'future', 'summary', 'recommendation',\n",
    "        'implications for clinical practice', 'concluding remark', 'closing remarks',\n",
    "        'takeaway', 'final remarks', 'overall conclusion', 'summary and conclusion',\n",
    "        'implications', 'closing statement', 'wrap-up', 'summary of findings',\n",
    "        'future directions', 'outlook', 'next steps', 'c onclusion', 'c o n c l u s i o n'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        'case study report', 'case report', 'case presentation', 'case description',\n",
    "        r'case \\d+', r'\\d+\\. case', 'case summary', 'case history', 'case overview',\n",
    "        'case study', 'case examination', 'case details', 'case documentation',\n",
    "        'case example', 'case profile', 'c ase', 'c a s e'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'financial disclosure',\n",
    "        'funding sources', 'funding support', 'financial support', 'grant support',\n",
    "        'grant acknowledgement', 'acknowledgement of funding', 'funder', 'acknowledgements',\n",
    "        'a c k n o w l e d g e m e n t', 'a c k f u n d'\n",
    "    ],\n",
    "    'AUTH_CONT': [\n",
    "        \"author contribution\", \"authors' contribution\", \"author's contribution\",\n",
    "        \"contribution of authors\", \"authors' roles\", \"author responsibilities\", \"authorship contributions\",\n",
    "        'a u t h o r c o n t r i b u t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'competing interests', 'conflict statement',\n",
    "        'financial conflicts', 'competing financial interests', 'c o m p i n t'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'abbreviations list', 'acronyms', 'nomenclature',\n",
    "        'glossary', 'terms', 'terminology', 'abbreviation glossary', 'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'SUPPL': [\n",
    "        'supplemental data', 'supplementary file', 'supplemental file', 'supplementary data',\n",
    "        'supplementary figure', 'supplemental figure', 'supporting information',\n",
    "        'supplemental file', 'supplemental material', 'supplementary material',\n",
    "        'supplement material', 'additional data files', 'supplemental information',\n",
    "        'supplementary information', 'supporting files', 'appendix', 'online appendix',\n",
    "        'supporting documentation', 'extra data', 'additional material', 'annex',\n",
    "        's u p p l e m e n t', 's u p p l e m e n t a r y'\n",
    "    ]\n",
    "}\n",
    "\n",
    "titleExactMapsBody = {\n",
    "    'INTRO': [\n",
    "        \"aim\", \"aims\", \"purpose\", \"purposes\", \"purpose/aim\",\n",
    "        \"purpose of study\", \"review\", \"reviews\", \"minireview\", \"overview\", \"background\",\n",
    "        'i n t r o d u c t i o n', 'intro'\n",
    "    ],\n",
    "    'METHODS': [\n",
    "        \"experimental\", \"the study\", \"protocol\", \"protocols\", \"procedure\", \"methodology\", \"data analysis\",\n",
    "        'm e t h o d s', 'methods'\n",
    "    ],\n",
    "    'DISCUSS': [\n",
    "        \"management\", \"comment\", \"comments\", \"discussion\", \"limitations\", \"perspectives\",\n",
    "        'd i s c u s s', 'discussion'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        \"case\", \"cases\", \"case study\", \"case report\", \"case overview\", 'case'\n",
    "    ]\n",
    "}\n",
    "\n",
    "titleMapsBack = {\n",
    "    'REF': [\n",
    "        'reference', 'literature cited', 'references', 'bibliography', 'source list', 'citations',\n",
    "        'works cited', 'cited literature', 'bibliographical references', 'citations list',\n",
    "        'r e f e r e n c e s'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'acknowlegement',\n",
    "        'acknowlegement', 'open access', 'financial support', 'grant',\n",
    "        'author note', 'financial disclosure', 'support statement', 'funding acknowledgment',\n",
    "        'a c k n o w l e d g e'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'glossary', 'abbreviations list', 'acronyms', 'terminology', 'abbreviation glossary',\n",
    "        'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'conflicts', 'interest', 'financial conflicts',\n",
    "        'c o m p i n t'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        'case study report', 'case report', 'case presentation', 'case description',\n",
    "        r'case \\d+', r'\\d+\\. case', 'case summary', 'case history', 'case overview',\n",
    "        'case study', 'case examination', 'case details', 'case documentation',\n",
    "        'case example', 'case profile', 'c ase', 'c a s e'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'financial disclosure',\n",
    "        'funding sources', 'funding support', 'financial support', 'grant support',\n",
    "        'grant acknowledgement', 'acknowledgement of funding', 'funder', 'acknowledgements',\n",
    "        'a c k n o w l e d g e m e n t', 'a c k f u n d'\n",
    "    ],\n",
    "    'AUTH_CONT': [\n",
    "        \"author contribution\", \"authors' contribution\", \"author's contribution\",\n",
    "        \"contribution of authors\", \"authors' roles\", \"author responsibilities\", \"authorship contributions\",\n",
    "        'a u t h o r c o n t r i b u t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'competing interests', 'conflict statement',\n",
    "        'financial conflicts', 'competing financial interests', 'c o m p i n t'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'abbreviations list', 'acronyms', 'nomenclature',\n",
    "        'glossary', 'terms', 'terminology', 'abbreviation glossary', 'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'SUPPL': [\n",
    "        'supplemental data', 'supplementary file', 'supplemental file', 'supplementary data',\n",
    "        'supplementary figure', 'supplemental figure', 'supporting information',\n",
    "        'supplemental file', 'supplemental material', 'supplementary material',\n",
    "        'supplement material', 'additional data files', 'supplemental information',\n",
    "        'supplementary information', 'supporting files', 'appendix', 'online appendix',\n",
    "        'supporting documentation', 'extra data', 'additional material', 'annex',\n",
    "        's u p p l e m e n t', 's u p p l e m e n t a r y'\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a495bc67-81b6-4922-83bf-06242ce46418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2dec99d-c14c-4a67-a394-c9d38cb4b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile regex patterns\n",
    "compiled_titleMapsBody = {\n",
    "    key: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]\n",
    "    for key, patterns in titleMapsBody.items()\n",
    "}\n",
    "\n",
    "compiled_titleExactMapsBody = {\n",
    "    key: [pattern.lower() for pattern in patterns]\n",
    "    for key, patterns in titleExactMapsBody.items()\n",
    "}\n",
    "\n",
    "compiled_titleMapsBack = {\n",
    "    key: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]\n",
    "    for key, patterns in titleMapsBack.items()\n",
    "}\n",
    "\n",
    "def createSecTag(soup, secType):\n",
    "    secTag = soup.new_tag('SecTag')\n",
    "    secTag['type'] = secType\n",
    "    return secTag\n",
    "\n",
    "# Function to read XML or GZ files and split into individual articles\n",
    "def getfileblocks(file_path, document_flag):\n",
    "    sub_file_blocks = []\n",
    "    if file_path.endswith('.gz'):\n",
    "        open_func = lambda x: gzip.open(x, 'rt', encoding='utf8')\n",
    "    else:\n",
    "        open_func = lambda x: open(x, 'r', encoding='utf8')\n",
    "\n",
    "    try:\n",
    "        with open_func(file_path) as fh:\n",
    "            content = fh.read()\n",
    "            if document_flag in ['f', 'a']:\n",
    "                # Split content by <!DOCTYPE article ...> or <article ...> tags\n",
    "                articles = re.split(r'(?=<!DOCTYPE article|<article(?![\\w-]))', content)\n",
    "                sub_file_blocks = [article.strip() for article in articles if article.strip() and '<!DOCTYPE' not in article]\n",
    "            else:\n",
    "                print('ERROR: unknown document type :' + document_flag)\n",
    "    except Exception as e:\n",
    "        print('Error processing file: ' + str(file_path))\n",
    "        print(e)\n",
    "\n",
    "    return sub_file_blocks\n",
    "\n",
    "# Function to split text into sentences using spaCy\n",
    "def sentence_split(text):\n",
    "    sentences = []\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        sentences.append(sent.text.strip())\n",
    "    return sentences\n",
    "\n",
    "# Function to process nested tags and collect sentences\n",
    "def call_sentence_tags(ch):\n",
    "    sentences = []\n",
    "    for gch in ch.children:\n",
    "        if isinstance(gch, str):\n",
    "            continue  # Skip strings directly under ch\n",
    "        if gch.name in ['article-title', 'title', 'subtitle', 'trans-title', 'trans-subtitle', 'alt-title', 'label', 'td', 'th']:\n",
    "            if gch.find('p', recursive=False):\n",
    "                sub_sentences = call_sentence_tags(gch)\n",
    "                sentences.extend(sub_sentences)\n",
    "            else:\n",
    "                text = gch.get_text(separator=' ', strip=True)\n",
    "                if text:\n",
    "                    sents = sentence_split(text)\n",
    "                    sentences.extend(sents)\n",
    "        elif gch.name in [\"sec\", \"fig\", \"statement\", \"div\", \"boxed-text\", \"list\", \"list-item\", \"disp-quote\", \"speech\",\n",
    "                          \"fn-group\", \"fn\", \"def-list\", \"def-item\", \"def\", \"ack\", \"array\", \"table-wrap\", \"table\",\n",
    "                          \"tbody\", \"thead\", \"tr\", \"caption\", \"answer\", \"sec-meta\", \"glossary\", \"question\", \"question-wrap\"]:\n",
    "            sub_sentences = call_sentence_tags(gch)\n",
    "            sentences.extend(sub_sentences)\n",
    "        elif gch.name == 'p':\n",
    "            sub_sentences = process_p_tag(gch)\n",
    "            sentences.extend(sub_sentences)\n",
    "        else:\n",
    "            text = gch.get_text(separator=' ', strip=True)\n",
    "            if text:\n",
    "                sents = sentence_split(text)\n",
    "                sentences.extend(sents)\n",
    "    return sentences\n",
    "\n",
    "# Function to process paragraph tags\n",
    "def process_p_tag(gch):\n",
    "    sentences = []\n",
    "    if not (len(gch.contents) == 1 and (not gch.contents[0].string) and (gch.contents[0].name in [\"ext-link\", \"e-mail\", \"uri\", \"inline-supplementary-material\", \"related-article\", \"related-object\", \"address\", \"alternatives\", \"array\", \"funding-source\", \"inline-graphic\"])):\n",
    "        text = gch.get_text(separator=' ', strip=True)\n",
    "        if text:\n",
    "            sents = sentence_split(text)\n",
    "            sentences.extend(sents)\n",
    "    return sentences\n",
    "\n",
    "# Function to process the front section\n",
    "def process_front(front):\n",
    "    sections = {}\n",
    "    keywords = []\n",
    "    \n",
    "    if front.find('article-meta'):\n",
    "        art_meta = front.find('article-meta')\n",
    "        \n",
    "        for ch in art_meta.find_all(recursive=False):\n",
    "            if ch.name in ['title-group', 'supplement', 'supplementary-material', 'abstract', 'trans-abstract', 'kwd-group', 'funding-group']:\n",
    "                section_title = ch.name.upper()\n",
    "                \n",
    "                if section_title == 'KWD-GROUP':\n",
    "                    # Extract keywords as a list from kwd-group\n",
    "                    keywords = [kwd.text.strip() for kwd in ch.find_all('kwd')]\n",
    "                else:\n",
    "                    sentences = call_sentence_tags(ch)\n",
    "                    if sentences:\n",
    "                        sections.setdefault(section_title, []).extend(sentences)\n",
    "            else:\n",
    "                pass  # Ignore other tags\n",
    "    \n",
    "    return sections, keywords\n",
    "\n",
    "# Function to process the back section\n",
    "def process_back(back):\n",
    "    sections = {}\n",
    "    for ch in back.find_all(recursive=False):\n",
    "        if ch.name in ['sec', 'p', 'ack', 'alternatives', 'array', 'preformat', 'fig', 'fig-group', 'question-wrap',\n",
    "                 'question-wrap-group', 'list', 'table-wrap-group', 'table-wrap', 'display-formula',\n",
    "                 'display-formula-group', 'def-list', 'list', 'supplementary-material', 'kwd-group',\n",
    "                 'funding-group', 'statement', 'ref-list', 'glossary']:\n",
    "            # Sections with titles\n",
    "            if ch.name == 'ref-list':\n",
    "                sentences = reference_sents(ch)\n",
    "                if sentences:\n",
    "                    sections.setdefault('REF', []).extend(sentences)\n",
    "            else:\n",
    "                title = ch.find('title')\n",
    "                if title:\n",
    "                    section_title = title.get_text(separator=' ', strip=True).strip().upper()\n",
    "                else:\n",
    "                    section_title = ch.name.upper()\n",
    "                sentences = call_sentence_tags(ch)\n",
    "                if sentences:\n",
    "                    sections.setdefault(section_title, []).extend(sentences)\n",
    "        else:\n",
    "            pass  # Ignore other tags\n",
    "    return sections\n",
    "\n",
    "# Function to process reference sentences\n",
    "def reference_sents(ref_list):\n",
    "    sentences = []\n",
    "    for ch in ref_list.children:\n",
    "        if isinstance(ch, str):\n",
    "            continue  # Skip strings directly under ref_list\n",
    "        if ch.name == 'ref':\n",
    "            sub_text = ''\n",
    "            for gch in ch.children:\n",
    "                if isinstance(gch, str):\n",
    "                    continue\n",
    "                sub_text += \" \" + \" \".join([d.string for d in gch.descendants if d.string])\n",
    "            if sub_text:\n",
    "                sents = sentence_split(sub_text)\n",
    "                sentences.extend(sents)\n",
    "        elif ch.name in [\"sec\", \"fig\", \"statement\", \"div\", \"boxed-text\", \"list\", \"list-item\", \"disp-quote\", \"speech\",\n",
    "                         \"fn-group\", \"fn\", \"def-list\", \"def-item\", \"def\", \"ack\", \"array\", \"table-wrap\", \"table\",\n",
    "                         \"tbody\", \"caption\", \"answer\", \"sec-meta\", \"glossary\", \"question\", \"question-wrap\"]:\n",
    "            sub_sentences = call_sentence_tags(ch)\n",
    "            sentences.extend(sub_sentences)\n",
    "        else:\n",
    "            pass  # Ignore other tags\n",
    "    return sentences\n",
    "\n",
    "# Function to match section titles to predefined section types\n",
    "def titleMatch(title, secFlag):\n",
    "    matchKeys = []\n",
    "    # Check if the flag is 'body' or 'back' and apply the respective mappings\n",
    "    if secFlag == 'body':\n",
    "        titleMaps = compiled_titleMapsBody\n",
    "        exactMaps = compiled_titleExactMapsBody\n",
    "    else:\n",
    "        titleMaps = compiled_titleMapsBack\n",
    "        exactMaps = {}\n",
    "\n",
    "    title_lower = title.lower().strip()\n",
    "    # Check exact matches first\n",
    "    for key, patterns in exactMaps.items():\n",
    "        if title_lower in patterns:\n",
    "            matchKeys.append(key)\n",
    "            break  # If exact match found, no need to check further\n",
    "\n",
    "    # If no exact match, check regex patterns\n",
    "    if not matchKeys:\n",
    "        for key, patterns in titleMaps.items():\n",
    "            if any(pattern.search(title_lower) for pattern in patterns):\n",
    "                matchKeys.append(key)\n",
    "\n",
    "    return ','.join(matchKeys) if matchKeys else None\n",
    "\n",
    "# Function to apply section tagging to the soup object\n",
    "def section_tag(soup):\n",
    "    # Add Figure sections\n",
    "    for fig in soup.find_all('fig', recursive=True):\n",
    "        if not fig.find_all('fig', recursive=True):\n",
    "            fig_tag = createSecTag(soup, 'FIG')\n",
    "            fig.wrap(fig_tag)\n",
    "    \n",
    "    # Add Table sections\n",
    "    for table in soup.find_all('table-wrap', recursive=True):\n",
    "        if not table.find_all('table-wrap', recursive=True):\n",
    "            table_tag = createSecTag(soup, 'TABLE')\n",
    "            table.wrap(table_tag)\n",
    "\n",
    "    # Process front section\n",
    "    if soup.front:\n",
    "        if soup.front.abstract:\n",
    "            secAbs = createSecTag(soup, 'ABSTRACT')\n",
    "            soup.front.abstract.wrap(secAbs)\n",
    "        if soup.front.find('kwd-group'):\n",
    "            secKwd = createSecTag(soup, 'KEYWORD')\n",
    "            soup.front.find('kwd-group').wrap(secKwd)\n",
    "\n",
    "    # Process body section\n",
    "    if soup.body:\n",
    "        for sec in soup.body.find_all('sec', recursive=False):\n",
    "            title = sec.find('title')\n",
    "            if title:\n",
    "                title_text = title.get_text(separator=' ', strip=True)\n",
    "                mappedTitle = titleMatch(title_text, 'body')\n",
    "                if mappedTitle:\n",
    "                    secBody = createSecTag(soup, mappedTitle)\n",
    "                    sec.wrap(secBody)\n",
    "    # Process back sections\n",
    "    if soup.back:\n",
    "        for sec in soup.back.find_all(['sec', 'ref-list', 'app-group', 'ack', 'glossary', 'notes', 'fn-group'], recursive=False):\n",
    "            if sec.name == 'ref-list':\n",
    "                secRef = createSecTag(soup, 'REF')\n",
    "                sec.wrap(secRef)\n",
    "            else:\n",
    "                title = sec.find('title')\n",
    "                if title:\n",
    "                    title_text = title.get_text(separator=' ', strip=True)\n",
    "                    mappedTitle = titleMatch(title_text, 'back')\n",
    "                    if mappedTitle:\n",
    "                        secBack = createSecTag(soup, mappedTitle)\n",
    "                        sec.wrap(secBack)\n",
    "\n",
    "# Function to process the body section\n",
    "def process_body(body):\n",
    "    sections = {}\n",
    "    for ch in body.find_all(recursive=False):\n",
    "        if ch.name == 'p':\n",
    "            sentences = process_p_tag(ch)\n",
    "            sections.setdefault('BODY', []).extend(sentences)\n",
    "        elif ch.name in ['sec', 'ack', 'alternatives', 'array', 'preformat', 'fig', 'fig-group', 'question-wrap', 'list', 'table-wrap-group', 'table-wrap', 'display-formula', 'display-formula-group', 'def-list', 'list', 'supplementary-material', 'kwd-group', 'funding-group', 'statement']:\n",
    "            title = ch.find('title')\n",
    "            if title:\n",
    "                section_title = title.get_text(separator=' ', strip=True).strip().upper()\n",
    "            else:\n",
    "                section_title = ch.name.upper()\n",
    "            sentences = call_sentence_tags(ch)\n",
    "            if sentences:\n",
    "                sections.setdefault(section_title, []).extend(sentences)\n",
    "    return sections\n",
    "\n",
    "# Main function to process each article and collect data\n",
    "def process_full_text(each_file):\n",
    "\n",
    "    # Replace body tag with orig_body to prevent BeautifulSoup from removing it\n",
    "    each_file = re.sub(r'<body(\\s[^>]*)?>', '<orig_body\\\\1>', each_file)\n",
    "    each_file = each_file.replace('</body>', '</orig_body>')\n",
    "    try:\n",
    "        xml_soup = BeautifulSoup(each_file, 'lxml')\n",
    "        # Remove extra html and body tags added by BeautifulSoup\n",
    "        if xml_soup.html:\n",
    "            xml_soup.html.unwrap()\n",
    "        if xml_soup.body:\n",
    "            xml_soup.body.unwrap()\n",
    "        if xml_soup.find('orig_body'):\n",
    "            xml_soup.find('orig_body').name = 'body'\n",
    "\n",
    "        # Extract attributes from the <article> tag\n",
    "        article_tag = xml_soup.find('article')\n",
    "        if article_tag:\n",
    "            open_status = article_tag.get('open-status', '')\n",
    "            article_type = article_tag.get('article-type', '')\n",
    "        else:\n",
    "            open_status = ''\n",
    "            article_type = ''\n",
    "\n",
    "        # Extract article IDs\n",
    "        article_ids = {}\n",
    "        for id_tag in xml_soup.find_all('article-id'):\n",
    "            id_type = id_tag.get('pub-id-type', 'unknown')\n",
    "            article_ids[id_type] = id_tag.text.strip()\n",
    "        if not article_ids:\n",
    "            print('No article IDs found')\n",
    "            return None\n",
    "\n",
    "        # Apply section tagging\n",
    "        section_tag(xml_soup)\n",
    "        \n",
    "        sections = {}\n",
    "        keywords = []\n",
    "\n",
    "        # Process sections under SecTag\n",
    "        for sec_tag in xml_soup.find_all('SecTag'):\n",
    "            sec_type = sec_tag.get('type', 'unknown').strip().upper()\n",
    "            if sec_type == 'KEYWORD':\n",
    "                # Extract keywords\n",
    "                keywords = [kwd.text.strip() for kwd in sec_tag.find_all('kwd')]\n",
    "                continue  # Skip further processing of keywords here\n",
    "            if sec_type not in sections:\n",
    "                sections[sec_type] = []\n",
    "            # Exclude nested 'SecTag's to avoid duplicate text\n",
    "            for nested_sec in sec_tag.find_all('SecTag', recursive=True):\n",
    "                nested_sec.extract()\n",
    "            sentences = call_sentence_tags(sec_tag)\n",
    "            sections[sec_type].extend(sentences)\n",
    "\n",
    "        # Process front section if not already processed\n",
    "        if xml_soup.article.find('front'):\n",
    "            front_sections, front_keywords = process_front(xml_soup.article.find('front'))\n",
    "            for k, v in front_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "            if front_keywords:\n",
    "                keywords.extend(front_keywords)\n",
    "\n",
    "        # Process body section if not already processed\n",
    "        if xml_soup.article.find('body'):\n",
    "            body_sections = process_body(xml_soup.article.find('body'))\n",
    "            for k, v in body_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "        \n",
    "        # Process back section if not already processed\n",
    "        if xml_soup.article.find('back'):\n",
    "            back_sections = process_back(xml_soup.article.find('back'))\n",
    "            for k, v in back_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "\n",
    "        # Remove empty sections\n",
    "        sections = {k: v for k, v in sections.items() if v}\n",
    "\n",
    "        return {\n",
    "            'article_ids': article_ids,\n",
    "            'open_status': open_status,\n",
    "            'article_type': article_type,\n",
    "            'keywords': keywords,\n",
    "            'sections': sections\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process each article and write to output file\n",
    "def process_each_article(each_file_path, out_file, document_flag):\n",
    "    files_list = getfileblocks(each_file_path, document_flag)\n",
    "    with open(out_file, 'w', encoding='utf-8') as out:\n",
    "        for each_file in tqdm(files_list, desc=\"Processing Articles\", disable=False):\n",
    "            if document_flag == 'f':\n",
    "                data = process_full_text(each_file)\n",
    "            else:\n",
    "                print('Document type not supported.')\n",
    "                continue\n",
    "            if data:\n",
    "                out.write(json.dumps(data) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9faabd0b-73af-4b64-8cb3-d9abc3d08c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def process_json(data, ordered_labels):\n",
    "    # Step 1: Initialize sections and directly map TITLE-GROUP to TITLE if present\n",
    "    sections = data['sections']\n",
    "    if \"TITLE-GROUP\" in sections:\n",
    "        sections[\"TITLE\"] = sections.pop(\"TITLE-GROUP\")\n",
    "    \n",
    "    # Step 2: Identify keys in sections not present in ordered_labels\n",
    "    section_keys = set(sections.keys())\n",
    "    ordered_labels_set = set(ordered_labels)\n",
    "    unfound_keys = section_keys - ordered_labels_set  # Keys in sections not in ordered_labels\n",
    "\n",
    "    # Step 3: Normalize only the unfound section keys (remove spaces, uppercase)\n",
    "    normalized_unfound_keys = {key.replace(\" \", \"\").upper(): key for key in unfound_keys}\n",
    "    \n",
    "    # Step 4: Map unfound normalized keys to ordered labels using fuzzy matching (threshold 80%)\n",
    "    mapped_labels = {}\n",
    "    for normalized_key, original_key in normalized_unfound_keys.items():\n",
    "        # Perform fuzzy matching\n",
    "        match, score = process.extractOne(normalized_key, ordered_labels, scorer=fuzz.partial_ratio)\n",
    "        if score >= 80:\n",
    "            mapped_labels[original_key] = match\n",
    "        else:\n",
    "            mapped_labels[original_key] = original_key  # Keep original if no close match\n",
    "    \n",
    "    # Step 5: Structure JSON without ordering or sent_id for now\n",
    "    result_json = {}\n",
    "    for section_key in sections:\n",
    "        label = mapped_labels.get(section_key, section_key)  # Use mapped label if exists, else original\n",
    "        result_json[label] = [{\"text\": text} for text in sections[section_key]]\n",
    "\n",
    "    # Step 6: Reorder JSON according to ordered_labels and add any unmapped sections at the end\n",
    "    ordered_json = {}\n",
    "    for label in ordered_labels:\n",
    "        if label in result_json:\n",
    "            ordered_json[label] = result_json.pop(label)\n",
    "    ordered_json.update(result_json)  # Add remaining sections in their original order\n",
    "    \n",
    "    # Step 7: Assign unique incremental sent_id starting from 1\n",
    "    sent_id = 1\n",
    "    for section in ordered_json.values():\n",
    "        for entry in section:\n",
    "            entry[\"sent_id\"] = sent_id\n",
    "            sent_id += 1  # Increment sent_id for each entry uniquely\n",
    "    \n",
    "    # Update the original data with the modified sections\n",
    "    data['sections'] = ordered_json\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c6382c-eec5-4af7-91d3-e261b6f4cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1.jsonl\t    output_no_batch.jsonl\tsentenciser-Copy2.ipynb\n",
      "output2.jsonl\t    patch-07-10-2024-0.xml.gz\tsentenciser-Copy3.ipynb\n",
      "outputa.jsonl\t    patch-28-01-2023-21.xml.gz\tsentenciser-Copy4.ipynb\n",
      "output_batch.jsonl  profile_stats\t\tsentenciser.ipynb\n",
      "output.jsonl\t    sentenciser-Copy1.ipynb\txx.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f67204",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file =  'patch-28-01-2023-21.xml.gz' #'patch-07-10-2024-0.xml.gz'\n",
    "output_file ='output_no_batch.jsonl' \n",
    "document_flag = 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d1e70b-a3a5-4a73-8ee1-051578aafb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|████████████████████████████████████████████████████████| 1000/1000 [11:30<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 01:57:32 2024    profile_stats\n",
      "\n",
      "         717888145 function calls (703447553 primitive calls) in 691.706 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 714 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "      3/1    0.000    0.000  691.706  691.706 {built-in method builtins.exec}\n",
      "        1    0.000    0.000  691.705  691.705 <string>:1(<module>)\n",
      "        1    0.033    0.033  691.705  691.705 /tmp/ipykernel_2618745/1192646138.py:350(process_each_article)\n",
      "     1000    0.169    0.000  689.238    0.689 /tmp/ipykernel_2618745/1192646138.py:260(process_full_text)\n",
      "63315/13037    1.784    0.000  609.355    0.047 /tmp/ipykernel_2618745/1192646138.py:54(call_sentence_tags)\n",
      "   170826    4.333    0.000  605.244    0.004 /tmp/ipykernel_2618745/1192646138.py:46(sentence_split)\n",
      "   170826    2.719    0.000  595.426    0.003 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:1016(__call__)\n",
      "   170826    0.745    0.000  433.851    0.003 spacy/pipeline/trainable_pipe.pyx:40(__call__)\n",
      "   170826    0.669    0.000  432.135    0.003 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:113(predict)\n",
      "   170826    3.265    0.000  431.086    0.003 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/model.py:330(predict)\n",
      "3928998/170826   10.983    0.000  427.821    0.003 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/chain.py:48(forward)\n",
      "10932864/341652    7.575    0.000  427.095    0.001 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/model.py:307(__call__)\n",
      "   512478    1.262    0.000  409.386    0.001 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/with_array.py:32(forward)\n",
      "   170826    1.101    0.000  275.110    0.002 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/with_array.py:70(_list_forward)\n",
      "   683304    3.356    0.000  260.041    0.000 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/residual.py:28(forward)\n",
      "    31530    0.307    0.000  241.837    0.008 /tmp/ipykernel_2618745/1192646138.py:84(process_p_tag)\n",
      "   854130   14.131    0.000  217.463    0.000 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/maxout.py:45(forward)\n",
      "   854130  146.794    0.000  147.800    0.000 thinc/backends/numpy_ops.pyx:91(gemm)\n",
      "   341652    0.940    0.000  132.316    0.000 /home/stirunag/falconframes_env/lib/python3.10/site-packages/thinc/layers/with_array.py:87(_ragged_forward)\n",
      "   170826    0.359    0.000  110.034    0.001 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/pipeline/attributeruler.py:132(__call__)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cProfile.run('process_each_article(input_file, output_file, document_flag)', 'profile_stats')\n",
    "\n",
    "    p = pstats.Stats('profile_stats')\n",
    "    p.sort_stats('cumtime').print_stats(20)  # Print top 20 functions by cumulative time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ec2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_each_article(input_file, output_file, document_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaebc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = getfileblocks(input_file, document_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cce26-b5b4-4f64-bf2a-a36047b8e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()  # Record the start time\n",
    "tt = process_full_text(ss[191])\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "print(f\"Execution time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "print(\"Structured JSON in strict order with sent_ids starting from 1:\")\n",
    "pprint.pprint(tt, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075ab1a-fa5b-43db-abc4-53ed8e76a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt['sections'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527023de-0b71-4b88-ae58-7111c7339cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_labels = ['TITLE', 'ABSTRACT', 'INTRO', 'METHODS', 'RESULTS', 'DISCUSS', 'CONCL', 'CASE', 'ACK_FUND', 'AUTH_CONT', 'COMP_INT', 'ABBR', 'SUPPL', 'REF', 'ACK_FUND', 'ABBR', 'COMP_INT', 'SUPPL', 'APPENDIX', 'AUTH_CONT']\n",
    "\n",
    "yy = process_json(tt, ordered_labels)\n",
    "pprint.pprint(yy, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a51c4b-e26b-4ad4-916b-aaa6c0b8232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a8fe0-c0e9-4cf2-90cf-bc26763cba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d5e61-0264-4b0e-8392-281e0e087260",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Title', 'Abstract'. 'Methods', 'Results', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8d40d-521f-4ef5-8ef1-47a34a0ff6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a9377-617e-4204-a6b1-a4516dfb9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def measure_execution_time(func, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Measure the execution time of a function.\n",
    "    \n",
    "#     Args:\n",
    "#     - func (function): The function to measure.\n",
    "#     - *args: Positional arguments to pass to the function.\n",
    "#     - **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "#     Returns:\n",
    "#     - result: The result of the function execution.\n",
    "#     - elapsed_time: The time taken to execute the function in seconds.\n",
    "#     \"\"\"\n",
    "#     start_time = time.time()  # Record the start time\n",
    "#     result = func(*args, **kwargs)  # Execute the function\n",
    "#     end_time = time.time()  # Record the end time\n",
    "#     elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "#     print(f\"Execution time: {elapsed_time:.4f} seconds\")\n",
    "#     return result, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d165c4f-9abe-4c58-9e78-ec36b3f7afc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
