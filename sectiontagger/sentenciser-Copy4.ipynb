{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930f199b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7f79553fe080>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import gzip\n",
    "import json\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import process, fuzz\n",
    "import spacy\n",
    "import difflib\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load(\"en_core_sci_sm\", disable=[\"parser\", \"ner\", \"tagger\", \"lemmatizer\"])\n",
    "nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd4533d-3318-4d0a-8c3a-33b7c288e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile regex patterns for section tagging\n",
    "titleMapsBody = {\n",
    "    'INTRO': [\n",
    "        'introduction', 'background', 'related literature', 'literature review', 'objective',\n",
    "        'aim ', 'purpose of this study', 'study (purpose|aim|aims)', r'\\d+\\. (purpose|aims|aim)',\n",
    "        '(aims|aim|purpose) of the study', '(the|drug|systematic|book) review', 'review of literature',\n",
    "        'related work', 'recent advance', 'overview', 'i ntroduction', 'historical overview',\n",
    "        'scope', 'context', 'rationale', 'hypothesis', 'motivation', 'i ntroduction', 'i ntro', 'i n t r o d u c t i o n'\n",
    "    ],\n",
    "    'METHODS': [\n",
    "        'supplement', 'methods and materials', 'method', 'material', 'experimental procedure',\n",
    "        'implementation', 'methodology', 'treatment', 'statistical analysis', \"experimental\",\n",
    "        r'\\d+\\. experimental$', 'experimental (section|evaluation|design|approach|protocol|setting|set up|investigation|detail|part|perspective|tool)',\n",
    "        \"the study\", r'\\d+\\. the study$', \"protocol\", \"protocols\", 'study protocol',\n",
    "        'construction and content', r'experiment \\d+', '^experiments$', 'analysis', 'utility',\n",
    "        'design', r'\\d+\\. theory$', \"theory\", 'theory and ', 'theory of ',\n",
    "        'data analysis', 'data collection', 'methodological approach', 'techniques', 'sample',\n",
    "        'materials and methods', 'analytical methods', 'research methods', 'methodological framework',\n",
    "        'm aterials and m ethods', 'm a t e r i a l s a n d m e t h o d s', 'm ethods'\n",
    "    ],\n",
    "    'RESULTS': [\n",
    "        'result', 'finding', 'diagnosis', 'outcomes', 'findings', 'observations',\n",
    "        'key results', 'main results', 'data', 'analysis results', 'primary results',\n",
    "        'research findings', 'experimental results', 'empirical findings', 'report of results',\n",
    "        'r esults', 'r e s u l t s'\n",
    "    ],\n",
    "    'DISCUSS': [\n",
    "        'discussion', 'management of', r'\\d+\\. management', 'safety and tolerability',\n",
    "        'limitations', 'perspective', 'commentary', r'\\d+\\. comment', 'interpretation',\n",
    "        'interpretation of results', 'analysis of findings', 'discussion and implications',\n",
    "        'contextualization', 'reflection', 'critical analysis', 'discussion and future work',\n",
    "        'insights', 'consideration', 'comparison with previous studies', 'd iscussion', 'd i s c u s s i o n'\n",
    "    ],\n",
    "    'CONCL': [\n",
    "        'conclusion', 'key message', 'future', 'summary', 'recommendation',\n",
    "        'implications for clinical practice', 'concluding remark', 'closing remarks',\n",
    "        'takeaway', 'final remarks', 'overall conclusion', 'summary and conclusion',\n",
    "        'implications', 'closing statement', 'wrap-up', 'summary of findings',\n",
    "        'future directions', 'outlook', 'next steps', 'c onclusion', 'c o n c l u s i o n'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        'case study report', 'case report', 'case presentation', 'case description',\n",
    "        r'case \\d+', r'\\d+\\. case', 'case summary', 'case history', 'case overview',\n",
    "        'case study', 'case examination', 'case details', 'case documentation',\n",
    "        'case example', 'case profile', 'c ase', 'c a s e'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'financial disclosure',\n",
    "        'funding sources', 'funding support', 'financial support', 'grant support',\n",
    "        'grant acknowledgement', 'acknowledgement of funding', 'funder', 'acknowledgements',\n",
    "        'a c k n o w l e d g e m e n t', 'a c k f u n d'\n",
    "    ],\n",
    "    'AUTH_CONT': [\n",
    "        \"author contribution\", \"authors' contribution\", \"author's contribution\",\n",
    "        \"contribution of authors\", \"authors' roles\", \"author responsibilities\", \"authorship contributions\",\n",
    "        'a u t h o r c o n t r i b u t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'competing interests', 'conflict statement',\n",
    "        'financial conflicts', 'competing financial interests', 'c o m p i n t'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'abbreviations list', 'acronyms', 'nomenclature',\n",
    "        'glossary', 'terms', 'terminology', 'abbreviation glossary', 'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'SUPPL': [\n",
    "        'supplemental data', 'supplementary file', 'supplemental file', 'supplementary data',\n",
    "        'supplementary figure', 'supplemental figure', 'supporting information',\n",
    "        'supplemental file', 'supplemental material', 'supplementary material',\n",
    "        'supplement material', 'additional data files', 'supplemental information',\n",
    "        'supplementary information', 'supporting files', 'appendix', 'online appendix',\n",
    "        'supporting documentation', 'extra data', 'additional material', 'annex',\n",
    "        's u p p l e m e n t', 's u p p l e m e n t a r y'\n",
    "    ]\n",
    "}\n",
    "\n",
    "titleExactMapsBody = {\n",
    "    'INTRO': [\n",
    "        \"aim\", \"aims\", \"purpose\", \"purposes\", \"purpose/aim\",\n",
    "        \"purpose of study\", \"review\", \"reviews\", \"minireview\", \"overview\", \"background\",\n",
    "        'i n t r o d u c t i o n', 'intro'\n",
    "    ],\n",
    "    'METHODS': [\n",
    "        \"experimental\", \"the study\", \"protocol\", \"protocols\", \"procedure\", \"methodology\", \"data analysis\",\n",
    "        'm e t h o d s', 'methods'\n",
    "    ],\n",
    "    'DISCUSS': [\n",
    "        \"management\", \"comment\", \"comments\", \"discussion\", \"limitations\", \"perspectives\",\n",
    "        'd i s c u s s', 'discussion'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        \"case\", \"cases\", \"case study\", \"case report\", \"case overview\", 'case'\n",
    "    ]\n",
    "}\n",
    "\n",
    "titleMapsBack = {\n",
    "    'REF': [\n",
    "        'reference', 'literature cited', 'references', 'bibliography', 'source list', 'citations',\n",
    "        'works cited', 'cited literature', 'bibliographical references', 'citations list',\n",
    "        'r e f e r e n c e s'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'acknowlegement',\n",
    "        'acknowlegement', 'open access', 'financial support', 'grant',\n",
    "        'author note', 'financial disclosure', 'support statement', 'funding acknowledgment',\n",
    "        'a c k n o w l e d g e'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'glossary', 'abbreviations list', 'acronyms', 'terminology', 'abbreviation glossary',\n",
    "        'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'conflicts', 'interest', 'financial conflicts',\n",
    "        'c o m p i n t'\n",
    "    ],\n",
    "    'CASE': [\n",
    "        'case study report', 'case report', 'case presentation', 'case description',\n",
    "        r'case \\d+', r'\\d+\\. case', 'case summary', 'case history', 'case overview',\n",
    "        'case study', 'case examination', 'case details', 'case documentation',\n",
    "        'case example', 'case profile', 'c ase', 'c a s e'\n",
    "    ],\n",
    "    'ACK_FUND': [\n",
    "        'funding', 'acknowledgement', 'acknowledgment', 'financial disclosure',\n",
    "        'funding sources', 'funding support', 'financial support', 'grant support',\n",
    "        'grant acknowledgement', 'acknowledgement of funding', 'funder', 'acknowledgements',\n",
    "        'a c k n o w l e d g e m e n t', 'a c k f u n d'\n",
    "    ],\n",
    "    'AUTH_CONT': [\n",
    "        \"author contribution\", \"authors' contribution\", \"author's contribution\",\n",
    "        \"contribution of authors\", \"authors' roles\", \"author responsibilities\", \"authorship contributions\",\n",
    "        'a u t h o r c o n t r i b u t i o n'\n",
    "    ],\n",
    "    'COMP_INT': [\n",
    "        'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "        'disclosure', 'declaration', 'competing interests', 'conflict statement',\n",
    "        'financial conflicts', 'competing financial interests', 'c o m p i n t'\n",
    "    ],\n",
    "    'ABBR': [\n",
    "        'abbreviation', 'abbreviations list', 'acronyms', 'nomenclature',\n",
    "        'glossary', 'terms', 'terminology', 'abbreviation glossary', 'a b b r e v i a t i o n'\n",
    "    ],\n",
    "    'SUPPL': [\n",
    "        'supplemental data', 'supplementary file', 'supplemental file', 'supplementary data',\n",
    "        'supplementary figure', 'supplemental figure', 'supporting information',\n",
    "        'supplemental file', 'supplemental material', 'supplementary material',\n",
    "        'supplement material', 'additional data files', 'supplemental information',\n",
    "        'supplementary information', 'supporting files', 'appendix', 'online appendix',\n",
    "        'supporting documentation', 'extra data', 'additional material', 'annex',\n",
    "        's u p p l e m e n t', 's u p p l e m e n t a r y'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# titleExactMapsBody = {\n",
    "#     'INTRO': [\n",
    "#         \"aim\", \"aims\", \"purpose\", \"purposes\", \"purpose/aim\",\n",
    "#         \"purpose of study\", \"review\", \"reviews\", \"minireview\", \"overview\", \"background\",\n",
    "#         'i n t r o d u c t i o n', 'intro'\n",
    "#     ],\n",
    "#     'METHODS': [\n",
    "#         \"experimental\", \"the study\", \"protocol\", \"protocols\", \"procedure\", \"methodology\", \"data analysis\",\n",
    "#         'm e t h o d s', 'methods'\n",
    "#     ],\n",
    "#     'DISCUSS': [\n",
    "#         \"management\", \"comment\", \"comments\", \"discussion\", \"limitations\", \"perspectives\",\n",
    "#         'd i s c u s s', 'discussion'\n",
    "#     ],\n",
    "#     'CASE': [\n",
    "#         \"case\", \"cases\", \"case study\", \"case report\", \"case overview\", 'case'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# titleMapsBack = {\n",
    "#     'REF': [\n",
    "#         'reference', 'literature cited', 'references', 'bibliography', 'source list', 'citations',\n",
    "#         'works cited', 'cited literature', 'bibliographical references', 'citations list',\n",
    "#         'r e f e r e n c e s'\n",
    "#     ],\n",
    "#     'ACK_FUND': [\n",
    "#         'funding', 'acknowledgement', 'acknowledgment', 'acknowlegement',\n",
    "#         'acknowlegement', 'open access', 'financial support', 'grant',\n",
    "#         'author note', 'financial disclosure', 'support statement', 'funding acknowledgment',\n",
    "#         'a c k n o w l e d g e'\n",
    "#     ],\n",
    "#     'ABBR': [\n",
    "#         'abbreviation', 'glossary', 'abbreviations list', 'acronyms', 'terminology', 'abbreviation glossary',\n",
    "#         'a b b r e v i a t i o n'\n",
    "#     ],\n",
    "#     'COMP_INT': [\n",
    "#         'competing interest', 'conflict of interest', 'conflicts of interest',\n",
    "#         'disclosure', 'declaration', 'conflicts', 'interest', 'financial conflicts',\n",
    "#         'c o m p i n t'\n",
    "#     ],\n",
    "#     'SUPPL': [\n",
    "#         'supplementary', 'supporting information', 'supplemental', 'web extra material',\n",
    "#         'supplemental files', 'online supplement', 'appendix', 'annex', 'additional resources',\n",
    "#         's u p p l e m e n t', 'supporting info'\n",
    "#     ],\n",
    "#     'APPENDIX': [\n",
    "#         'appendix', 'appendices', 'annex', 'additional material', 'extra material', 'a p p e n d i x'\n",
    "#     ],\n",
    "#     'AUTH_CONT': [\n",
    "#         'author', 'contribution', 'authors’ contributions', 'author contributions', 'roles of authors', 'authorship roles',\n",
    "#         'a u t h o r s h i p'\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "#     ],\n",
    "#     'SUPPL': [\n",
    "#         'supplementary', 'supporting information', 'supplemental', 'web extra material',\n",
    "#         'supplemental files', 'online supplement', 'appendix', 'annex', 'additional resources',\n",
    "#         's u p p l e m e n t', 'supporting info'\n",
    "#     ],\n",
    "#     'APPENDIX': [\n",
    "#         'appendix', 'appendices', 'annex', 'additional material', 'extra material', 'a p p e n d i x'\n",
    "#     ],\n",
    "#     'AUTH_CONT': [\n",
    "#         'author', 'contribution', 'authors’ contributions', 'author contributions', 'roles of authors', 'authorship roles',\n",
    "#         'a u t h o r s h i p'\n",
    "#     ]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2dec99d-c14c-4a67-a394-c9d38cb4b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile regex patterns\n",
    "compiled_titleMapsBody = {\n",
    "    key: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]\n",
    "    for key, patterns in titleMapsBody.items()\n",
    "}\n",
    "\n",
    "compiled_titleExactMapsBody = {\n",
    "    key: [pattern.lower() for pattern in patterns]\n",
    "    for key, patterns in titleExactMapsBody.items()\n",
    "}\n",
    "\n",
    "compiled_titleMapsBack = {\n",
    "    key: [re.compile(pattern, re.IGNORECASE) for pattern in patterns]\n",
    "    for key, patterns in titleMapsBack.items()\n",
    "}\n",
    "\n",
    "def createSecTag(soup, secType):\n",
    "    secTag = soup.new_tag('SecTag')\n",
    "    secTag['type'] = secType\n",
    "    return secTag\n",
    "\n",
    "# Function to read XML or GZ files and split into individual articles\n",
    "def getfileblocks(file_path, document_flag):\n",
    "    sub_file_blocks = []\n",
    "    if file_path.endswith('.gz'):\n",
    "        open_func = lambda x: gzip.open(x, 'rt', encoding='utf8')\n",
    "    else:\n",
    "        open_func = lambda x: open(x, 'r', encoding='utf8')\n",
    "\n",
    "    try:\n",
    "        with open_func(file_path) as fh:\n",
    "            content = fh.read()\n",
    "            if document_flag in ['f', 'a']:\n",
    "                # Split content by <!DOCTYPE article ...> or <article ...> tags\n",
    "                articles = re.split(r'(?=<!DOCTYPE article|<article(?![\\w-]))', content)\n",
    "                sub_file_blocks = [article.strip() for article in articles if article.strip() and '<!DOCTYPE' not in article]\n",
    "            else:\n",
    "                print('ERROR: unknown document type :' + document_flag)\n",
    "    except Exception as e:\n",
    "        print('Error processing file: ' + str(file_path))\n",
    "        print(e)\n",
    "\n",
    "    return sub_file_blocks\n",
    "\n",
    "# Function to split text into sentences using spaCy\n",
    "def batch_sentence_split(text_segments, batch_size=100):\n",
    "    \"\"\"\n",
    "    General function to process a list of text segments and split them into sentences using spaCy.\n",
    "    \n",
    "    Args:\n",
    "    - text_segments (list of str): List of text segments to be processed.\n",
    "    - batch_size (int): The number of texts to process in one batch (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "    - List of sentences extracted from the text segments.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    # Use spaCy's pipe function to process all collected texts in one batch\n",
    "    docs = nlp.pipe(text_segments, batch_size=batch_size, n_process=3)  # Adjust batch_size and n_process as needed\n",
    "\n",
    "    for doc in docs:\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent.text.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "# Function to process nested tags and collect sentences\n",
    "def call_sentence_tags(ch):\n",
    "    \"\"\"\n",
    "    Collect all the text from nested tags and process them in batches using spaCy for faster sentence splitting.\n",
    "    \n",
    "    Args:\n",
    "    - ch (BeautifulSoup Tag): The parent tag to process and extract text from nested tags.\n",
    "\n",
    "    Returns:\n",
    "    - List of sentences extracted from the nested tags.\n",
    "    \"\"\"\n",
    "    text_segments = []\n",
    "\n",
    "    # Collect all text segments to be processed\n",
    "    def collect_text_segments(node):\n",
    "        for gch in node.children:\n",
    "            if isinstance(gch, str):\n",
    "                continue  # Skip strings directly under ch\n",
    "            if gch.name in ['article-title', 'title', 'subtitle', 'trans-title', 'trans-subtitle', 'alt-title', 'label', 'td', 'th']:\n",
    "                if gch.find('p', recursive=False):\n",
    "                    collect_text_segments(gch)\n",
    "                else:\n",
    "                    text = gch.get_text(separator=' ', strip=True)\n",
    "                    if text:\n",
    "                        text_segments.append(text)\n",
    "            elif gch.name in [\"sec\", \"fig\", \"statement\", \"div\", \"boxed-text\", \"list\", \"list-item\", \"disp-quote\", \"speech\",\n",
    "                              \"fn-group\", \"fn\", \"def-list\", \"def-item\", \"def\", \"ack\", \"array\", \"table-wrap\", \"table\",\n",
    "                              \"tbody\", \"thead\", \"tr\", \"caption\", \"answer\", \"sec-meta\", \"glossary\", \"question\", \"question-wrap\"]:\n",
    "                collect_text_segments(gch)\n",
    "            elif gch.name == 'p':\n",
    "                sub_text = gch.get_text(separator=' ', strip=True)\n",
    "                if sub_text:\n",
    "                    text_segments.append(sub_text)\n",
    "            else:\n",
    "                text = gch.get_text(separator=' ', strip=True)\n",
    "                if text:\n",
    "                    text_segments.append(text)\n",
    "\n",
    "    # Recursively collect text segments\n",
    "    collect_text_segments(ch)\n",
    "\n",
    "\n",
    "    return batch_sentence_split(text_segments)\n",
    "\n",
    "\n",
    "# Function to process paragraph tags\n",
    "from bs4 import Tag\n",
    "\n",
    "def process_p_tag(p_tags):\n",
    "    \"\"\"\n",
    "    Process multiple <p> tags in batch, collecting text and splitting into sentences using spaCy.\n",
    "    \n",
    "    Args:\n",
    "    - p_tags (list of BeautifulSoup Tag): List of <p> tags to process.\n",
    "\n",
    "    Returns:\n",
    "    - List of sentences extracted from the <p> tags.\n",
    "    \"\"\"\n",
    "    text_segments = []\n",
    "\n",
    "    # Collect text from each <p> tag\n",
    "    for gch in p_tags:\n",
    "        if isinstance(gch, Tag):  # Ensure gch is a Tag object\n",
    "            if not (len(gch.contents) == 1 and \n",
    "                    (not gch.contents[0].string) and \n",
    "                    (gch.contents[0].name in [\"ext-link\", \"e-mail\", \"uri\", \"inline-supplementary-material\", \n",
    "                                              \"related-article\", \"related-object\", \"address\", \"alternatives\", \n",
    "                                              \"array\", \"funding-source\", \"inline-graphic\"])):\n",
    "                text = gch.get_text(separator=' ', strip=True)\n",
    "                if text:\n",
    "                    text_segments.append(text)\n",
    "\n",
    "    return batch_sentence_split(text_segments)\n",
    "\n",
    "\n",
    "\n",
    "# Function to process the front section\n",
    "def process_front(front):\n",
    "    sections = {}\n",
    "    keywords = []\n",
    "    \n",
    "    if front.find('article-meta'):\n",
    "        art_meta = front.find('article-meta')\n",
    "        \n",
    "        for ch in art_meta.find_all(recursive=False):\n",
    "            if ch.name in ['title-group', 'supplement', 'supplementary-material', 'abstract', 'trans-abstract', 'kwd-group', 'funding-group']:\n",
    "                section_title = ch.name.upper()\n",
    "                \n",
    "                if section_title == 'KWD-GROUP':\n",
    "                    # Extract keywords as a list from kwd-group\n",
    "                    keywords = [kwd.text.strip() for kwd in ch.find_all('kwd')]\n",
    "                else:\n",
    "                    sentences = call_sentence_tags(ch)\n",
    "                    if sentences:\n",
    "                        sections.setdefault(section_title, []).extend(sentences)\n",
    "            else:\n",
    "                pass  # Ignore other tags\n",
    "    \n",
    "    return sections, keywords\n",
    "\n",
    "# Function to process the back section\n",
    "def process_back(back):\n",
    "    sections = {}\n",
    "    for ch in back.find_all(recursive=False):\n",
    "        if ch.name in ['sec', 'p', 'ack', 'alternatives', 'array', 'preformat', 'fig', 'fig-group', 'question-wrap',\n",
    "                 'question-wrap-group', 'list', 'table-wrap-group', 'table-wrap', 'display-formula',\n",
    "                 'display-formula-group', 'def-list', 'list', 'supplementary-material', 'kwd-group',\n",
    "                 'funding-group', 'statement', 'ref-list', 'glossary']:\n",
    "            # Sections with titles\n",
    "            if ch.name == 'ref-list':\n",
    "                sentences = reference_sents(ch)\n",
    "                if sentences:\n",
    "                    sections.setdefault('REF', []).extend(sentences)\n",
    "            else:\n",
    "                title = ch.find('title')\n",
    "                if title:\n",
    "                    section_title = title.get_text(separator=' ', strip=True).strip().upper()\n",
    "                else:\n",
    "                    section_title = ch.name.upper()\n",
    "                sentences = call_sentence_tags(ch)\n",
    "                if sentences:\n",
    "                    sections.setdefault(section_title, []).extend(sentences)\n",
    "        else:\n",
    "            pass  # Ignore other tags\n",
    "    return sections\n",
    "\n",
    "def reference_sents(ref_list):\n",
    "    \"\"\"\n",
    "    Process reference sentences by collecting text from <ref> tags and using batch processing to split into sentences.\n",
    "    \n",
    "    Args:\n",
    "    - ref_list (BeautifulSoup Tag): The parent tag containing references.\n",
    "\n",
    "    Returns:\n",
    "    - List of sentences extracted from the references.\n",
    "    \"\"\"\n",
    "    text_segments = []\n",
    "\n",
    "    # Collect text from <ref> tags and other relevant nested tags\n",
    "    for ch in ref_list.children:\n",
    "        if isinstance(ch, str):\n",
    "            continue  # Skip strings directly under ref_list\n",
    "        if ch.name == 'ref':\n",
    "            sub_text = ''\n",
    "            for gch in ch.children:\n",
    "                if isinstance(gch, str):\n",
    "                    continue\n",
    "                sub_text += \" \" + \" \".join([d.string for d in gch.descendants if d.string])\n",
    "            if sub_text:\n",
    "                text_segments.append(sub_text.strip())\n",
    "        elif ch.name in [\"sec\", \"fig\", \"statement\", \"div\", \"boxed-text\", \"list\", \"list-item\", \"disp-quote\", \"speech\",\n",
    "                         \"fn-group\", \"fn\", \"def-list\", \"def-item\", \"def\", \"ack\", \"array\", \"table-wrap\", \"table\",\n",
    "                         \"tbody\", \"caption\", \"answer\", \"sec-meta\", \"glossary\", \"question\", \"question-wrap\"]:\n",
    "            sub_sentences = call_sentence_tags(ch)\n",
    "            text_segments.extend(sub_sentences)\n",
    "\n",
    "\n",
    "    return batch_sentence_split(text_segments)\n",
    "\n",
    "\n",
    "# Function to match section titles to predefined section types\n",
    "def titleMatch(title, secFlag):\n",
    "    matchKeys = []\n",
    "    # Check if the flag is 'body' or 'back' and apply the respective mappings\n",
    "    if secFlag == 'body':\n",
    "        titleMaps = compiled_titleMapsBody\n",
    "        exactMaps = compiled_titleExactMapsBody\n",
    "    else:\n",
    "        titleMaps = compiled_titleMapsBack\n",
    "        exactMaps = {}\n",
    "\n",
    "    title_lower = title.lower().strip()\n",
    "    # Check exact matches first\n",
    "    for key, patterns in exactMaps.items():\n",
    "        if title_lower in patterns:\n",
    "            matchKeys.append(key)\n",
    "            break  # If exact match found, no need to check further\n",
    "\n",
    "    # If no exact match, check regex patterns\n",
    "    if not matchKeys:\n",
    "        for key, patterns in titleMaps.items():\n",
    "            if any(pattern.search(title_lower) for pattern in patterns):\n",
    "                matchKeys.append(key)\n",
    "\n",
    "    return ','.join(matchKeys) if matchKeys else None\n",
    "\n",
    "# Function to apply section tagging to the soup object\n",
    "def section_tag(soup):\n",
    "    # Add Figure sections\n",
    "    for fig in soup.find_all('fig', recursive=True):\n",
    "        if not fig.find_all('fig', recursive=True):\n",
    "            fig_tag = createSecTag(soup, 'FIG')\n",
    "            fig.wrap(fig_tag)\n",
    "    \n",
    "    # Add Table sections\n",
    "    for table in soup.find_all('table-wrap', recursive=True):\n",
    "        if not table.find_all('table-wrap', recursive=True):\n",
    "            table_tag = createSecTag(soup, 'TABLE')\n",
    "            table.wrap(table_tag)\n",
    "\n",
    "    # Process front section\n",
    "    if soup.front:\n",
    "        if soup.front.abstract:\n",
    "            secAbs = createSecTag(soup, 'ABSTRACT')\n",
    "            soup.front.abstract.wrap(secAbs)\n",
    "        if soup.front.find('kwd-group'):\n",
    "            secKwd = createSecTag(soup, 'KEYWORD')\n",
    "            soup.front.find('kwd-group').wrap(secKwd)\n",
    "\n",
    "    # Process body section\n",
    "    if soup.body:\n",
    "        for sec in soup.body.find_all('sec', recursive=False):\n",
    "            title = sec.find('title')\n",
    "            if title:\n",
    "                title_text = title.get_text(separator=' ', strip=True)\n",
    "                mappedTitle = titleMatch(title_text, 'body')\n",
    "                if mappedTitle:\n",
    "                    secBody = createSecTag(soup, mappedTitle)\n",
    "                    sec.wrap(secBody)\n",
    "    # Process back sections\n",
    "    if soup.back:\n",
    "        for sec in soup.back.find_all(['sec', 'ref-list', 'app-group', 'ack', 'glossary', 'notes', 'fn-group'], recursive=False):\n",
    "            if sec.name == 'ref-list':\n",
    "                secRef = createSecTag(soup, 'REF')\n",
    "                sec.wrap(secRef)\n",
    "            else:\n",
    "                title = sec.find('title')\n",
    "                if title:\n",
    "                    title_text = title.get_text(separator=' ', strip=True)\n",
    "                    mappedTitle = titleMatch(title_text, 'back')\n",
    "                    if mappedTitle:\n",
    "                        secBack = createSecTag(soup, mappedTitle)\n",
    "                        sec.wrap(secBack)\n",
    "\n",
    "# Function to process the body section\n",
    "def process_body(body):\n",
    "    sections = {}\n",
    "    for ch in body.find_all(recursive=False):\n",
    "        if ch.name == 'p':\n",
    "            sentences = process_p_tag(ch)\n",
    "            sections.setdefault('BODY', []).extend(sentences)\n",
    "        elif ch.name in ['sec', 'ack', 'alternatives', 'array', 'preformat', 'fig', 'fig-group', 'question-wrap', 'list', 'table-wrap-group', 'table-wrap', 'display-formula', 'display-formula-group', 'def-list', 'list', 'supplementary-material', 'kwd-group', 'funding-group', 'statement']:\n",
    "            title = ch.find('title')\n",
    "            if title:\n",
    "                section_title = title.get_text(separator=' ', strip=True).strip().upper()\n",
    "            else:\n",
    "                section_title = ch.name.upper()\n",
    "            sentences = call_sentence_tags(ch)\n",
    "            if sentences:\n",
    "                sections.setdefault(section_title, []).extend(sentences)\n",
    "    return sections\n",
    "\n",
    "# Main function to process each article and collect data\n",
    "def process_full_text(each_file):\n",
    "\n",
    "    # Replace body tag with orig_body to prevent BeautifulSoup from removing it\n",
    "    each_file = re.sub(r'<body(\\s[^>]*)?>', '<orig_body\\\\1>', each_file)\n",
    "    each_file = each_file.replace('</body>', '</orig_body>')\n",
    "    try:\n",
    "        xml_soup = BeautifulSoup(each_file, 'lxml')\n",
    "        # Remove extra html and body tags added by BeautifulSoup\n",
    "        if xml_soup.html:\n",
    "            xml_soup.html.unwrap()\n",
    "        if xml_soup.body:\n",
    "            xml_soup.body.unwrap()\n",
    "        if xml_soup.find('orig_body'):\n",
    "            xml_soup.find('orig_body').name = 'body'\n",
    "\n",
    "        # Extract attributes from the <article> tag\n",
    "        article_tag = xml_soup.find('article')\n",
    "        if article_tag:\n",
    "            open_status = article_tag.get('open-status', '')\n",
    "            article_type = article_tag.get('article-type', '')\n",
    "        else:\n",
    "            open_status = ''\n",
    "            article_type = ''\n",
    "\n",
    "        # Extract article IDs\n",
    "        article_ids = {}\n",
    "        for id_tag in xml_soup.find_all('article-id'):\n",
    "            id_type = id_tag.get('pub-id-type', 'unknown')\n",
    "            article_ids[id_type] = id_tag.text.strip()\n",
    "        if not article_ids:\n",
    "            print('No article IDs found')\n",
    "            return None\n",
    "\n",
    "        # Apply section tagging\n",
    "        section_tag(xml_soup)\n",
    "        \n",
    "        sections = {}\n",
    "        keywords = []\n",
    "\n",
    "        # Process sections under SecTag\n",
    "        for sec_tag in xml_soup.find_all('SecTag'):\n",
    "            sec_type = sec_tag.get('type', 'unknown').strip().upper()\n",
    "            if sec_type == 'KEYWORD':\n",
    "                # Extract keywords\n",
    "                keywords = [kwd.text.strip() for kwd in sec_tag.find_all('kwd')]\n",
    "                continue  # Skip further processing of keywords here\n",
    "            if sec_type not in sections:\n",
    "                sections[sec_type] = []\n",
    "            # Exclude nested 'SecTag's to avoid duplicate text\n",
    "            for nested_sec in sec_tag.find_all('SecTag', recursive=True):\n",
    "                nested_sec.extract()\n",
    "            sentences = call_sentence_tags(sec_tag)\n",
    "            sections[sec_type].extend(sentences)\n",
    "\n",
    "        # Process front section if not already processed\n",
    "        if xml_soup.article.find('front'):\n",
    "            front_sections, front_keywords = process_front(xml_soup.article.find('front'))\n",
    "            for k, v in front_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "            if front_keywords:\n",
    "                keywords.extend(front_keywords)\n",
    "\n",
    "        # Process body section if not already processed\n",
    "        if xml_soup.article.find('body'):\n",
    "            body_sections = process_body(xml_soup.article.find('body'))\n",
    "            for k, v in body_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "        \n",
    "        # Process back section if not already processed\n",
    "        if xml_soup.article.find('back'):\n",
    "            back_sections = process_back(xml_soup.article.find('back'))\n",
    "            for k, v in back_sections.items():\n",
    "                sections.setdefault(k, []).extend(v)\n",
    "\n",
    "        # Remove empty sections\n",
    "        sections = {k: v for k, v in sections.items() if v}\n",
    "\n",
    "        return {\n",
    "            'article_ids': article_ids,\n",
    "            'open_status': open_status,\n",
    "            'article_type': article_type,\n",
    "            'keywords': keywords,\n",
    "            'sections': sections\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing article: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process each article and write to output file\n",
    "def process_each_article(each_file_path, out_file, document_flag):\n",
    "    files_list = getfileblocks(each_file_path, document_flag)\n",
    "    with open(out_file, 'w', encoding='utf-8') as out:\n",
    "        for each_file in tqdm(files_list, desc=\"Processing Articles\", disable=False):\n",
    "            if document_flag == 'f':\n",
    "                data = process_full_text(each_file)\n",
    "            else:\n",
    "                print('Document type not supported.')\n",
    "                continue\n",
    "            if data:\n",
    "                out.write(json.dumps(data) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9faabd0b-73af-4b64-8cb3-d9abc3d08c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "def process_json(data, ordered_labels):\n",
    "    # Step 1: Initialize sections and directly map TITLE-GROUP to TITLE if present\n",
    "    sections = data['sections']\n",
    "    if \"TITLE-GROUP\" in sections:\n",
    "        sections[\"TITLE\"] = sections.pop(\"TITLE-GROUP\")\n",
    "    \n",
    "    # Step 2: Identify keys in sections not present in ordered_labels\n",
    "    section_keys = set(sections.keys())\n",
    "    ordered_labels_set = set(ordered_labels)\n",
    "    unfound_keys = section_keys - ordered_labels_set  # Keys in sections not in ordered_labels\n",
    "\n",
    "    # Step 3: Normalize only the unfound section keys (remove spaces, uppercase)\n",
    "    normalized_unfound_keys = {key.replace(\" \", \"\").upper(): key for key in unfound_keys}\n",
    "    \n",
    "    # Step 4: Map unfound normalized keys to ordered labels using fuzzy matching (threshold 80%)\n",
    "    mapped_labels = {}\n",
    "    for normalized_key, original_key in normalized_unfound_keys.items():\n",
    "        # Perform fuzzy matching\n",
    "        match, score = process.extractOne(normalized_key, ordered_labels, scorer=fuzz.partial_ratio)\n",
    "        if score >= 80:\n",
    "            mapped_labels[original_key] = match\n",
    "        else:\n",
    "            mapped_labels[original_key] = original_key  # Keep original if no close match\n",
    "    \n",
    "    # Step 5: Structure JSON without ordering or sent_id for now\n",
    "    result_json = {}\n",
    "    for section_key in sections:\n",
    "        label = mapped_labels.get(section_key, section_key)  # Use mapped label if exists, else original\n",
    "        result_json[label] = [{\"text\": text} for text in sections[section_key]]\n",
    "\n",
    "    # Step 6: Reorder JSON according to ordered_labels and add any unmapped sections at the end\n",
    "    ordered_json = {}\n",
    "    for label in ordered_labels:\n",
    "        if label in result_json:\n",
    "            ordered_json[label] = result_json.pop(label)\n",
    "    ordered_json.update(result_json)  # Add remaining sections in their original order\n",
    "    \n",
    "    # Step 7: Assign unique incremental sent_id starting from 1\n",
    "    sent_id = 1\n",
    "    for section in ordered_json.values():\n",
    "        for entry in section:\n",
    "            entry[\"sent_id\"] = sent_id\n",
    "            sent_id += 1  # Increment sent_id for each entry uniquely\n",
    "    \n",
    "    # Update the original data with the modified sections\n",
    "    data['sections'] = ordered_json\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c6382c-eec5-4af7-91d3-e261b6f4cbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1.jsonl\t    output_no_batch.jsonl\tsentenciser-Copy2.ipynb\n",
      "output2.jsonl\t    patch-07-10-2024-0.xml.gz\tsentenciser-Copy3.ipynb\n",
      "outputa.jsonl\t    patch-28-01-2023-21.xml.gz\tsentenciser-Copy4.ipynb\n",
      "output_batch.jsonl  profile_stats\t\tsentenciser.ipynb\n",
      "output.jsonl\t    sentenciser-Copy1.ipynb\txx.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f67204",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file =  'patch-28-01-2023-21.xml.gz' #'patch-07-10-2024-0.xml.gz'\n",
    "output_file ='output_batch.jsonl' \n",
    "document_flag = 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96d1e70b-a3a5-4a73-8ee1-051578aafb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|████████████████████████████████████████████████████████| 1000/1000 [17:17<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Oct 27 01:56:31 2024    profile_stats\n",
      "\n",
      "         662357480 function calls (662102723 primitive calls) in 1039.090 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 529 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000 1039.090 1039.090 {built-in method builtins.exec}\n",
      "        1    0.001    0.001 1039.090 1039.090 <string>:1(<module>)\n",
      "        1    0.225    0.225 1039.089 1039.089 /tmp/ipykernel_2589526/1224015570.py:412(process_each_article)\n",
      "     1000    0.777    0.001 1035.152    1.035 /tmp/ipykernel_2589526/1224015570.py:322(process_full_text)\n",
      "    13827    5.401    0.000  951.370    0.069 /tmp/ipykernel_2589526/1224015570.py:46(batch_sentence_split)\n",
      "   186078    3.123    0.000  941.021    0.005 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:1534(pipe)\n",
      "    13037    0.332    0.000  936.118    0.072 /tmp/ipykernel_2589526/1224015570.py:70(call_sentence_tags)\n",
      "   186078    2.581    0.000  932.859    0.005 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:1632(_multiprocessing_pipe)\n",
      "    27922    0.151    0.000  417.479    0.015 /home/stirunag/falconframes_env/lib/python3.10/site-packages/spacy/language.py:1688(<genexpr>)\n",
      "    14095    0.302    0.000  417.328    0.030 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/connection.py:246(recv)\n",
      "    14095    0.256    0.000  415.125    0.029 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/connection.py:413(_recv_bytes)\n",
      "    28190    0.516    0.000  414.802    0.015 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/connection.py:374(_recv)\n",
      "    69282  410.352    0.006  410.352    0.006 {built-in method posix.read}\n",
      "    41481    2.932    0.000  321.329    0.008 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/process.py:110(start)\n",
      "    41481    1.001    0.000  313.942    0.008 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/context.py:222(_Popen)\n",
      "    41481    2.158    0.000  312.854    0.008 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/context.py:278(_Popen)\n",
      "    41481    1.038    0.000  310.166    0.007 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/popen_fork.py:15(__init__)\n",
      "    41481    3.234    0.000  243.461    0.006 /home/stirunag/miniconda3/lib/python3.10/multiprocessing/popen_fork.py:62(_launch)\n",
      "    41481  235.481    0.006  237.106    0.006 {built-in method posix.fork}\n",
      "   172251    0.105    0.000  168.750    0.001 {method 'from_bytes' of 'spacy.tokens.doc.Doc' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cProfile.run('process_each_article(input_file, output_file, document_flag)', 'profile_stats')\n",
    "\n",
    "    p = pstats.Stats('profile_stats')\n",
    "    p.sort_stats('cumtime').print_stats(20)  # Print top 20 functions by cumulative time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ec2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_each_article(input_file, output_file, document_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaaebc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = getfileblocks(input_file, document_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42eb2ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a8cce26-b5b4-4f64-bf2a-a36047b8e5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.8288 seconds\n",
      "Structured JSON in strict order with sent_ids starting from 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'article_ids': {'pmcid': '9878372',\n",
       "  'publisher-id': 'v12i1e41533',\n",
       "  'pmid': '36630158',\n",
       "  'doi': '10.2196/41533'},\n",
       " 'open_status': 'O',\n",
       " 'article_type': 'research-article',\n",
       " 'keywords': ['general practice',\n",
       "  'vital signs/methods',\n",
       "  'vital signs/standards',\n",
       "  'photoplethysmography',\n",
       "  'remote photoplethysmography',\n",
       "  'rPPG',\n",
       "  'Lifelight',\n",
       "  'contactless',\n",
       "  'software'],\n",
       " 'sections': {'ABSTRACT': ['Background Measuring vital signs (VS) is an important aspect of clinical care but is time-consuming and requires multiple pieces of equipment and trained staff.',\n",
       "   'Interest in the contactless measurement of VS has grown since the COVID-19 pandemic, including in nonclinical situations.',\n",
       "   'Lifelight is an app being developed as a medical device for the contactless measurement of VS using remote photoplethysmography (rPPG) via the camera on smart devices.',\n",
       "   'The VISION-D (Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care—Development) and VISION-V (Validation) studies demonstrated the accuracy of Lifelight compared with standard-of-care measurement of blood pressure, pulse rate, and respiratory rate, supporting the certification of Lifelight as a class I Conformité Européenne (CE) medical device.',\n",
       "   'Objective To support further development of the Lifelight app, the observational VISION Multisite Development (VISION-MD) study is collecting high-quality data from a broad range of patients, including those with VS measurements outside the normal healthy range and patients who are critically ill.',\n",
       "   'Methods The study is recruiting adults (aged ≥16 years) who are inpatients (some critically ill), outpatients, and healthy volunteers, aiming to cover a broad range of normal and clinically relevant VS values; there are no exclusion criteria.',\n",
       "   'High-resolution 60-second videos of the face are recorded by the Lifelight app while simultaneously measuring VS using standard-of-care methods (automated sphygmomanometer for blood pressure; finger clip sensor for pulse rate and oxygen saturation; manual counting of respiratory rate).',\n",
       "   'Feedback from patients and nurses who use Lifelight is collected via a questionnaire.',\n",
       "   'Data to estimate the cost-effectiveness of Lifelight compared with standard-of-care VS measurement are also being collected.',\n",
       "   'A new method for rPPG signal processing is currently being developed, based on the identification of small areas of high-quality signals in each individual.',\n",
       "   'Anticipated recruitment is 1950 participants, with the expectation that data from approximately 1700 will be used for software development.',\n",
       "   'Data from 250 participants will be retained to test the performance of Lifelight against predefined performance targets.',\n",
       "   'Results Recruitment began in May 2021 but was hindered by the restrictions instigated during the COVID-19 pandemic.',\n",
       "   'The development of data processing methodology is in progress.',\n",
       "   'The data for analysis will become available from September 2022, and the algorithms will be refined continuously to improve clinical accuracy.',\n",
       "   'The performance of Lifelight compared with that of the standard-of-care measurement of VS will then be tested.',\n",
       "   'Recruitment will resume if further data are required.',\n",
       "   'The analyses are expected to be completed in early 2023.',\n",
       "   'Conclusions This study will support the refinement of data collection and processing toward the development of a robust app that is suitable for routine clinical use.',\n",
       "   'Trial Registration ClinicalTrials.gov NCT04763746; https://clinicaltrials.gov/ct2/show/NCT04763746 International Registered Report Identifier (IRRID) DERR1-10.2196/41533'],\n",
       "  'INTRO': ['Introduction',\n",
       "   \"The measurement of vital signs (VS) provides important information about a patient's health and, importantly, a change in VS may herald a deterioration in health [ 1 ].\",\n",
       "   'Despite the importance of VS to inform clinical decision-making, the accuracy and timeliness of measurement are in need of improvement [ 2 - 4 ].',\n",
       "   'However, the measurement of VS requires using multiple pieces of equipment that need to be calibrated regularly and is time-consuming.',\n",
       "   'It may also be uncomfortable and stressful for patients, potentially compromising the utility of the information obtained.',\n",
       "   'Standard-of-care medical equipment is not suitable for patients who require regular measurement of VS in the home or community setting to monitor long-term health conditions because of cost, the complexity of the measuring processes, and the need for calibration of equipment.',\n",
       "   'A study of 725 patients reported that while 53% followed at least 10 of the recommended steps necessary for accurate blood pressure (BP) measurement at home, only 1% followed all 15 recommendations [ 5 ].',\n",
       "   'Thus, home measurement of VS is important—and respiratory rate and pulse rate in particular—but requires several pieces of equipment (BP monitor and pulse oximeter) and for patients to be educated in best practices.',\n",
       "   'The COVID-19 pandemic highlighted the need for remote or contactless VS measurement to reduce the risk of infection, which can be operated by people without specific medical training.',\n",
       "   'The shift away from face-to-face to digital consultations during the pandemic also points to the need for easy but accurate measurement of VS.',\n",
       "   'Photoplethysmography (PPG) is an optical technique based on the measurement of the light reflected from the skin surface, which changes due to volumetric changes in the facial blood vessels; small variations in perfusion provide valuable information about the cardiovascular system [ 6 ].',\n",
       "   'PPG has been used to measure pulse rate [ 7 , 8 ], oxygen saturation [ 9 ], BP [ 10 , 11 ], and respiratory rate [ 7 , 12 ] and to detect atrial fibrillation [ 13 ].',\n",
       "   'Lifelight (Xim Ltd) is an app being developed for the contactless measurement of VS using remote PPG (rPPG) via the camera on smart devices such as phones and tablets.',\n",
       "   'The app captures the average color of the region of interest 30 times every second for 60 seconds and sends this as red, green, and blue values to the server for further processing.',\n",
       "   'VS values are obtained from the green channel.',\n",
       "   'The VISION-D (Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care-Development) study measured VS in 8585 patients and healthy volunteers simultaneously using Lifelight and standard-of-care methods.',\n",
       "   'The data were used for machine learning to improve the accuracy of the Lifelight algorithms used to calculate VS.',\n",
       "   'The smaller VISION-V (Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care–Validation) study demonstrated the accuracy of the Lifelight app compared with standard-of-care methods for measuring pulse rate, respiratory rate, and diastolic BP [ 14 ], providing the basis for the current class I Conformité Européenne (CE) registration [ 15 ].',\n",
       "   'However, some of the methods used in the VISION-V study differed from the procedures described in the standard for BP measurement (ISO81060-2) because of the novel nature of the Lifelight technology.',\n",
       "   'Furthermore, these early studies did not include participants with BP values across the full range likely to be encountered in clinical practice.',\n",
       "   'To further improve the accuracy of Lifelight, the Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care–Multisite Development (VISION-MD) study is collecting data from a wide range of outpatients, inpatients, and patients who are critically ill, and across the full range of skin tones, for use in machine learning.',\n",
       "   'In VISION-D and VISION-V, full-face videos were recorded, but a high proportion of data were not usable.',\n",
       "   'Thus, high-resolution full-face videos are being recorded to maximize the opportunity for machine learning.',\n",
       "   'These data will also be used to evaluate alternative methods of defining the region of interest, as the full face includes areas that are not relevant (eg, areas covered by facial hair and areas that illicit a poor signal).',\n",
       "   'Given that only a small proportion of the raw video signal is relevant for rPPG measurement of VS (1%-2%), we are developing ways to enhance data collection and signal processing.',\n",
       "   'Video recordings will be of higher resolution than those in the VISION-V and VISION-D studies, and data processing is focusing on the midface region (cheeks, nose, and top of the lip), rather than the whole face; these areas are computationally efficient for rPPG because of their large area and good-quality signal [ 16 ] but are not likely to be affected by autoregulation of cerebral blood flow (which discounts the forehead) [ 17 ].',\n",
       "   'We are also developing a method to identify small regions of interest in the midface in each participant where signal quality is the highest.',\n",
       "   'This approach is expected to overcome some of the challenges of rPPG for routine clinical use, such as positioning of the participant relative to the light source.',\n",
       "   'VISION-MD (Clinicaltrials.gov NCT04763746) aims to advance the development and accuracy of the Lifelight app as a noninvasive and easy-to-use device to measure VS in hospitals and the community.',\n",
       "   'The study will collect data from a broad range of patients to further develop the accuracy of Lifelight to a level sufficient for clinical applications, including screening and monitoring of cardiovascular disease.',\n",
       "   'The initial data collected are being used for machine learning; later data will be used to test the accuracy of Lifelight compared with standard-of-care measurement of VS.',\n",
       "   'Thus, the primary objective of VISION-MD is to further develop the Lifelight algorithms across extensive clinical ranges, including critically ill patients and in patients with different skin tones.',\n",
       "   'Secondary objectives are: (1) to improve and test the efficacy of Lifelight estimates for BP, pulse rate, respiratory rate, and oxygen saturation in multiple clinical settings (eg, critical care, outpatient clinics, and general hospital wards); (2) to evaluate the impact of variables on the accuracy of Lifelight VS measurements (eg, age, sex, temperature, health condition, medication, skin tone, and ambient lighting); (3) to understand the health economic potential of Lifelight; and (4) to compare the patients’ experience of current contact-based methods for measuring VS and Lifelight and to evaluate the patients’ acceptance and appetite for Lifelight.'],\n",
       "  'METHODS': ['Methods',\n",
       "   'Ethics Approval',\n",
       "   'The VISION-MD protocol was approved by the South Berkshire Research Ethics Committee on November 24, 2020 (20/SC/0432).',\n",
       "   'Before the study started, the initial study protocol was approved by Health Research Authority (HRA) Wales on January 18, 2021 (IRAS 289242).',\n",
       "   'HRA Wales has also approved subsequent protocol amendments.',\n",
       "   'Participants and Recruitment',\n",
       "   'Participants are being recruited from multiple venues across Portsmouth Hospitals University NHS Trust, Barts Health NHS Trust, London, and from the community in London and Portsmouth (eg, religious places, community centers, offices, patient events, waiting areas in general practices, academic institutions, sports facilities, and care homes).',\n",
       "   'The participants are inpatients, outpatients, friends and family of patients, visitors, hospital staff members, and the general public.',\n",
       "   'The study staff approach inpatients during their hospital stay and outpatients while waiting for appointments.',\n",
       "   'For adults lacking capacity (eg, critically ill patients), the next of kin are contacted by telephone.',\n",
       "   'In addition, ethics-approved advertising materials are disseminated to Trust staff by email and in meetings, and posters are displayed in staff, patient, and public areas.',\n",
       "   'Inclusion criteria include individuals aged 16 years and older, sufficiently conversant in the English language, and able and willing to comply with all study requirements and to provide informed consent (either themselves or empowered by law to provide it).',\n",
       "   'There are no exclusion criteria.',\n",
       "   'Eligible potential participants are provided with an ethics-approved participant information sheet explaining the study aims, what is involved, and the requirements for participation; members of the team are available to discuss the study with interested individuals.',\n",
       "   'Informed consent is obtained electronically using Research Electronic Data Capture (REDCap), a secure National Health Service (NHS)–compliant web-based platform for survey and database management (project-redcap-org).',\n",
       "   'For adults lacking capacity, informed consent is obtained from a nominated consultee (next of kin or a doctor not involved in the study).',\n",
       "   'Participation in the study is entirely voluntary, refusal to participate does not incur a penalty or loss of medical benefits, and participants may withdraw from the study at any time.',\n",
       "   'Recruitment started in May 2021 but was compromised by restrictions implemented during the COVID-19 pandemic to limit access to hospitals by the general public.',\n",
       "   'Protocol amendments were made to increase community recruitment in light of these issues.',\n",
       "   'Target recruitment is approximately 1950 participants to generate measurements for use in the initial training data set and for performance testing.',\n",
       "   'However, the final sample size will depend on the incremental improvement in accuracy of the Lifelight algorithm and therefore cannot be predicted (see Sample Size section).',\n",
       "   'The study will continue until the accuracy of Lifelight for measuring VS is sufficient for various clinical use cases.',\n",
       "   'Study Procedures',\n",
       "   'Premeasurement observations',\n",
       "   'A brief set of demographic and medical history questions are asked, limited to the presence or absence of conditions that might affect skin perfusion and pigmentation and cardiovascular processes and any prescription medicines for these conditions.',\n",
       "   'The study staff record a set of premeasurement observations and the presence or absence of sweat on the participant’s face; any facial hair on the cheeks; tattoos, jewelry, birthmarks, scars, or other features on the face; the use of foundation or concealer; and the position of the participant (seated, prone, supine, or lying on one side).',\n",
       "   'Subprotocol assignment',\n",
       "   'Patients with capacity are recruited into 1 of 3 subprotocols depending on premeasurement observations ( Table 1 ).',\n",
       "   'Participants may also be recruited to a subprotocol based on their skin tone (Fitzpatrick Skin Type scale [ 18 ]) to meet prespecified targets.',\n",
       "   'Adults who lack capacity are recruited into subprotocol 4.',\n",
       "   'Participants may be involved in up to 10 study sessions, allowing the collection of longitudinal data.',\n",
       "   'The subprotocol approach allows the study personnel to focus on fewer tasks.',\n",
       "   'It also enables high-quality data collection while avoiding the collection of data that would not be used to meet study objectives, consistent with the General Data Protection Regulation for data minimization.',\n",
       "   'VS measurement',\n",
       "   'The study staff ensure that participants have been at rest for at least 10\\xa0minutes before VS measurement starts and that they have not consumed any food or drink in the previous 30\\xa0minutes (other than intravenous fluids or nasogastric feeding).',\n",
       "   'In each study session, VS is measured as per the subprotocol using the standard-of-care equipment while simultaneously capturing a video of the participant’s face using the Data Collect app running on a tablet (standard iPad 9.7, 2018) positioned approximately 1\\xa0m away and angled toward the participant’s face.',\n",
       "   'Controls and instructions on the device start and stop the 60-second video recording.',\n",
       "   'Background luminosity is measured using a handheld lux meter.',\n",
       "   'The study staff have been briefed on the optimum Lifelight measurement conditions.',\n",
       "   'Recordings are repeated once or twice, as set out in Table 1 .',\n",
       "   'The app does not return any measurements to the user or participant.',\n",
       "   'VS measurements are taken and coordinated by 2 nurses, one of whom announces the start and finish of the recording period on the Data Collect app.',\n",
       "   'BP is measured using a standard clinical automatic sphygmomanometer with an appropriately sized cuff (width at least two-thirds of upper arm length) on the participant’s nondominant upper arm (unless contraindicated) or via an arterial line if fitted.',\n",
       "   'BP is recorded at the start of the recording period.',\n",
       "   'A standard clinical finger clip sensor for the measurement of oxygen saturation and pulse rate is placed on a finger on the opposite side of the body to the sphygmomanometer.',\n",
       "   'Oxygen saturation and pulse rate are measured at 0, 30, and 60 seconds of the recording period and averaged.',\n",
       "   'Respiratory rate is determined manually by counting chest rises throughout the 60-second period.',\n",
       "   'The nurse may place their hand on the participant’s chest to increase the accuracy of manual counting but being mindful not to obscure the camera’s line of sight.',\n",
       "   'Each study session takes approximately 30\\xa0minutes.',\n",
       "   'Once the measurements are completed, the study staff complete the postmeasurement observation questions relating to how much the participant moved, their position, whether they were wearing glasses, any hairstyle or other item (eg, face covering) that obscured any part of their face during the recording, and whether the software reported “face not found” at any point during the recording.',\n",
       "   'Patient Feedback',\n",
       "   'Equal proportions of participants in subprotocols 1-3 are being asked to complete a questionnaire related to VS measurement and their preferences.',\n",
       "   'The data are fully anonymized and recorded without any identifiable information (including participant ID code).',\n",
       "   'Clinical Feedback',\n",
       "   'A questionnaire is available to garner feedback on the technology from the clinical user’s point of view (ie, the nurses who take the VS measurements).',\n",
       "   'Questionnaire and interview data are fully anonymized and recorded without any identifiable information.',\n",
       "   'Health Economics Data Collection',\n",
       "   'The study also includes activities to obtain information and data to assess the cost-saving potential of Lifelight in different clinical settings, including as a tool to detect undiagnosed cardiovascular disease and to monitor symptoms.',\n",
       "   'The cost of BP monitoring equipment and its maintenance and calibration will also be determined.',\n",
       "   'Stopwatch observational studies are run to determine how long it takes to measure VS using standard-of-care equipment and Lifelight, starting from the time when the clinician decides to conduct a VS check and incorporating the time it takes to find the measuring equipment, roll up the patient’s sleeve, put on the devices, wait for the result, and put the equipment away.',\n",
       "   'This part of the study will involve approximately 20 participants.',\n",
       "   'Privacy and Data Collection',\n",
       "   'Each study participant is assigned a unique sequential ID; no identifiable data are stored.',\n",
       "   'All documents are stored securely and are only accessible by the study staff and authorized personnel.',\n",
       "   'The code linking the ID to the participant’s personal information is kept within the hospital study site and can only be accessed by the research team.',\n",
       "   'Full-resolution video data are uploaded during the study.',\n",
       "   'The consent form allows the participants to decide whether data can be shared as full-face video or with identifying features obscured.',\n",
       "   'Videos collected in the study constitute personal data, as it may be possible to identify participants, but are collected for research purposes only (not clinical care) and are processed within the legitimate interests of Xim Ltd. These data will be protected according to the General Data Protection Regulation.',\n",
       "   'Data Handling',\n",
       "   'For each reading, a high-quality video of the whole face is saved to the internal storage of the iPad in encrypted form.',\n",
       "   'Anonymized rPPG data (the average color of areas of the face) are saved directly and immediately sent to an NHS-compliant cloud server.',\n",
       "   'Subsequent analysis will be performed using the encrypted files, which are downloaded to a processing site, decrypted, and processed automatically (ie, without any person viewing the videos).',\n",
       "   'This procedure will result in anonymized aggregate data sets.',\n",
       "   'Decrypted files will subsequently be deleted from the processing site.',\n",
       "   'All protocol-required information besides video data is collected in an electronic case report form.',\n",
       "   'The REDCap electronic cloud is used to store and manage all consent and study data.',\n",
       "   'All data collected about study participants are kept strictly confidential.',\n",
       "   'Performance Targets',\n",
       "   'The accuracy of Lifelight using the training data generated in VISION-D was sufficient to support the certification of Lifelight as a class I CE medical device [ 15 ].',\n",
       "   'However, the accuracy needs to be improved further for use in routine clinical practice.',\n",
       "   'Table 2 lists the performance targets for Lifelight; training data collected during VISION-MD will support the progress toward these targets.',\n",
       "   'Sample Size',\n",
       "   'The sample size cannot be formally calculated because it depends on the incremental improvement in the accuracy of Lifelight achieved through machine learning using the training data generated in the study.',\n",
       "   'However, indicative sample sizes for the 4 subprotocols have been calculated by assessing the optimal data requirements to enable algorithm training toward the standards defined in Table 3 , balanced against the practicality of achieving the targets.',\n",
       "   'The split between training and testing data will be determined during the study according to the quality of the data collected.',\n",
       "   'The initial protocol anticipated data collection from about 8400 participants for training and a further 1000 for independent testing of accuracy, but the recruitment has been compromised by restrictions implemented during the COVID-19 pandemic.',\n",
       "   'However, the high-quality video recording (compared with VISION-D and VISION-V) supported a protocol amendment to reduce the recruitment to 1950 participants (see Results section), with the expectation that data from about 1700 will be used for training the algorithms and data from 250 used for testing.',\n",
       "   'The study management team is monitoring the progress of data collection and accuracy, and updates the study teams monthly.',\n",
       "   'The study will continue until the accuracy of Lifelight for measuring VS is sufficient for various clinical use cases.',\n",
       "   'As skin tone is expected to affect the accuracy of Lifelight, the aim is to recruit participants across the full Fitzpatrick skin tone scale (1-6).',\n",
       "   'To allow the impact of skin tone measurement accuracy to be determined with statistical robustness, the full data set will be sampled to create a subset for skin tone analyses in which the prevalence of the usually less prevalent skin tones is amplified.',\n",
       "   'This subset will contain 750-1000 measurements, with 15%-20% each from categories 1, 2 and 3, 4, and 5 and 6.',\n",
       "   'These measurements should be spread across the subprotocols as indicated in Table 3 .',\n",
       "   'Data Analysis',\n",
       "   'The training data will be used to further develop the signal extraction and processing methodology.',\n",
       "   'The test data will subsequently be used to determine the performance of Lifelight against the targets set out in Table 2 .',\n",
       "   'All statistical analyses will be performed using Microsoft Excel.',\n",
       "   'All analyses will be completed per protocol since there is no intention to treat.',\n",
       "   'There will be no imputation of missing or implausible data, and any missing, implausible, or problematic readings will be excluded from the analysis.',\n",
       "   'If the Lifelight software is unable to detect the participant’s face during the measurement period, this will be recorded in the electronic case report form, and the measurements will be deleted from the data set.'],\n",
       "  'TABLE': ['Table 1',\n",
       "   'Recruitment criteria and vital sign measurement in subprotocols 1-4.',\n",
       "   'Subprotocol',\n",
       "   'Recruitment criteria',\n",
       "   'Measurements',\n",
       "   'Measurements, n',\n",
       "   'PR a',\n",
       "   'BP b',\n",
       "   'RR c',\n",
       "   'SpO 2 d',\n",
       "   '1',\n",
       "   'Abnormal BP e',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '3',\n",
       "   '2',\n",
       "   'Any participant',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '2',\n",
       "   '3',\n",
       "   'Expected to have low SpO 2 f',\n",
       "   '✓',\n",
       "   '2',\n",
       "   '4',\n",
       "   'Adults lacking capacity',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '✓',\n",
       "   '3',\n",
       "   'a PR: pulse rate.',\n",
       "   'b BP: blood pressure.',\n",
       "   'c RR: respiratory rate.',\n",
       "   'd SpO 2 : oxygen saturation.',\n",
       "   'e Abnormal defined as systolic blood pressure <100 mm Hg or >140\\xa0mm Hg.',\n",
       "   'f Low SpO 2 (anticipated to be ≤95%).',\n",
       "   'Table 2',\n",
       "   'Performance (accuracy) targets for Lifelight.',\n",
       "   'Vital signs',\n",
       "   'Accuracy target',\n",
       "   'Basis for target',\n",
       "   'Blood pressure',\n",
       "   'SBP a can be measured with standard deviation ≤8\\xa0mm Hg British Hypertension Society Grade C for SBP measurement',\n",
       "   'ISO81060-2 b for blood pressure cuffs [ 19 ]',\n",
       "   'Pulse rate',\n",
       "   'Root mean square error of ≤3\\xa0beats per minute',\n",
       "   'Most common accuracy of CE c -marked commercially available devices',\n",
       "   'Respiratory rate',\n",
       "   'Maximum error tolerance of 5 breaths per minute',\n",
       "   'Accuracy of Philips Health watch, a CE-marked contact-based photoplethysmography device',\n",
       "   'Oxygen saturation',\n",
       "   'Maximum error tolerance of 4%',\n",
       "   'ISO80601-2-61 standard for pulse oximeters [ 20 ]',\n",
       "   'a SBP: systolic blood pressure.',\n",
       "   'b ISO: International Organization for Standardization.',\n",
       "   'c CE: Conformité Européenne.',\n",
       "   'Table 3',\n",
       "   'Indicative sample size targets.',\n",
       "   'Subprotocol',\n",
       "   'Indicative sample size, n a',\n",
       "   'Characteristics',\n",
       "   'Participants with skin tone categories 1, 4, 5, and 6',\n",
       "   '1',\n",
       "   '1500',\n",
       "   'Roughly 100 participants will be recruited with SOC b -determined SBP c in each 10\\xa0mm Hg increment from <90 mm Hg to >200\\xa0mm Hg (ie, <90 mm Hg; 90-99 mm Hg; 100-109 mm Hg, etc) d',\n",
       "   'Ideally ≥4 in each SBP band',\n",
       "   '2',\n",
       "   '375',\n",
       "   'N/A e',\n",
       "   'Ideally ≥10 in each SBP band',\n",
       "   '3',\n",
       "   '35',\n",
       "   'Approximately 33% with SOC-measured oxygen saturation <88%, 88%-92%, and 93%-95%',\n",
       "   'Ideally, each band will include participants with each skin tone',\n",
       "   '4',\n",
       "   'No specific target; likely to be a small proportion',\n",
       "   'N/A',\n",
       "   'N/A',\n",
       "   'a Participants in subprotocol 4 (ie, those without the capacity to provide informed consent) are likely to have vital sign values outside of the normal range and will contribute to all subprotocol targets.',\n",
       "   'Only the first study session per participant contributes to the sample size.',\n",
       "   'b SOC: standard of care.',\n",
       "   'c SBP: systolic blood pressure.',\n",
       "   'd Can include participants with SBP measured from an arterial line.',\n",
       "   'e N/A: not applicable.'],\n",
       "  'RESULTS': ['Results',\n",
       "   'The prototype Lifelight technology has been in development since 2016.',\n",
       "   'The recruitment of participants for VISION-MD started in May 2021 but was compromised by the restrictions implemented to manage the COVID-19 pandemic, including restricting hospital access to the general public.',\n",
       "   'Protocol amendments were thus made to enhance community recruitment, including the use of incentives such as chocolates or gift cards.',\n",
       "   'In addition, the higher-resolution video recording (compared with the earlier VISION studies) supported reduction of the recruitment target to 1950, which is expected to yield sufficient high-quality measurements for machine learning and subsequent testing (reflected in a further protocol amendment).',\n",
       "   'An additional amendment allowed the measurement of BP and pulse rate using devices other than the standard-of-care Welch Allyn devices (and indicated in the electronic case report form), as not all participating centers had the originally specified equipment.',\n",
       "   'Data for analysis will become available from September 2022, and the algorithms will be continuously refined to improve clinical accuracy.',\n",
       "   'We anticipate that the final analyses to determine the performance of Lifelight against the targets set out in Table 2 will be complete in early 2023.'],\n",
       "  'DISCUSS': ['Discussion',\n",
       "   'The VISION-MD study is expected to provide sufficient high-quality data from a wide range of healthy volunteers and patients (including critically ill patients) to further develop the accuracy of the software for estimating VS in clinical and community settings.',\n",
       "   'While the VISION-V and -D studies demonstrated the potential value of Lifelight in the contactless measurement of VS and supported class I CE certification [ 15 ], further refinement of data collection and analysis methods is needed—particularly VS measurements outside the normal healthy range—to develop the algorithms for clinical use.',\n",
       "   'The high-quality videos collected in the VISION-MD studies will be instrumental in training the algorithms being developed for data processing.',\n",
       "   'A proportion of the data collected will be retained for testing the performance of Lifelight in estimating VS compared with the standard of care.',\n",
       "   'The study findings will be published in high-impact peer-reviewed scientific journals and presented at international cardiology, respiratory, and medical device conferences.'],\n",
       "  'ABBR': ['Abbreviations',\n",
       "   'BP',\n",
       "   'blood pressure',\n",
       "   'CE',\n",
       "   'Conformité Européenne',\n",
       "   'HRA',\n",
       "   'Health Research Authority',\n",
       "   'NHS',\n",
       "   'National Health Service',\n",
       "   'PPG',\n",
       "   'photoplethysmography',\n",
       "   'REDCap',\n",
       "   'Research Electronic Data Capture',\n",
       "   'rPPG',\n",
       "   'remote photoplethysmography',\n",
       "   'VS',\n",
       "   'vital signs',\n",
       "   'VISION-D',\n",
       "   'Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care–Development',\n",
       "   'VISION-V',\n",
       "   'Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care–Validation',\n",
       "   'VISION-MD',\n",
       "   'Measurement of Vital Signs by Lifelight Software in Comparison to the Standard of Care–Multisite Development'],\n",
       "  'REF': ['1 Buist MD Jarmolowski E Burton PR Bernard SA Waxman BP Anderson J Recognising clinical instability in hospital patients before cardiac arrest or unplanned admission to intensive care.',\n",
       "   'A pilot study in a tertiary-care hospital Med J Aust 1999 07 05 171 1 22 25 10.5694/j.1326-5377.1999.tb123492.x 10451667 10451667 2 Hands C Reid E Meredith P Smith GB Prytherch DR Schmidt PE Featherstone PI Patterns in the recording of vital signs and early warning scores: compliance with a clinical escalation protocol BMJ Qual Saf 2013 09 22 9 719 726 10.1136/bmjqs-2013-001954 23603474 bmjqs-2013-001954 3 van Leuvan CH Mitchell I Missed opportunities?',\n",
       "   \"An observational study of vital sign measurements Crit Care Resusc 2008 06 10 2 111 115 18522524 18522524 4 Ludikhuize J Smorenburg SM de Rooij SE de Jonge E Identification of deteriorating patients on general wards; measurement of vital parameters and potential effectiveness of the Modified Early Warning Score J Crit Care 2012 08 27 4 424.e7 13 10.1016/j.jcrc.2012.01.003 22341727 S0883-9441(12)00016-0 5 Flacco ME Manzoli L Bucci M Capasso L Comparcini D Simonetti V Gualano MR Nocciolini M D'Amario C Cicolini G Uneven accuracy of home blood pressure measurement: a multicentric survey J Clin Hypertens (Greenwich) 2015 08 17 8 638 643 10.1111/jch.12552 10.1111/jch.12552 25880129 25880129 6 Kamal AA Harness JB Irving G Mearns AJ Skin photoplethysmography—a review Comput Methods Programs Biomed 1989 04 28 4 257 269 10.1016/0169-2607(89)90159-4 2649304 0169-2607(89)90159-4 2649304 7 Johansson A Oberg PA Sedin G Monitoring of heart and respiratory rates in newborn infants using a new photoplethysmographic technique J Clin Monit Comput 1999 12 15 7-8 461 467 10.1023/a:1009912831366 12578044 12578044 8 Poh M Poh YC Validation of a standalone smartphone application for measuring heart rate using imaging photoplethysmography Telemed J E Health 2017 08 23 8 678 683 10.1089/tmj.2016.0230 28140834 28140834 9 Aoyagi T Miyasaka K Pulse oximetry: its invention, contribution to medicine, and future tasks Anesth Analg 2002 01 94 1 Suppl S1 3 11900029 10 Elgendi M Fletcher R Liang Y Howard N Lovell NH Abbott D Lim K Ward R The use of photoplethysmography for assessing hypertension NPJ Digit Med 2019 2 60 10.1038/s41746-019-0136-7 10.1038/s41746-019-0136-7 31388564 136 31388564 11 Radha M de Groot K Rajani N Wong CCP Kobold N Vos V Fonseca P Mastellos N Wark PA Velthoven N Haakma R Aarts RM Estimating blood pressure trends and the nocturnal dip from photoplethysmography Physiol Meas 2019 02 26 40 2 025006 10.1088/1361-6579/ab030e 30699397 30699397 12 Nilsson L Johansson A Kalman S Monitoring of respiratory rate in postoperative care using a new photoplethysmographic technique J Clin Monit Comput 2000 16 4 309 315 10.1023/a:1011424732717 12578078 12578078 13 Sun Y Yang Y Wu B Huang P Cheng S Wu B Chen C Contactless facial video recording with deep learning models for the detection of atrial fibrillation Sci Rep 2022 01 07 12 1 281 10.1038/s41598-021-03453-y 10.1038/s41598-021-03453-y 34996908 10.1038/s41598-021-03453-y 34996908 14 Heiden E Jones T Measurement of vital signs using Lifelight® Remote Photoplethysmography: results of the VISION-D and VISION-V observational studies JMIR Form Res 2022 6 11 e36340 10.2196/36340 36374541 15 Medtech innovation briefing MIB213: Lifelight First for monitoring vital signs 2020 National Institute for Health and Care Excellence 2022 2022-10-11 https://www.nice.org.uk/advice/mib213/chapter/The-technology 16 Kwon S Kim J Lee D Park K ROI analysis for remote photoplethysmography on facial video Annu Int Conf IEEE Eng Med Biol Soc 2015 08 4938 4941 10.1109/EMBC.2015.7319499 26737399 26737399 17 Kashima H Ikemura T Hayashi N Regional differences in facial skin blood flow responses to the cold pressor and static handgrip tests Eur J Appl Physiol 2013 04 113 4 1035 1041 10.1007/s00421-012-2522-6 23064980 23064980 18 Colvonen PJ Response to: investigating sources of inaccuracy in wearable optical heart rate sensors NPJ Digit Med 2021 02 26 4 1 38 10.1038/s41746-021-00408-5 10.1038/s41746-021-00408-5 33637822 10.1038/s41746-021-00408-5 33637822 19 ISO 81060-2+A1: non-invasive sphygmomanometers— part 2: clinical investigation of intermittent automated measurement type ISO 2019 2022-10-11 https://www.iso.org/standard/73339.html 20 ISO 80601-2-61: medical electrical equipment— part 2-61: particular requirements for basic safety and essential performance of pulse oximeter equipment ISO 2019 2022-10-11 https://www.iso.org/standard/67963.html\"],\n",
       "  'TITLE-GROUP': ['Measurement of Vital Signs by Lifelight Software in Comparison to Standard of Care Multisite Development (VISION-MD): Protocol for an Observational Study'],\n",
       "  'ACK': ['The VISION-MD protocol was codeveloped by Barts Biomedical Research Centre, Portsmouth Hospitals University NHS Trust, Mind over Matter Medtech, and Xim Ltd. This report is an independent research funded by the National Institute for Health Research (Artificial Intelligence, Developing Lifelight: A contactless vital signs monitor for CVD screening, AI_AWARD02031) and NHSX.',\n",
       "   'The views expressed in this publication are those of the authors and not necessarily those of the National Institute for Health Research, NHSX, or the Department of Health and Social Care.',\n",
       "   'Medical writing support was provided by Helen Barham, PhD (The Text Doctor), funded by Xim Ltd.',\n",
       "   'The contributors associated with Lifelight Trials Group are as follows: Sharon Allard, Dr Mark Lyons-Amos, Bethany Armstead, Rosalynn Austin, Rebecca Baker, Dr Michelle Baker Moffat, Armida Balawon, Debbi Barnes, Sonia Baryschpolec, Sean Beech, Selina Begum, Lauren Bell, Helen Blackman, Marie Broadway, Kate Burrows, Philippa Copnall, Zoe Daly, Joanne Dash, Mini David, Teresa Day, Jacqueline Denham, Rodrigo Dias, Alison Dimmer, Gemma Dixon, Tracey Dobson, Catherine Edwards, Carole Fogg, Dr Jim Forrer, Francis Galera, Zoe Garner, Andrew Gribbin, Elizabeth Hawes, Serena Howe, Karen Hudson, Amanda Hungate, Victoria Hunter, Jo Kerr, Adam Kiddle, Arjun Kumar, Shanqin Liu, Beverley Longhurst, Sharon McCready, Shoid Miah, Maria Moon, Kirsty Parker, Gina Pelletier, Connie Petronzio, David Petronzio, Michelle Pople, Benildo Jr Quiros, Deidre Rodgers, Dr Mike Sadler, Kerrie Scott, Josh Sephton, Samantha Smith, Bruce Stanley, Nina Szarazova, Nick Thorne, Monika Thwaites, Sarah Tronk, Catherine Tuffrey, Marcus Tuke, Charlotte Turner, James Turner, Lewis Valaitis, Dr Lieke van Putten, Lyn Vinall, Marie White, Melanie Willcox, Jonathon Winter, Carole Wragg, Kim Wren.']}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()  # Record the start time\n",
    "tt = process_full_text(ss[191])\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "print(f\"Execution time: {elapsed_time:.4f} seconds\")\n",
    "\n",
    "print(\"Structured JSON in strict order with sent_ids starting from 1:\")\n",
    "# pprint.pprint(tt, indent=2)\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7075ab1a-fa5b-43db-abc4-53ed8e76a66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ABSTRACT', 'INTRO', 'METHODS', 'TABLE', 'RESULTS', 'DISCUSS', 'ABBR', 'REF', 'TITLE-GROUP', 'ACK'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt['sections'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527023de-0b71-4b88-ae58-7111c7339cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_labels = ['TITLE', 'ABSTRACT', 'INTRO', 'METHODS', 'RESULTS', 'DISCUSS', 'CONCL', 'CASE', 'ACK_FUND', 'AUTH_CONT', 'COMP_INT', 'ABBR', 'SUPPL', 'REF', 'ACK_FUND', 'ABBR', 'COMP_INT', 'SUPPL', 'APPENDIX', 'AUTH_CONT']\n",
    "\n",
    "yy = process_json(tt, ordered_labels)\n",
    "pprint.pprint(yy, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a51c4b-e26b-4ad4-916b-aaa6c0b8232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a8fe0-c0e9-4cf2-90cf-bc26763cba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d5e61-0264-4b0e-8392-281e0e087260",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Title', 'Abstract'. 'Methods', 'Results', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8d40d-521f-4ef5-8ef1-47a34a0ff6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154a9377-617e-4204-a6b1-a4516dfb9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_execution_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Measure the execution time of a function.\n",
    "    \n",
    "    Args:\n",
    "    - func (function): The function to measure.\n",
    "    - *args: Positional arguments to pass to the function.\n",
    "    - **kwargs: Keyword arguments to pass to the function.\n",
    "\n",
    "    Returns:\n",
    "    - result: The result of the function execution.\n",
    "    - elapsed_time: The time taken to execute the function in seconds.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Record the start time\n",
    "    result = func(*args, **kwargs)  # Execute the function\n",
    "    end_time = time.time()  # Record the end time\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    print(f\"Execution time: {elapsed_time:.4f} seconds\")\n",
    "    return result, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473b49a-1c6e-4965-8d9f-1b4d74759c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
